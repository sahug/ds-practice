{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aVx4E4zZlPHQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bv3U_23AoWHB"
   },
   "outputs": [],
   "source": [
    "org_train_data = pd.read_csv(\"data/Tabular Playground Series - Apr 2021/train.csv\")\n",
    "org_test_data = pd.read_csv(\"data/Tabular Playground Series - Apr 2021/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "IyhQ-XO9pXyU",
    "outputId": "5b1bde6e-7b74-454e-c786-f1c99e11948e"
   },
   "outputs": [],
   "source": [
    "org_train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "9GLOMuJ1o86g",
    "outputId": "d40ac502-84b4-4102-a458-d2ceba748105"
   },
   "outputs": [],
   "source": [
    "org_test_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNZVmNdJtikp"
   },
   "outputs": [],
   "source": [
    "# Check for Null data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2Us0YVvpG77",
    "outputId": "6fbcbf59-1c39-4216-dbd6-5b50248cbf21"
   },
   "outputs": [],
   "source": [
    "org_train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HVGZ5D67pO7P",
    "outputId": "79a7e9e9-936e-4f9f-aa0a-33491120d433"
   },
   "outputs": [],
   "source": [
    "org_test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOe5Ft0UtlxS"
   },
   "outputs": [],
   "source": [
    "# Check % of Null data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oG7WXiHvt1Ej"
   },
   "outputs": [],
   "source": [
    "def missingdata(data):\n",
    "    total = data.isnull().sum().sort_values(ascending=False)\n",
    "    percent = (data.isnull().sum() / data.isnull().count() * 100).sort_values(\n",
    "        ascending=False\n",
    "    )\n",
    "    ms = pd.concat([total, percent], axis=1, keys=[\"Total\", \"Percent\"])\n",
    "    ms = ms[ms[\"Percent\"] > 0]\n",
    "    f, ax = plt.subplots(figsize=(8, 6))\n",
    "    plt.xticks(rotation=\"90\")\n",
    "\n",
    "    fig = sns.barplot(ms.index, ms[\"Percent\"], color=\"green\", alpha=0.8)\n",
    "    plt.xlabel(\"Features\", fontsize=15)\n",
    "    plt.ylabel(\"Percent of missing values\", fontsize=15)\n",
    "    plt.title(\"Percent missing data by feature\", fontsize=15)\n",
    "    return ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 692
    },
    "id": "ub-ZAPW1yRg6",
    "outputId": "7443080b-158a-4cf2-c170-10e5f2aa3bcf"
   },
   "outputs": [],
   "source": [
    "missingdata(org_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 692
    },
    "id": "jMLr96vazY3m",
    "outputId": "c9141fbd-8ebf-4658-a912-326ae7e5243b"
   },
   "outputs": [],
   "source": [
    "missingdata(org_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdAB7m2uqa82"
   },
   "outputs": [],
   "source": [
    "# Drop unnecessary columns. Cabin has 70% Null values in bothe Train and Test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "1e0UiCNPqfe4",
    "outputId": "f5a9c3f3-1cf6-4632-97ea-3e4bea60be06"
   },
   "outputs": [],
   "source": [
    "# Drop from Train Dataset\n",
    "\n",
    "org_train_data.drop([\"PassengerId\", \"Cabin\"], axis=1, inplace=True)\n",
    "org_train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "iqynvcYprTA5",
    "outputId": "e4d498fb-d00d-4e0e-a702-166f8882b9a1"
   },
   "outputs": [],
   "source": [
    "# Drop from Test Dataset\n",
    "org_test_pssg_id = org_test_data[\"PassengerId\"]\n",
    "org_test_data.drop([\"PassengerId\", \"Cabin\"], axis=1, inplace=True)\n",
    "org_test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOX5T5lrr-Tb"
   },
   "outputs": [],
   "source": [
    "def fill_age(data):\n",
    "    # Populate Missing Age - Has SibSp and Parch\n",
    "    # Age for passanger travelling with SibSp and Parch\n",
    "\n",
    "    # Find Median Age for passanger travelling with SibSp and Parch\n",
    "    age_median_sibsp_parch = data[(data[\"Parch\"] > 0) & (data[\"SibSp\"] > 0)][\n",
    "        \"Age\"\n",
    "    ].median()\n",
    "\n",
    "    # Populate Age for passanger travelling with SibSp and Parch where Age is Null\n",
    "    data.loc[\n",
    "        (data[\"Parch\"] > 0) & (data[\"SibSp\"] > 0) & (data[\"Age\"].isnull()), \"Age\"\n",
    "    ] = age_median_sibsp_parch\n",
    "\n",
    "    # Populate Missing Age - Has SibSp and NO Parch\n",
    "    # Age for passanger travelling with SibSp and NO Parch\n",
    "\n",
    "    # Find Median Age for passanger travelling with SibSp and NO Parch\n",
    "    age_median_sibsp_no_parch = data[(data[\"Parch\"] == 0) & (data[\"SibSp\"] > 0)][\n",
    "        \"Age\"\n",
    "    ].median()\n",
    "\n",
    "    # Populate Age for passanger travelling with SibSp and NO Parch where Age is Null\n",
    "    data.loc[\n",
    "        (data[\"Parch\"] == 0) & (data[\"SibSp\"] > 0) & (data[\"Age\"].isnull()), \"Age\"\n",
    "    ] = age_median_sibsp_no_parch\n",
    "\n",
    "    # Populate Missing Age - Has NO SibSp and NO Parch\n",
    "    # Age for passanger travelling with NO SibSp and NO Parch\n",
    "\n",
    "    # Find Median Age for passanger travelling with NO SibSp and NO Parch\n",
    "    age_median_no_sibsp_no_parch = data[(data[\"Parch\"] == 0) & (data[\"SibSp\"] == 0)][\n",
    "        \"Age\"\n",
    "    ].median()\n",
    "\n",
    "    # Populate Age for passanger travelling with NO SibSp and NO Parch where Age is Null\n",
    "    data.loc[\n",
    "        (data[\"Parch\"] == 0) & (data[\"SibSp\"] == 0) & (data[\"Age\"].isnull()), \"Age\"\n",
    "    ] = age_median_no_sibsp_no_parch\n",
    "\n",
    "    # Populate Missing Age - Has NO SibSp and Has Parch\n",
    "    # Age for passanger travelling with NO SibSp and Has Parch\n",
    "\n",
    "    # Find Median Age for passanger travelling with NO SibSp and Has Parch\n",
    "    age_median_no_sibsp_no_parch = data[(data[\"Parch\"] > 0) & (data[\"SibSp\"] == 0)][\n",
    "        \"Age\"\n",
    "    ].median()\n",
    "\n",
    "    # Populate Age for passanger travelling with NO SibSp and Has Parch where Age is Null\n",
    "    data.loc[\n",
    "        (data[\"Parch\"] > 0) & (data[\"SibSp\"] == 0) & (data[\"Age\"].isnull()), \"Age\"\n",
    "    ] = age_median_no_sibsp_no_parch\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cL22lt0jstEY"
   },
   "outputs": [],
   "source": [
    "# Fill Age for Train and Test Dataset\n",
    "org_train_data = fill_age(org_train_data)\n",
    "org_test_data = fill_age(org_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFnV--8roKQT"
   },
   "outputs": [],
   "source": [
    "# Fill Fare for Test and Train Dataset\n",
    "org_train_data[\"Fare\"].fillna(org_train_data[\"Fare\"].mode()[0], inplace=True)\n",
    "org_test_data[\"Fare\"].fillna(org_train_data[\"Fare\"].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BHy1YeTCqE4A"
   },
   "outputs": [],
   "source": [
    "# Check for Tickets and add Has_ticket\n",
    "org_train_data[\"Has_Ticket\"] = np.where(org_train_data[\"Ticket\"].isnull(), 0, 1)\n",
    "org_test_data[\"Has_Ticket\"] = np.where(org_test_data[\"Ticket\"].isnull(), 0, 1)\n",
    "\n",
    "# Drop Ticket\n",
    "org_train_data.drop(\"Ticket\", axis=1, inplace=True)\n",
    "org_test_data.drop(\"Ticket\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FTT9w4G0hx_j"
   },
   "outputs": [],
   "source": [
    "# Fill Embarked for Test and Train Dataset\n",
    "org_train_data[\"Embarked\"].fillna(org_train_data[\"Embarked\"].mode()[0], inplace=True)\n",
    "org_test_data[\"Embarked\"].fillna(org_train_data[\"Embarked\"].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "foLrvtILxjqq"
   },
   "outputs": [],
   "source": [
    "# Apply One Hot Encoding to Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JHSVKHSCRlVh"
   },
   "outputs": [],
   "source": [
    "# Sex\n",
    "def encode_sex(data):\n",
    "    return pd.get_dummies(data[\"Sex\"], drop_first=True)\n",
    "\n",
    "\n",
    "train_sex = encode_sex(org_train_data)\n",
    "test_sex = encode_sex(org_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9OTnDQIxPOc"
   },
   "outputs": [],
   "source": [
    "# Pclass\n",
    "def encode_pclass(data):\n",
    "    return pd.get_dummies(data[\"Pclass\"], drop_first=True)\n",
    "\n",
    "\n",
    "train_pclass = encode_pclass(org_train_data)\n",
    "test_pclass = encode_pclass(org_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VOU3R5AkxW2G"
   },
   "outputs": [],
   "source": [
    "# Embarked\n",
    "def encode_embarked(data):\n",
    "    return pd.get_dummies(org_train_data[\"Embarked\"], drop_first=True)\n",
    "\n",
    "\n",
    "train_embarked = encode_embarked(org_train_data)\n",
    "test_embarked = encode_embarked(org_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKhXeKPQxwBl"
   },
   "outputs": [],
   "source": [
    "# Add Encoded Sex, embarked, pclass to Train Dataset\n",
    "org_train_data = pd.concat(\n",
    "    [org_train_data, train_sex, train_embarked, train_pclass], axis=1\n",
    ")\n",
    "org_test_data = pd.concat([org_test_data, test_sex, test_embarked, test_pclass], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbEH7aCpyAoS"
   },
   "outputs": [],
   "source": [
    "# Drop Sex, embarked, pclass to Train Dataset\n",
    "org_train_data.drop([\"Pclass\", \"Sex\", \"Embarked\"], axis=1, inplace=True)\n",
    "org_test_data.drop([\"Pclass\", \"Sex\", \"Embarked\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Sex, embarked, pclass to Train Dataset\n",
    "org_train_data.drop([\"Name\"], axis=1, inplace=True)\n",
    "org_test_data.drop([\"Name\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "5n5ZSTnf1q_g",
    "outputId": "d6198592-0321-4a72-8c8c-249a7115ba6b"
   },
   "outputs": [],
   "source": [
    "org_train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "CYYylw5i1uIr",
    "outputId": "35e0b338-0734-446d-d1b2-0fa9e097eb61"
   },
   "outputs": [],
   "source": [
    "org_test_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lsvy3xa3KLmH"
   },
   "outputs": [],
   "source": [
    "training_feature = org_train_data.drop(\"Survived\", axis=1)\n",
    "training_target = org_train_data[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(\n",
    "    training_feature.corr(), annot=True, cmap=\"RdYlGn\", linewidths=0.2\n",
    ")  # data.corr()-->correlation matrix\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(20, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nQYCJGNq92Gh",
    "outputId": "87a39814-d9d7-40b3-e083-e1cef2e50095"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "training_feature = standard_scaler.fit_transform(training_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9L7LdIhjK5_t",
    "outputId": "77ec0bf7-0f16-4811-ffc0-7b63f0975a23"
   },
   "outputs": [],
   "source": [
    "training_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LiNa_imXMrRP",
    "outputId": "8fd47196-ba3c-4260-db74-32e2a5353b56"
   },
   "outputs": [],
   "source": [
    "training_feature.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WJcTEuAXMxRq",
    "outputId": "83c48fca-1434-4b92-db77-c2c26519b65e"
   },
   "outputs": [],
   "source": [
    "org_test_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvVuZSlmlkCh"
   },
   "source": [
    "**Model**\n",
    "\n",
    "Now we are ready to train a model and predict the required solution. There are lot of predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\n",
    "\n",
    "- Logistic Regression\n",
    "- KNN\n",
    "- Support Vector Machines\n",
    "- Naive Bayes classifier\n",
    "- Decision Tree\n",
    "- Random Forrest\n",
    "- Linear Discriminant Analysis\n",
    "- Ada Boost Classifier\n",
    "- Gradient Boosting Classifier\n",
    "\n",
    "And also compared above given classifiers and evaluate the mean accuracy of each of them by a stratified kfold cross validation procedure and plot accuracy based confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1G2u1YtYlyxP"
   },
   "source": [
    "## Split Data to Test and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQK4ARSUlwmQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    cross_val_predict,\n",
    "    cross_val_score,\n",
    "    train_test_split,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtiE2kh2mx3G"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    training_feature, training_target, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xKcU1CtwngNt",
    "outputId": "c2f77712-811e-4d17-9724-23529d50e1a2"
   },
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtEBzUHMnwjC"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "3paoP8Dhn1zg",
    "outputId": "54c6c562-18bd-4372-c68e-51803588e95e"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(x_train, y_train)\n",
    "lr_prediction = logistic_regression.predict(x_test)\n",
    "\n",
    "print(\"-------------- The Accuracy of the model ----------------------------\")\n",
    "print(\n",
    "    \"The Accuracy of the Logistic Regression is\",\n",
    "    round(accuracy_score(y_test, lr_prediction) * 100, 2),\n",
    ")\n",
    "\n",
    "# k=10, split the data into 10 equal parts\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=22)\n",
    "\n",
    "lr_cv_score = cross_val_score(\n",
    "    logistic_regression, training_feature, training_target, cv=kfold, scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"The cross validated score for Logistic Regression is:\",\n",
    "    round(lr_cv_score.mean() * 100, 2),\n",
    ")\n",
    "\n",
    "y_pred = cross_val_predict(\n",
    "    logistic_regression, training_feature, training_target, cv=kfold\n",
    ")\n",
    "\n",
    "sns.heatmap(\n",
    "    confusion_matrix(training_target, y_pred), annot=True, fmt=\"3.0f\", cmap=\"summer\"\n",
    ")\n",
    "\n",
    "plt.title(\"Logistic Regression Confusion Matrix\", y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression HyperParameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "logistic_regression = LogisticRegression()\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=22)\n",
    "\n",
    "param_grid = {\n",
    "    \"penalty\": [\"l2\", \"l1\"],\n",
    "    \"C\": [100, 10, 1.0, 0.1, 0.01],\n",
    "}\n",
    "\n",
    "lr_model = GridSearchCV(\n",
    "    estimator=logistic_regression,\n",
    "    param_grid=param_grid,\n",
    "    cv=kfold,\n",
    "    n_jobs=4,\n",
    "    scoring=\"accuracy\",\n",
    ")\n",
    "\n",
    "lr_model.fit(x_train, y_train)\n",
    "\n",
    "# Best score\n",
    "print(lr_model.best_score_)\n",
    "\n",
    "# Best Estimator\n",
    "lr_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_regression = LogisticRegression(\n",
    "    C=1.0,\n",
    "    class_weight=None,\n",
    "    dual=False,\n",
    "    fit_intercept=True,\n",
    "    intercept_scaling=1,\n",
    "    max_iter=100,\n",
    "    multi_class=\"warn\",\n",
    "    n_jobs=None,\n",
    "    penalty=\"l1\",\n",
    "    random_state=None,\n",
    "    solver=\"warn\",\n",
    "    tol=0.0001,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    ")\n",
    "\n",
    "logistic_regression.fit(x_train, y_train)\n",
    "\n",
    "lr_prediction = logistic_regression.predict(x_test)\n",
    "\n",
    "print(\"-------------- The Accuracy of the model ----------------------------\")\n",
    "print(\n",
    "    \"The Accuracy of the Logistic Regression is\",\n",
    "    round(accuracy_score(y_test, lr_prediction) * 100, 2),\n",
    ")\n",
    "\n",
    "# k=10, split the data into 10 equal parts\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=22)\n",
    "\n",
    "lr_cv_score = cross_val_score(\n",
    "    logistic_regression, training_feature, training_target, cv=kfold, scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"The cross validated score for Logistic Regression is:\",\n",
    "    round(lr_cv_score.mean() * 100, 2),\n",
    ")\n",
    "\n",
    "y_pred = cross_val_predict(\n",
    "    logistic_regression, training_feature, training_target, cv=kfold\n",
    ")\n",
    "\n",
    "sns.heatmap(\n",
    "    confusion_matrix(training_target, y_pred), annot=True, fmt=\"3.0f\", cmap=\"summer\"\n",
    ")\n",
    "\n",
    "plt.title(\"Logistic Regression Confusion Matrix\", y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLH72vic-rS3"
   },
   "source": [
    "## RandomForestClassifier Hyper-Parameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "\n",
    "n_estimator = range(500, 800, 100)\n",
    "\n",
    "param_grid = {\"n_estimators\": n_estimator}\n",
    "\n",
    "# k=10, split the data into 10 equal parts\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=22)\n",
    "\n",
    "rf_classisier_cv = GridSearchCV(\n",
    "    random_forest,\n",
    "    param_grid=param_grid,\n",
    "    cv=kfold,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=4,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "rf_classisier_cv.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# Best score\n",
    "print(rf_classisier_cv.best_score_)\n",
    "\n",
    "# Best Estimator\n",
    "rf_classisier_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestClassifier with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "PTVV1d5c-tMS",
    "outputId": "ab0b1491-95b0-4969-aecf-bc9904f9d261"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = RandomForestClassifier(\n",
    "    criterion=\"gini\",\n",
    "    n_estimators=700,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=1,\n",
    "    max_features=\"auto\",\n",
    "    oob_score=True,\n",
    "    random_state=1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "random_forest.fit(x_train, y_train)\n",
    "\n",
    "rf_prediction = random_forest.predict(x_test)\n",
    "\n",
    "print(\"-------------- The Accuracy of the model ----------------------------\")\n",
    "print(\n",
    "    \"The Accuracy of the Random Forest is\",\n",
    "    round(accuracy_score(y_test, rf_prediction) * 100, 2),\n",
    ")\n",
    "\n",
    "# k=10, split the data into 10 equal parts\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=22)\n",
    "\n",
    "rf_cv_score = cross_val_score(\n",
    "    random_forest, training_feature, training_target, cv=kfold, scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"The cross validated score for Random Forest is:\",\n",
    "    round(rf_cv_score.mean() * 100, 2),\n",
    ")\n",
    "\n",
    "y_pred = cross_val_predict(random_forest, training_feature, training_target, cv=kfold)\n",
    "\n",
    "sns.heatmap(\n",
    "    confusion_matrix(training_target, y_pred), annot=True, fmt=\"3.0f\", cmap=\"summer\"\n",
    ")\n",
    "\n",
    "plt.title(\"Random Forest Confusion Matrix\", y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFsoCz-1oDwv"
   },
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eD6SI9X_oLzK"
   },
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "# svc = SVC()\n",
    "# svc.fit(x_train, y_train)\n",
    "# svc_prediction = svc.predict(x_test)\n",
    "\n",
    "# print('-------------- The Accuracy of the model ----------------------------')\n",
    "# print('The Accuracy of the SVC is', round(accuracy_score(y_test, svc_prediction) * 100, 2))\n",
    "\n",
    "# # k=10, split the data into 10 equal parts\n",
    "# kfold = KFold(n_splits=10, shuffle=True, random_state=22)\n",
    "\n",
    "# svc_cv_score = cross_val_score(svc, training_feature, training_target, cv=kfold, scoring='accuracy')\n",
    "\n",
    "# print('The cross validated score for SVC is:', round(svc_cv_score.mean() * 100, 2))\n",
    "\n",
    "# y_pred = cross_val_predict(svc, training_feature, training_target, cv=kfold)\n",
    "\n",
    "# sns.heatmap(confusion_matrix(training_target, y_pred), annot=True, fmt='3.0f', cmap=\"summer\")\n",
    "\n",
    "# plt.title('SVC Confusion Matrix', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcEn-XsR2_IQ"
   },
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "eYFuSvQn22hA",
    "outputId": "f594602a-1048-4565-f33e-54388be8fde6"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier(\n",
    "    criterion=\"gini\", min_samples_split=10, min_samples_leaf=1, max_features=\"auto\"\n",
    ")\n",
    "\n",
    "dt_classifier.fit(x_train, y_train)\n",
    "\n",
    "dt_prediction = dt_classifier.predict(x_test)\n",
    "\n",
    "print(\"-------------- The Accuracy of the model ----------------------------\")\n",
    "print(\n",
    "    \"The Accuracy of the Decision Tree is\",\n",
    "    round(accuracy_score(y_test, dt_prediction) * 100, 2),\n",
    ")\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "dt_cv_score = cross_val_score(\n",
    "    dt_classifier, training_feature, training_target, cv=kfold, scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"The cross validated score for Decision Tree is:\",\n",
    "    round(dt_cv_score.mean() * 100, 2),\n",
    ")\n",
    "\n",
    "y_pred = cross_val_predict(dt_classifier, training_feature, training_target, cv=kfold)\n",
    "\n",
    "sns.heatmap(\n",
    "    confusion_matrix(training_target, y_pred), annot=True, fmt=\"3.0f\", cmap=\"summer\"\n",
    ")\n",
    "\n",
    "plt.title(\"Decision Tree Confusion Matrix\", y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree HyperParameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "param_grid = {\"max_depth\": [5, 10, 15], \"max_features\": [5, 7, 10]}\n",
    "\n",
    "dt_model = GridSearchCV(\n",
    "    estimator=decision_tree,\n",
    "    param_grid=param_grid,\n",
    "    cv=kfold,\n",
    "    n_jobs=4,\n",
    "    scoring=\"accuracy\",\n",
    ")\n",
    "\n",
    "dt_model.fit(x_train, y_train)\n",
    "\n",
    "# Best score\n",
    "print(dt_model.best_score_)\n",
    "\n",
    "# Best Estimator\n",
    "dt_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecesionTree with Best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier(\n",
    "    class_weight=None,\n",
    "    criterion=\"gini\",\n",
    "    max_depth=5,\n",
    "    max_features=10,\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    presort=False,\n",
    "    random_state=None,\n",
    "    splitter=\"best\",\n",
    ")\n",
    "\n",
    "dt_classifier.fit(x_train, y_train)\n",
    "\n",
    "dt_prediction = dt_classifier.predict(x_test)\n",
    "\n",
    "print(\"-------------- The Accuracy of the model ----------------------------\")\n",
    "print(\n",
    "    \"The Accuracy of the Decision Tree is\",\n",
    "    round(accuracy_score(y_test, dt_prediction) * 100, 2),\n",
    ")\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "dt_cv_score = cross_val_score(\n",
    "    dt_classifier, training_feature, training_target, cv=kfold, scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"The cross validated score for Decision Tree is:\",\n",
    "    round(dt_cv_score.mean() * 100, 2),\n",
    ")\n",
    "\n",
    "y_pred = cross_val_predict(dt_classifier, training_feature, training_target, cv=kfold)\n",
    "\n",
    "sns.heatmap(\n",
    "    confusion_matrix(training_target, y_pred), annot=True, fmt=\"3.0f\", cmap=\"summer\"\n",
    ")\n",
    "\n",
    "plt.title(\"Decision Tree Confusion Matrix\", y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAuRW2CttqbC"
   },
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "78wUiPSBthYj",
    "outputId": "75afc61b-8d1c-49fc-ae99-93419f48b535"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_classifier = AdaBoostClassifier()\n",
    "ada_classifier.fit(x_train, y_train)\n",
    "ada_prediction = ada_classifier.predict(x_test)\n",
    "\n",
    "print(\"-------------- The Accuracy of the model ----------------------------\")\n",
    "print(\n",
    "    \"The Accuracy of the ADA Boost Classifier is\",\n",
    "    round(accuracy_score(y_test, ada_prediction) * 100, 2),\n",
    ")\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "ada_cv_score = cross_val_score(\n",
    "    ada_classifier, training_feature, training_target, cv=kfold, scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"The cross validated score for ADA Boost Classifier is:\",\n",
    "    round(ada_cv_score.mean() * 100, 2),\n",
    ")\n",
    "\n",
    "y_pred = cross_val_predict(ada_classifier, training_feature, training_target, cv=kfold)\n",
    "\n",
    "sns.heatmap(\n",
    "    confusion_matrix(training_target, y_pred), annot=True, fmt=\"3.0f\", cmap=\"summer\"\n",
    ")\n",
    "\n",
    "plt.title(\"ADA Boost Classifier Confusion Matrix\", y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "ada_classifier = AdaBoostClassifier()\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"learning_rate\": [0.001, 0.01, 0.1, 0.2, 0.5],\n",
    "}\n",
    "\n",
    "ada_model = GridSearchCV(\n",
    "    ada_classifier,\n",
    "    param_grid=param_grid,\n",
    "    cv=kfold,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=4,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "ada_model.fit(x_train, y_train)\n",
    "\n",
    "# Best score\n",
    "print(ada_model.best_score_)\n",
    "\n",
    "# Best Estimator\n",
    "ada_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoostClassifier with Best Paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_classifier = AdaBoostClassifier(\n",
    "    algorithm=\"SAMME.R\",\n",
    "    base_estimator=None,\n",
    "    learning_rate=0.5,\n",
    "    n_estimators=200,\n",
    "    random_state=None,\n",
    ")\n",
    "\n",
    "ada_classifier.fit(x_train, y_train)\n",
    "ada_prediction = ada_classifier.predict(x_test)\n",
    "\n",
    "print(\"-------------- The Accuracy of the model ----------------------------\")\n",
    "print(\n",
    "    \"The Accuracy of the ADA Boost Classifier is\",\n",
    "    round(accuracy_score(y_test, ada_prediction) * 100, 2),\n",
    ")\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "ada_cv_score = cross_val_score(\n",
    "    ada_classifier, training_feature, training_target, cv=kfold, scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"The cross validated score for ADA Boost Classifier is:\",\n",
    "    round(ada_cv_score.mean() * 100, 2),\n",
    ")\n",
    "\n",
    "y_pred = cross_val_predict(ada_classifier, training_feature, training_target, cv=kfold)\n",
    "\n",
    "sns.heatmap(\n",
    "    confusion_matrix(training_target, y_pred), annot=True, fmt=\"3.0f\", cmap=\"summer\"\n",
    ")\n",
    "\n",
    "plt.title(\"ADA Boost Classifier Confusion Matrix\", y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5heUWP1wJUL"
   },
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "OhUmguJUwQBs",
    "outputId": "32a844c4-00f7-49f6-f111-5786658f634e"
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda_classifier = LinearDiscriminantAnalysis()\n",
    "\n",
    "print(lda_classifier)\n",
    "\n",
    "lda_classifier.fit(x_train, y_train)\n",
    "\n",
    "lda_prediction = lda_classifier.predict(x_test)\n",
    "\n",
    "print(\"-------------- The Accuracy of the model ----------------------------\")\n",
    "print(\n",
    "    \"The Accuracy of the LDA Classifier is\",\n",
    "    round(accuracy_score(y_test, lda_prediction) * 100, 2),\n",
    ")\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "lda_cv_score = cross_val_score(\n",
    "    lda_classifier, training_feature, training_target, cv=kfold, scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"The cross validated score for LDA Classifier is:\",\n",
    "    round(lda_cv_score.mean() * 100, 2),\n",
    ")\n",
    "\n",
    "y_pred = cross_val_predict(lda_classifier, training_feature, training_target, cv=kfold)\n",
    "\n",
    "sns.heatmap(\n",
    "    confusion_matrix(training_target, y_pred), annot=True, fmt=\"3.0f\", cmap=\"summer\"\n",
    ")\n",
    "\n",
    "plt.title(\"LDA Classifier Confusion Matrix\", y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA HyperParameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lda_classifier = LinearDiscriminantAnalysis()\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    \"solver\": [\"lsqr\", \"eigen\"],\n",
    "    \"shrinkage\": [0, 0.5, 1.0],\n",
    "    \"tol\": [0.0001, 0.001, 0.01, 0.1],\n",
    "}\n",
    "\n",
    "lda_model = GridSearchCV(\n",
    "    lda_classifier,\n",
    "    param_grid=param_grid,\n",
    "    cv=kfold,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=4,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "lda_model.fit(x_train, y_train)\n",
    "\n",
    "# Best score\n",
    "print(lda_model.best_score_)\n",
    "\n",
    "# Best Estimator\n",
    "lda_model.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda_classifier = LinearDiscriminantAnalysis(\n",
    "    n_components=None,\n",
    "    priors=None,\n",
    "    shrinkage=0,\n",
    "    solver=\"lsqr\",\n",
    "    store_covariance=False,\n",
    "    tol=0.0001,\n",
    ")\n",
    "\n",
    "print(lda_classifier)\n",
    "\n",
    "lda_classifier.fit(x_train, y_train)\n",
    "\n",
    "lda_prediction = lda_classifier.predict(x_test)\n",
    "\n",
    "print(\"-------------- The Accuracy of the model ----------------------------\")\n",
    "print(\n",
    "    \"The Accuracy of the LDA Classifier is\",\n",
    "    round(accuracy_score(y_test, lda_prediction) * 100, 2),\n",
    ")\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "lda_cv_score = cross_val_score(\n",
    "    lda_classifier, training_feature, training_target, cv=kfold, scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"The cross validated score for LDA Classifier is:\",\n",
    "    round(lda_cv_score.mean() * 100, 2),\n",
    ")\n",
    "\n",
    "y_pred = cross_val_predict(lda_classifier, training_feature, training_target, cv=kfold)\n",
    "\n",
    "sns.heatmap(\n",
    "    confusion_matrix(training_target, y_pred), annot=True, fmt=\"3.0f\", cmap=\"summer\"\n",
    ")\n",
    "\n",
    "plt.title(\"LDA Classifier Confusion Matrix\", y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qe5q6IHDxWsy"
   },
   "source": [
    "## Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "jwVCpnYtxYhk",
    "outputId": "17728e9e-9bf2-42df-cb10-4b5754e8b51c"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "print(gb_classifier)\n",
    "\n",
    "gb_classifier.fit(x_train, y_train)\n",
    "\n",
    "gb_prediction = gb_classifier.predict(x_test)\n",
    "\n",
    "print(\"-------------- The Accuracy of the model ----------------------------\")\n",
    "print(\n",
    "    \"The Accuracy of the GB Classifier is\",\n",
    "    round(accuracy_score(y_test, gb_prediction) * 100, 2),\n",
    ")\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "gb_cv_score = cross_val_score(\n",
    "    gb_classifier, training_feature, training_target, cv=kfold, scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"The cross validated score for GB Classifier is:\",\n",
    "    round(gb_cv_score.mean() * 100, 2),\n",
    ")\n",
    "\n",
    "y_pred = cross_val_predict(gb_classifier, training_feature, training_target, cv=kfold)\n",
    "\n",
    "sns.heatmap(\n",
    "    confusion_matrix(training_target, y_pred), annot=True, fmt=\"3.0f\", cmap=\"summer\"\n",
    ")\n",
    "\n",
    "plt.title(\"GB Classifier Confusion Matrix\", y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKPCD3nbU3-L"
   },
   "source": [
    "## Gradient Boost HyperParameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "ud7AkaGpU1jc",
    "outputId": "83793b24-7471-41ed-c023-33f96a5d9bb7"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "param_grid = {\n",
    "    \"loss\": [\"deviance\"],\n",
    "    \"n_estimators\": [100, 200, 300, 400],\n",
    "    \"learning_rate\": [0.01, 0.1, 1.0],\n",
    "    \"max_depth\": [4, 8],\n",
    "    \"min_samples_leaf\": [100, 150],\n",
    "    \"max_features\": [5, 10],\n",
    "}\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "gb_model = GridSearchCV(\n",
    "    gb_classifier,\n",
    "    param_grid=param_grid,\n",
    "    cv=kfold,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=4,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "gb_model.fit(x_train, y_train)\n",
    "\n",
    "# Best score\n",
    "print(gb_model.best_score_)\n",
    "\n",
    "# Best Estimator\n",
    "gb_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"Stats\": x_train.columns, \"Feature\": gb_model.best_estimator_.feature_importances_}\n",
    "df = pd.DataFrame(d)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKY5Iv84H-RV"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_classifier = GradientBoostingClassifier(\n",
    "    criterion=\"friedman_mse\",\n",
    "    init=None,\n",
    "    learning_rate=0.1,\n",
    "    loss=\"deviance\",\n",
    "    max_depth=8,\n",
    "    max_features=\"auto\",\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    min_samples_leaf=150,\n",
    "    min_samples_split=2,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    n_estimators=100,\n",
    "    n_iter_no_change=None,\n",
    "    presort=\"auto\",\n",
    "    random_state=None,\n",
    "    subsample=1.0,\n",
    "    tol=0.0001,\n",
    "    validation_fraction=0.1,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    ")\n",
    "\n",
    "print(gb_classifier)\n",
    "\n",
    "gb_classifier.fit(x_train, y_train)\n",
    "\n",
    "gb_prediction = gb_classifier.predict(x_test)\n",
    "\n",
    "print(\"-------------- The Accuracy of the model ----------------------------\")\n",
    "print(\n",
    "    \"The Accuracy of the GB Classifier is\",\n",
    "    round(accuracy_score(y_test, gb_prediction) * 100, 2),\n",
    ")\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "gb_cv_score = cross_val_score(\n",
    "    gb_classifier, training_feature, training_target, cv=kfold, scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"The cross validated score for GB Classifier is:\",\n",
    "    round(gb_cv_score.mean() * 100, 2),\n",
    ")\n",
    "\n",
    "y_pred = cross_val_predict(gb_classifier, training_feature, training_target, cv=kfold)\n",
    "\n",
    "sns.heatmap(\n",
    "    confusion_matrix(training_target, y_pred), annot=True, fmt=\"3.0f\", cmap=\"summer\"\n",
    ")\n",
    "\n",
    "plt.title(\"GB Classifier Confusion Matrix\", y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": [\n",
    "            \"LogisticRegression\",\n",
    "            \"RandomForestClassifier\",\n",
    "            \"DecisionTreeClassifier\",\n",
    "            \"AdaBoostClassifier\",\n",
    "            \"LinearDiscriminantAnalysis\",\n",
    "            \"GradientBoostingClassifier\",\n",
    "        ],\n",
    "        \"Score\": [\n",
    "            lr_cv_score.mean(),\n",
    "            rf_cv_score.mean(),\n",
    "            dt_cv_score.mean(),\n",
    "            ada_cv_score.mean(),\n",
    "            lda_cv_score.mean(),\n",
    "            gb_cv_score.mean(),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "models.sort_values(by=\"Score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.DataFrame(x_train)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "importances = mutual_info_classif(x_train, y_train)\n",
    "feat_importances = pd.Series(importances, x_train.columns[0:len(x_train.columns)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances.plot(kind=\"bar\", color=\"teal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "sel = SelectKBest(mutual_info_classif, k=2).fit(x_train, y_train)\n",
    "x_train.columns[sel.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ig = sel.transform(x_train)\n",
    "x_test_ig = sel.transform(x_test)\n",
    "x_train_ig.shape, x_test_ig.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_classifier = GradientBoostingClassifier(\n",
    "    criterion=\"friedman_mse\",\n",
    "    init=None,\n",
    "    learning_rate=0.1,\n",
    "    loss=\"deviance\",\n",
    "    max_depth=5,\n",
    "    max_features=\"auto\",\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    min_samples_leaf=200,\n",
    "    min_samples_split=2,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    n_estimators=300,\n",
    "    n_iter_no_change=None,\n",
    "    presort=\"auto\",\n",
    "    random_state=None,\n",
    "    subsample=1.0,\n",
    "    tol=0.0001,\n",
    "    validation_fraction=0.1,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    ")\n",
    "\n",
    "print(gb_classifier)\n",
    "\n",
    "gb_classifier.fit(x_train, y_train)\n",
    "\n",
    "gb_prediction = gb_classifier.predict(x_test)\n",
    "\n",
    "print(\"-------------- The Accuracy of the model ----------------------------\")\n",
    "print(\n",
    "    \"The Accuracy of the GB Classifier is\",\n",
    "    round(accuracy_score(y_test, gb_prediction) * 100, 2),\n",
    ")\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "gb_cv_score = cross_val_score(\n",
    "    gb_classifier, training_feature, training_target, cv=kfold, scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"The cross validated score for GB Classifier is:\",\n",
    "    round(gb_cv_score.mean() * 100, 2),\n",
    ")\n",
    "\n",
    "y_pred = cross_val_predict(gb_classifier, training_feature, training_target, cv=kfold)\n",
    "\n",
    "sns.heatmap(\n",
    "    confusion_matrix(training_target, y_pred), annot=True, fmt=\"3.0f\", cmap=\"summer\"\n",
    ")\n",
    "\n",
    "plt.title(\"GB Classifier Confusion Matrix\", y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "importances = chi2(x_train, y_train)\n",
    "feat_importances = pd.Series(importances[1], index=x_train.columns)\n",
    "feat_importances.sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances.plot(kind=\"bar\", color=\"teal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "sel = SelectKBest(chi2, k=5).fit(x_train, y_train)\n",
    "x_train.columns[sel.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_chi = sel.transform(x_train)\n",
    "x_test_chi = sel.transform(x_test)\n",
    "x_train_chi.shape, x_test_chi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_classifier = GradientBoostingClassifier(\n",
    "    criterion=\"friedman_mse\",\n",
    "    init=None,\n",
    "    learning_rate=0.1,\n",
    "    loss=\"deviance\",\n",
    "    max_depth=5,\n",
    "    max_features=\"auto\",\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    min_samples_leaf=200,\n",
    "    min_samples_split=2,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    n_estimators=300,\n",
    "    n_iter_no_change=None,\n",
    "    presort=\"auto\",\n",
    "    random_state=None,\n",
    "    subsample=1.0,\n",
    "    tol=0.0001,\n",
    "    validation_fraction=0.1,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    ")\n",
    "\n",
    "print(gb_classifier)\n",
    "\n",
    "gb_classifier.fit(x_train_chi, y_train)\n",
    "\n",
    "gb_prediction = gb_classifier.predict(x_test_chi)\n",
    "\n",
    "print(\"-------------- The Accuracy of the model ----------------------------\")\n",
    "print(\n",
    "    \"The Accuracy of the GB Classifier is\",\n",
    "    round(accuracy_score(y_test, gb_prediction) * 100, 2),\n",
    ")\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "gb_cv_score = cross_val_score(\n",
    "    gb_classifier, training_feature, training_target, cv=kfold, scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"The cross validated score for GB Classifier is:\",\n",
    "    round(gb_cv_score.mean() * 100, 2),\n",
    ")\n",
    "\n",
    "y_pred = cross_val_predict(gb_classifier, training_feature, training_target, cv=kfold)\n",
    "\n",
    "sns.heatmap(\n",
    "    confusion_matrix(training_target, y_pred), annot=True, fmt=\"3.0f\", cmap=\"summer\"\n",
    ")\n",
    "\n",
    "plt.title(\"GB Classifier Confusion Matrix\", y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_classifier = GradientBoostingClassifier(\n",
    "    criterion=\"friedman_mse\",\n",
    "    init=None,\n",
    "    learning_rate=0.1,\n",
    "    loss=\"deviance\",\n",
    "    max_depth=5,\n",
    "    max_features=\"auto\",\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    min_samples_leaf=200,\n",
    "    min_samples_split=2,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    n_estimators=300,\n",
    "    n_iter_no_change=None,\n",
    "    presort=\"auto\",\n",
    "    random_state=None,\n",
    "    subsample=1.0,\n",
    "    tol=0.0001,\n",
    "    validation_fraction=0.1,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    ")\n",
    "\n",
    "print(gb_classifier)\n",
    "\n",
    "gb_classifier.fit(x_train, y_train)\n",
    "\n",
    "gb_prediction = gb_classifier.predict(x_test)\n",
    "\n",
    "print(\"-------------- The Accuracy of the model ----------------------------\")\n",
    "print(\n",
    "    \"The Accuracy of the GB Classifier is\",\n",
    "    round(accuracy_score(y_test, gb_prediction) * 100, 2),\n",
    ")\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "gb_cv_score = cross_val_score(\n",
    "    gb_classifier, training_feature, training_target, cv=kfold, scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"The cross validated score for GB Classifier is:\",\n",
    "    round(gb_cv_score.mean() * 100, 2),\n",
    ")\n",
    "\n",
    "y_pred = cross_val_predict(gb_classifier, training_feature, training_target, cv=kfold)\n",
    "\n",
    "sns.heatmap(\n",
    "    confusion_matrix(training_target, y_pred), annot=True, fmt=\"3.0f\", cmap=\"summer\"\n",
    ")\n",
    "\n",
    "plt.title(\"GB Classifier Confusion Matrix\", y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\n",
    "    \"data/Tabular Playground Series - Apr 2021/sample_submission.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gb_classifier.predict(org_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df = pd.DataFrame()\n",
    "export_df[\"PassengerId\"] = org_test_pssg_id\n",
    "export_df[\"Survived\"] = y_pred\n",
    "export_df.to_csv(\n",
    "    \"data/Tabular Playground Series - Apr 2021/my_gb_2_classifier_submission.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "x_pca = pca.fit_transform(x_train)\n",
    "x_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(x_pca[:, 0], x_pca[:, 1], c=y_train, cmap=\"viridis\")\n",
    "plt.xlabel(\"First Principal Component\")\n",
    "plt.ylabel(\"Second Principal Component\")\n",
    "plt.title(\"Scatter plot for Second principal component and First principal component\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential, regularizers  # Importing sequential model\n",
    "from tensorflow.keras.layers import (\n",
    "    BatchNormalization,  # Importing layers\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ig.shape[1], x_test_ig.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annModel(x_train, y_train, x_test, y_test, batch, epochs):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(\n",
    "        Dense(\n",
    "            x_train.shape[1],\n",
    "            activation=\"relu\",\n",
    "            input_dim=x_train.shape[1],\n",
    "            kernel_regularizer=regularizers.l2(0.01),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.add(Dense(5, activation=\"relu\"))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(5, activation=\"relu\"))\n",
    "\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer=\"Adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=batch,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        validation_data=(x_test, y_test),\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annModel(x_train_ig, y_train, x_test_ig, y_test, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annModel(x_train_chi, y_train, x_test_chi, y_test, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[\"male\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ann_model = annModel(x_train, y_train, x_test, y_test, 10, 10)\n",
    "ann_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ann_model.predict_classes(org_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_test_pssg_id.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\n",
    "    \"data/Tabular Playground Series - Apr 2021/sample_submission.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df = pd.DataFrame()\n",
    "export_df[\"PassengerId\"] = org_test_pssg_id\n",
    "export_df[\"Survived\"] = y_pred\n",
    "export_df.to_csv(\n",
    "    \"data/Tabular Playground Series - Apr 2021/my_submission.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tabular Playground Series - Apr 2021.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
