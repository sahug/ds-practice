{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenVaccine - COVID-19 mRNA Vaccine Degradation Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this competition, you will be predicting the degradation rates at various locations along RNA sequence.\n",
    "\n",
    "There are multiple ground truth values provided in the training data. While the submission format requires all 5 to be predicted, only the following are scored: reactivity, deg_Mg_pH10, and deg_Mg_50C.\n",
    "\n",
    "#### Files\n",
    " - `train.json` - the training data\n",
    " - `test.json` - the test set, without any columns associated with the ground truth.\n",
    " - `sample_submission.csv` - a sample submission file in the correct format\n",
    "\n",
    "#### Columns\n",
    " - `id` - An arbitrary identifier for each sample.\n",
    " \n",
    " \n",
    " - `seq_scored` - `(68 in Train and Public Test, 91 in Private Test)` Integer value denoting the number of positions used in scoring with predicted values. This should match the length of `reactivity, deg_* and *_error_*` columns. Note that molecules used for the Private Test will be longer than those in the Train and Public Test data, so the size of this vector will be different.\n",
    " \n",
    " \n",
    " - `seq_length` - `(107 in Train and Public Test, 130 in Private Test)` Integer values, denotes the length of sequence. `Note that molecules used for the Private Test will be longer than those in the Train and Public Test data, so the size of this vector will be different`.\n",
    " \n",
    " \n",
    " - `sequence` - `(1x107 string in Train and Public Test, 130 in Private Test)` Describes the RNA sequence, a combination of `A, G, U, and C` for each sample. Should be 107 characters long, and the first 68 bases should correspond to the 68 positions specified in `seq_scored` (note: indexed starting at 0).\n",
    " \n",
    " \n",
    " - `structure` - `(1x107 string in Train and Public Test, 130 in Private Test)` An array of `(, ), and .` characters that describe whether a base is estimated to be paired or unpaired. Paired bases are denoted by opening and closing parentheses `e.g. (....) means that base 0 is paired to base 5, and bases 1-4 are unpaired`.\n",
    " \n",
    " \n",
    " - `reactivity` - `(1x68 vector in Train and Public Test, 1x91 in Private Test)` An array of floating point numbers, should have the same length as `seq_scored`. These numbers are reactivity values for the first 68 bases as denoted in sequence, and used to determine the likely secondary structure of the RNA sample.\n",
    " \n",
    " \n",
    " - `deg_pH10` - `(1x68 vector in Train and Public Test, 1x91 in Private Test)` An array of floating point numbers, should have the same length as `seq_scored`. These numbers are reactivity values for the first 68 bases as denoted in sequence, and used to determine the likelihood of degradation at the base/linkage after incubating without magnesium at high pH (pH 10).\n",
    " \n",
    " \n",
    " - `deg_Mg_pH10` - `(1x68 vector in Train and Public Test, 1x91 in Private Test)` An array of floating point numbers, should have the same length as seq_scored. These numbers are reactivity values for the first 68 bases as denoted in sequence, and used to determine the likelihood of degradation at the base/linkage after incubating with magnesium in high pH (pH 10).\n",
    " \n",
    " \n",
    " - `deg_50C` - `(1x68 vector in Train and Public Test, 1x91 in Private Test)` An array of floating point numbers, should have the same length as `seq_scored`. These numbers are reactivity values for the first 68 bases as denoted in `sequence`, and used to determine the likelihood of degradation at the base/linkage after incubating without magnesium at high temperature (50 degrees Celsius).\n",
    " \n",
    " \n",
    " - `deg_Mg_50C` - `(1x68 vector in Train and Public Test, 1x91 in Private Test)` An array of floating point numbers, should have the same length as `seq_scored`. These numbers are reactivity values for the first 68 bases as denoted in sequence, and used to determine the likelihood of degradation at the base/linkage after incubating with magnesium at high temperature (50 degrees Celsius).\n",
    " \n",
    " \n",
    " - `*_error_*` - An array of floating point numbers, should have the same length as the corresponding reactivity or `deg_* `columns, calculated errors in experimental values obtained in reactivity and `deg_*` columns.\n",
    " \n",
    " \n",
    " - `predicted_loop_type` - `(1x107 string)` Describes the structural context `(also referred to as loop type)` of each character in sequence. Loop types assigned by `bpRNA` from `Vienna RNAfold` 2 structure. \n",
    " \n",
    " From the `bpRNA_documentation`:   \n",
    " \n",
    "  - `S`: paired \"Stem\" \n",
    "  - `M`: Multiloop \n",
    "  - `I`: Internal loop \n",
    "  - `B`: Bulge \n",
    "  - `H`: Hairpin loop \n",
    "  - `E`: dangling End \n",
    "  - `X`: eXternal loop\n",
    "  - `S/N` filter Indicates if the sample passed filters described below in Additional Notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Notes\n",
    "At the beginning of the competition, Stanford scientists have data on 3029 RNA `sequences` of length 107. For technical reasons, measurements cannot be carried out on the final bases of these RNA `sequences`, so we have experimental data (ground truth) in 5 conditions for the first 68 bases.\n",
    "\n",
    "We have split out 629 of these 3029 sequences for a public test set to allow for continuous evaluation through the competition, on the public leaderboard. These sequences, in `test.json`, have been additionally filtered based on three criteria detailed below to ensure that this subset is not dominated by any large cluster of RNA molecules with poor data, which might bias the public leaderboard. The remaining 2400 sequences for which we have data are in `train.json`.\n",
    "\n",
    "For our final and most important scoring (the Private Leaderbooard), Stanford scientists are carrying out measurements on 3005 new `RNAs`, which have somewhat longer lengths of 130 bases. For these data, we expect to have measurements for the first 91 bases, again missing the ends of the RNA. These sequences constitute another 3005 of the 3634 sequences in `test.json`.\n",
    "\n",
    "For those interested in how the 629 107-base sequences in `test.json` were filtered, here were the steps to ensure a diverse and high quality test set for public leaderboard scoring:\n",
    "\n",
    "Minimum value across all 5 conditions must be greater than -0.5.\n",
    "\n",
    "Mean signal/noise across all 5 conditions must be greater than 1.0. [Signal/noise is defined as mean( measurement value over 68 nts )/mean( statistical error in measurement value over 68 nts)]\n",
    "\n",
    "To help ensure sequence diversity, the resulting sequences were clustered into clusters with less than 50% sequence similarity, and the 629 test set sequences were chosen from clusters with 3 or fewer members. That is, any sequence in the test set should be sequence similar to at most 2 other `sequences`.\n",
    "\n",
    "Note that these filters have not been applied to the 2400 RNAs in the public training data `train.json` \u2014 some of those measurements have negative values or poor `signal-to-noise`, or some `RNA` sequences have near-identical sequences in that set. But we are providing all those data in case competitors can squeeze out more signal.\n",
    "\n",
    "After discussion, the three filters noted above will be applied to Private Test on 3005 sequences, and predictions on sequences that do not pass the filters will not be included in scoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Scheme\n",
    "(1) train denoising auto encoder model using all data including train and test data\n",
    "\n",
    "(2) from the weights of denoising auto encoder model, finetune to predict targets such as reactivity\n",
    "rough network architecture\n",
    "\n",
    "`inputs -> conv1ds -> aggregation of neighborhoods -> multi head attention -> aggregation of neighborhoods -> multi head attention -> conv1d -> predict`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_dir = None # model dir for resuming training. if None, train from scrach\n",
    "\n",
    "one_fold = False # if True, train model at only first fold. use if you try a new idea quickly.\n",
    "run_test = False # if True, use small data. you can check whether this code run or not\n",
    "denoise = True # if True, use train data whose signal_to_noise > 1\n",
    "\n",
    "ae_epochs = 20 # epoch of training of denoising auto encoder\n",
    "ae_epochs_each = 5 # epoch of training of denoising auto encoder each time. \n",
    "                   # I use train data (seqlen = 107) and private test data (seqlen = 130) for auto encoder training.\n",
    "                   # I dont know how to easily fit keras model to use both of different shape data simultaneously, \n",
    "                   # so I call fit function several times. \n",
    "ae_batch_size = 32\n",
    "\n",
    "epochs_list = [30, 10, 3, 3, 5, 5]\n",
    "batch_size_list = [8, 16, 32, 64, 128, 256] \n",
    "\n",
    "## copy pretrain model to working dir\n",
    "import shutil\n",
    "import glob\n",
    "if pretrain_dir is not None:\n",
    "    for d in glob.glob(pretrain_dir + \"*\"):\n",
    "        shutil.copy(d, \".\")\n",
    "    \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import gc\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "train = pd.read_json(\"data/stanford-covid-vaccine/train.json\", lines=True)\n",
    "if denoise:\n",
    "    train = train[train.signal_to_noise > 1].reset_index(drop=True)\n",
    "test = pd.read_json(\"data/stanford-covid-vaccine/test.json\", lines=True)\n",
    "test_pub = test[test[\"seq_length\"] == 107]\n",
    "test_pri = test[test[\"seq_length\"] == 130]\n",
    "sub = pd.read_csv(\"data/stanford-covid-vaccine/sample_submission.csv\")\n",
    "\n",
    "if run_test:  ## to test\n",
    "    train = train[:30]\n",
    "    test_pub = test_pub[:30]\n",
    "    test_pri = test_pri[:30]\n",
    "\n",
    "As = []\n",
    "\n",
    "for id in tqdm(train[\"id\"]):\n",
    "    a = np.load(f\"data/stanford-covid-vaccine/bpps/{id}.npy\")\n",
    "    As.append(a)\n",
    "\n",
    "As = np.array(As)\n",
    "\n",
    "As_pub = []\n",
    "\n",
    "for id in tqdm(test_pub[\"id\"]):\n",
    "    a = np.load(f\"data/stanford-covid-vaccine/bpps/{id}.npy\")\n",
    "    As_pub.append(a)\n",
    "\n",
    "As_pub = np.array(As_pub)\n",
    "\n",
    "As_pri = []\n",
    "\n",
    "for id in tqdm(test_pri[\"id\"]):\n",
    "    a = np.load(f\"data/stanford-covid-vaccine/bpps/{id}.npy\")\n",
    "    As_pri.append(a)\n",
    "\n",
    "As_pri = np.array(As_pri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sub.shape)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "targets = list(sub.columns[1:])\n",
    "print(targets)\n",
    "\n",
    "y_train = []\n",
    "seq_len = train[\"seq_length\"].iloc[0]\n",
    "seq_len_target = train[\"seq_scored\"].iloc[0]\n",
    "ignore = -10000\n",
    "ignore_length = seq_len - seq_len_target\n",
    "for target in targets:\n",
    "    y = np.vstack(train[target])\n",
    "    dummy = np.zeros([y.shape[0], ignore_length]) + ignore\n",
    "    y = np.hstack([y, dummy])\n",
    "    y_train.append(y)\n",
    "y = np.stack(y_train, axis=2)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_structure_adj(train):\n",
    "    ## get adjacent matrix from structure sequence\n",
    "\n",
    "    ## here I calculate adjacent matrix of each base pair,\n",
    "    ## but eventually ignore difference of base pair and integrate into one matrix\n",
    "    Ss = []\n",
    "    for i in tqdm(range(len(train))):\n",
    "        seq_length = train[\"seq_length\"].iloc[i]\n",
    "        structure = train[\"structure\"].iloc[i]\n",
    "        sequence = train[\"sequence\"].iloc[i]\n",
    "\n",
    "        cue = []\n",
    "        a_structures = {\n",
    "            (\"A\", \"U\"): np.zeros([seq_length, seq_length]),\n",
    "            (\"C\", \"G\"): np.zeros([seq_length, seq_length]),\n",
    "            (\"U\", \"G\"): np.zeros([seq_length, seq_length]),\n",
    "            (\"U\", \"A\"): np.zeros([seq_length, seq_length]),\n",
    "            (\"G\", \"C\"): np.zeros([seq_length, seq_length]),\n",
    "            (\"G\", \"U\"): np.zeros([seq_length, seq_length]),\n",
    "        }\n",
    "        a_structure = np.zeros([seq_length, seq_length])\n",
    "        for i in range(seq_length):\n",
    "            if structure[i] == \"(\":\n",
    "                cue.append(i)\n",
    "            elif structure[i] == \")\":\n",
    "                start = cue.pop()\n",
    "                #                 a_structure[start, i] = 1\n",
    "                #                 a_structure[i, start] = 1\n",
    "                a_structures[(sequence[start], sequence[i])][start, i] = 1\n",
    "                a_structures[(sequence[i], sequence[start])][i, start] = 1\n",
    "\n",
    "        a_strc = np.stack([a for a in a_structures.values()], axis=2)\n",
    "        a_strc = np.sum(a_strc, axis=2, keepdims=True)\n",
    "        Ss.append(a_strc)\n",
    "\n",
    "    Ss = np.array(Ss)\n",
    "    print(Ss.shape)\n",
    "    return Ss\n",
    "\n",
    "\n",
    "Ss = get_structure_adj(train)\n",
    "Ss_pub = get_structure_adj(test_pub)\n",
    "Ss_pri = get_structure_adj(test_pri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_matrix(As):\n",
    "    ## adjacent matrix based on distance on the sequence\n",
    "    ## D[i, j] = 1 / (abs(i - j) + 1) ** pow, pow = 1, 2, 4\n",
    "\n",
    "    idx = np.arange(As.shape[1])\n",
    "    Ds = []\n",
    "    for i in range(len(idx)):\n",
    "        d = np.abs(idx[i] - idx)\n",
    "        Ds.append(d)\n",
    "\n",
    "    Ds = np.array(Ds) + 1\n",
    "    Ds = 1 / Ds\n",
    "    Ds = Ds[None, :, :]\n",
    "    Ds = np.repeat(Ds, len(As), axis=0)\n",
    "\n",
    "    Dss = []\n",
    "    for i in [1, 2, 4]:\n",
    "        Dss.append(Ds ** i)\n",
    "    Ds = np.stack(Dss, axis=3)\n",
    "    print(Ds.shape)\n",
    "    return Ds\n",
    "\n",
    "\n",
    "Ds = get_distance_matrix(As)\n",
    "Ds_pub = get_distance_matrix(As_pub)\n",
    "Ds_pri = get_distance_matrix(As_pri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## concat adjecent\n",
    "As = np.concatenate([As[:, :, :, None], Ss, Ds], axis=3).astype(np.float32)\n",
    "As_pub = np.concatenate([As_pub[:, :, :, None], Ss_pub, Ds_pub], axis=3).astype(\n",
    "    np.float32\n",
    ")\n",
    "As_pri = np.concatenate([As_pri[:, :, :, None], Ss_pri, Ds_pri], axis=3).astype(\n",
    "    np.float32\n",
    ")\n",
    "del Ss, Ds, Ss_pub, Ds_pub, Ss_pri, Ds_pri\n",
    "As.shape, As_pub.shape, As_pri.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sequence\n",
    "def return_ohe(n, i):\n",
    "    tmp = [0] * n\n",
    "    tmp[i] = 1\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def get_input(train):\n",
    "    ## get node features, which is one hot encoded\n",
    "    mapping = {}\n",
    "    vocab = [\"A\", \"G\", \"C\", \"U\"]\n",
    "    for i, s in enumerate(vocab):\n",
    "        mapping[s] = return_ohe(len(vocab), i)\n",
    "    X_node = np.stack(\n",
    "        train[\"sequence\"].apply(lambda x: list(map(lambda y: mapping[y], list(x))))\n",
    "    )\n",
    "\n",
    "    mapping = {}\n",
    "    vocab = [\"S\", \"M\", \"I\", \"B\", \"H\", \"E\", \"X\"]\n",
    "    for i, s in enumerate(vocab):\n",
    "        mapping[s] = return_ohe(len(vocab), i)\n",
    "    X_loop = np.stack(\n",
    "        train[\"predicted_loop_type\"].apply(\n",
    "            lambda x: list(map(lambda y: mapping[y], list(x)))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    mapping = {}\n",
    "    vocab = [\".\", \"(\", \")\"]\n",
    "    for i, s in enumerate(vocab):\n",
    "        mapping[s] = return_ohe(len(vocab), i)\n",
    "    X_structure = np.stack(\n",
    "        train[\"structure\"].apply(lambda x: list(map(lambda y: mapping[y], list(x))))\n",
    "    )\n",
    "\n",
    "    X_node = np.concatenate([X_node, X_loop], axis=2)\n",
    "\n",
    "    ## interaction\n",
    "    a = np.sum(X_node * (2 ** np.arange(X_node.shape[2])[None, None, :]), axis=2)\n",
    "    vocab = sorted(set(a.flatten()))\n",
    "    print(vocab)\n",
    "    ohes = []\n",
    "    for v in vocab:\n",
    "        ohes.append(a == v)\n",
    "    ohes = np.stack(ohes, axis=2)\n",
    "    X_node = np.concatenate([X_node, ohes], axis=2).astype(np.float32)\n",
    "\n",
    "    print(X_node.shape)\n",
    "    return X_node\n",
    "\n",
    "\n",
    "X_node = get_input(train)\n",
    "X_node_pub = get_input(test_pub)\n",
    "X_node_pri = get_input(test_pri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers as L\n",
    "\n",
    "\n",
    "def mcrmse(t, p, seq_len_target=seq_len_target):\n",
    "    ## calculate mcrmse score by using numpy\n",
    "    t = t[:, :seq_len_target]\n",
    "    p = p[:, :seq_len_target]\n",
    "\n",
    "    score = np.mean(np.sqrt(np.mean(np.mean((p - t) ** 2, axis=1), axis=0)))\n",
    "    return score\n",
    "\n",
    "\n",
    "def mcrmse_loss(t, y, seq_len_target=seq_len_target):\n",
    "    ## calculate mcrmse score by using tf\n",
    "    t = t[:, :seq_len_target]\n",
    "    y = y[:, :seq_len_target]\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.sqrt(tf.reduce_mean(tf.reduce_mean((t - y) ** 2, axis=1), axis=0))\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "\n",
    "def attention(x_inner, x_outer, n_factor, dropout):\n",
    "    x_Q = L.Conv1D(\n",
    "        n_factor,\n",
    "        1,\n",
    "        activation=\"linear\",\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        bias_initializer=\"glorot_uniform\",\n",
    "    )(x_inner)\n",
    "    x_K = L.Conv1D(\n",
    "        n_factor,\n",
    "        1,\n",
    "        activation=\"linear\",\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        bias_initializer=\"glorot_uniform\",\n",
    "    )(x_outer)\n",
    "    x_V = L.Conv1D(\n",
    "        n_factor,\n",
    "        1,\n",
    "        activation=\"linear\",\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        bias_initializer=\"glorot_uniform\",\n",
    "    )(x_outer)\n",
    "    x_KT = L.Permute((2, 1))(x_K)\n",
    "    res = L.Lambda(lambda c: K.batch_dot(c[0], c[1]) / np.sqrt(n_factor))([x_Q, x_KT])\n",
    "    #     res = tf.expand_dims(res, axis = 3)\n",
    "    #     res = L.Conv2D(16, 3, 1, padding = \"same\", activation = \"relu\")(res)\n",
    "    #     res = L.Conv2D(1, 3, 1, padding = \"same\", activation = \"relu\")(res)\n",
    "    #     res = tf.squeeze(res, axis = 3)\n",
    "    att = L.Lambda(lambda c: K.softmax(c, axis=-1))(res)\n",
    "    att = L.Lambda(lambda c: K.batch_dot(c[0], c[1]))([att, x_V])\n",
    "    return att\n",
    "\n",
    "\n",
    "def multi_head_attention(x, y, n_factor, n_head, dropout):\n",
    "    if n_head == 1:\n",
    "        att = attention(x, y, n_factor, dropout)\n",
    "    else:\n",
    "        n_factor_head = n_factor // n_head\n",
    "        heads = [attention(x, y, n_factor_head, dropout) for i in range(n_head)]\n",
    "        att = L.Concatenate()(heads)\n",
    "        att = L.Dense(\n",
    "            n_factor,\n",
    "            kernel_initializer=\"glorot_uniform\",\n",
    "            bias_initializer=\"glorot_uniform\",\n",
    "        )(att)\n",
    "    x = L.Add()([x, att])\n",
    "    x = L.LayerNormalization()(x)\n",
    "    if dropout > 0:\n",
    "        x = L.Dropout(dropout)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def res(x, unit, kernel=3, rate=0.1):\n",
    "    h = L.Conv1D(unit, kernel, 1, padding=\"same\", activation=None)(x)\n",
    "    h = L.LayerNormalization()(h)\n",
    "    h = L.LeakyReLU()(h)\n",
    "    h = L.Dropout(rate)(h)\n",
    "    return L.Add()([x, h])\n",
    "\n",
    "\n",
    "def forward(x, unit, kernel=3, rate=0.1):\n",
    "    #     h = L.Dense(unit, None)(x)\n",
    "    h = L.Conv1D(unit, kernel, 1, padding=\"same\", activation=None)(x)\n",
    "    h = L.LayerNormalization()(h)\n",
    "    h = L.Dropout(rate)(h)\n",
    "    #         h = tf.keras.activations.swish(h)\n",
    "    h = L.LeakyReLU()(h)\n",
    "    h = res(h, unit, kernel, rate)\n",
    "    return h\n",
    "\n",
    "\n",
    "def adj_attn(x, adj, unit, n=2, rate=0.1):\n",
    "    x_a = x\n",
    "    x_as = []\n",
    "    for i in range(n):\n",
    "        x_a = forward(x_a, unit)\n",
    "        x_a = tf.matmul(adj, x_a)  ## aggregate neighborhoods\n",
    "        x_as.append(x_a)\n",
    "    if n == 1:\n",
    "        x_a = x_as[0]\n",
    "    else:\n",
    "        x_a = L.Concatenate()(x_as)\n",
    "    x_a = forward(x_a, unit)\n",
    "    return x_a\n",
    "\n",
    "\n",
    "def get_base(config):\n",
    "    ## base model architecture\n",
    "    ## node, adj -> middle feature\n",
    "\n",
    "    node = tf.keras.Input(shape=(None, X_node.shape[2]), name=\"node\")\n",
    "    adj = tf.keras.Input(shape=(None, None, As.shape[3]), name=\"adj\")\n",
    "\n",
    "    adj_learned = L.Dense(1, \"relu\")(adj)\n",
    "    adj_all = L.Concatenate(axis=3)([adj, adj_learned])\n",
    "\n",
    "    xs = []\n",
    "    xs.append(node)\n",
    "    x1 = forward(node, 128, kernel=3, rate=0.0)\n",
    "    x2 = forward(x1, 64, kernel=6, rate=0.0)\n",
    "    x3 = forward(x2, 32, kernel=15, rate=0.0)\n",
    "    x4 = forward(x3, 16, kernel=30, rate=0.0)\n",
    "    x = L.Concatenate()([x1, x2, x3, x4])\n",
    "\n",
    "    for unit in [64, 32]:\n",
    "        x_as = []\n",
    "        for i in range(adj_all.shape[3]):\n",
    "            x_a = adj_attn(x, adj_all[:, :, :, i], unit, rate=0.0)\n",
    "            x_as.append(x_a)\n",
    "        x_c = forward(x, unit, kernel=30)\n",
    "\n",
    "        x = L.Concatenate()(x_as + [x_c])\n",
    "        x = forward(x, unit)\n",
    "        x = multi_head_attention(x, x, unit, 4, 0.0)\n",
    "        xs.append(x)\n",
    "\n",
    "    x = L.Concatenate()(xs)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[node, adj], outputs=[x])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_ae_model(base, config):\n",
    "    ## denoising auto encoder part\n",
    "    ## node, adj -> middle feature -> node\n",
    "\n",
    "    node = tf.keras.Input(shape=(None, X_node.shape[2]), name=\"node\")\n",
    "    adj = tf.keras.Input(shape=(None, None, As.shape[3]), name=\"adj\")\n",
    "\n",
    "    x = base([L.SpatialDropout1D(0.3)(node), adj])\n",
    "    x = forward(x, 64, rate=0.3)\n",
    "    p = L.Dense(X_node.shape[2], \"sigmoid\")(x)\n",
    "\n",
    "    loss = -tf.reduce_mean(\n",
    "        20 * node * tf.math.log(p + 1e-4) + (1 - node) * tf.math.log(1 - p + 1e-4)\n",
    "    )\n",
    "    model = tf.keras.Model(inputs=[node, adj], outputs=[loss])\n",
    "\n",
    "    opt = get_optimizer()\n",
    "    model.compile(optimizer=opt, loss=lambda t, y: y)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model(base, config):\n",
    "    ## regression part\n",
    "    ## node, adj -> middle feature -> prediction of targets\n",
    "\n",
    "    node = tf.keras.Input(shape=(None, X_node.shape[2]), name=\"node\")\n",
    "    adj = tf.keras.Input(shape=(None, None, As.shape[3]), name=\"adj\")\n",
    "\n",
    "    x = base([node, adj])\n",
    "    x = forward(x, 128, rate=0.4)\n",
    "    x = L.Dense(5, None)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[node, adj], outputs=[x])\n",
    "\n",
    "    opt = get_optimizer()\n",
    "    model.compile(optimizer=opt, loss=mcrmse_loss)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_optimizer():\n",
    "    sgd = tf.keras.optimizers.SGD(0.05, momentum=0.9, nesterov=True)\n",
    "    adam = tf.optimizers.Adam()\n",
    "    radam = tfa.optimizers.RectifiedAdam()\n",
    "    lookahead = tfa.optimizers.Lookahead(adam, sync_period=6)\n",
    "    swa = tfa.optimizers.SWA(adam)\n",
    "    return adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here train denoising auto encoder model using all data\n",
    "\n",
    "config = {}  ## not use now\n",
    "if ae_epochs > 0:\n",
    "    base = get_base(config)\n",
    "    ae_model = get_ae_model(base, config)\n",
    "    ## TODO : simultaneous train\n",
    "    for i in range(ae_epochs // ae_epochs_each):\n",
    "        print(f\"------ {i} ------\")\n",
    "        print(\"--- train ---\")\n",
    "        ae_model.fit(\n",
    "            [X_node, As],\n",
    "            [X_node[:, 0]],\n",
    "            epochs=ae_epochs_each,\n",
    "            batch_size=ae_batch_size,\n",
    "        )\n",
    "        print(\"--- public ---\")\n",
    "        ae_model.fit(\n",
    "            [X_node_pub, As_pub],\n",
    "            [X_node_pub[:, 0]],\n",
    "            epochs=ae_epochs_each,\n",
    "            batch_size=ae_batch_size,\n",
    "        )\n",
    "        print(\"--- private ---\")\n",
    "        ae_model.fit(\n",
    "            [X_node_pri, As_pri],\n",
    "            [X_node_pri[:, 0]],\n",
    "            epochs=ae_epochs_each,\n",
    "            batch_size=ae_batch_size,\n",
    "        )\n",
    "        gc.collect()\n",
    "    print(\"****** save ae model ******\")\n",
    "    base.save_weights(\"./base_ae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## here train regression model from pretrain auto encoder model\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(5, shuffle=True, random_state=42)\n",
    "\n",
    "scores = []\n",
    "preds = np.zeros([len(X_node), X_node.shape[1], 5])\n",
    "for i, (tr_idx, va_idx) in enumerate(kfold.split(X_node, As)):\n",
    "    print(f\"------ fold {i} start -----\")\n",
    "    print(f\"------ fold {i} start -----\")\n",
    "    print(f\"------ fold {i} start -----\")\n",
    "    X_node_tr = X_node[tr_idx]\n",
    "    X_node_va = X_node[va_idx]\n",
    "    As_tr = As[tr_idx]\n",
    "    As_va = As[va_idx]\n",
    "    y_tr = y[tr_idx]\n",
    "    y_va = y[va_idx]\n",
    "\n",
    "    base = get_base(config)\n",
    "    if ae_epochs > 0:\n",
    "        print(\"****** load ae model ******\")\n",
    "        base.load_weights(\"./base_ae\")\n",
    "    model = get_model(base, config)\n",
    "    if pretrain_dir is not None:\n",
    "        d = f\"./model{i}\"\n",
    "        print(f\"--- load from {d} ---\")\n",
    "        model.load_weights(d)\n",
    "    for epochs, batch_size in zip(epochs_list, batch_size_list):\n",
    "        print(f\"epochs : {epochs}, batch_size : {batch_size}\")\n",
    "        model.fit(\n",
    "            [X_node_tr, As_tr],\n",
    "            [y_tr],\n",
    "            validation_data=([X_node_va, As_va], [y_va]),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_freq=3,\n",
    "        )\n",
    "\n",
    "    model.save_weights(f\"./model{i}\")\n",
    "    p = model.predict([X_node_va, As_va])\n",
    "    scores.append(mcrmse(y_va, p))\n",
    "    print(f\"fold {i}: mcrmse {scores[-1]}\")\n",
    "    preds[va_idx] = p\n",
    "    if one_fold:\n",
    "        break\n",
    "\n",
    "pd.to_pickle(preds, \"oof.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pub = 0\n",
    "p_pri = 0\n",
    "for i in range(5):\n",
    "    model.load_weights(f\"./model{i}\")\n",
    "    p_pub += model.predict([X_node_pub, As_pub]) / 5\n",
    "    p_pri += model.predict([X_node_pri, As_pri]) / 5\n",
    "    if one_fold:\n",
    "        p_pub *= 5\n",
    "        p_pri *= 5\n",
    "        break\n",
    "\n",
    "for i, target in enumerate(targets):\n",
    "    test_pub[target] = [list(p_pub[k, :, i]) for k in range(p_pub.shape[0])]\n",
    "    test_pri[target] = [list(p_pri[k, :, i]) for k in range(p_pri.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_ls = []\n",
    "for df, preds in [(test_pub, p_pub), (test_pri, p_pri)]:\n",
    "    for i, uid in enumerate(df.id):\n",
    "        single_pred = preds[i]\n",
    "\n",
    "        single_df = pd.DataFrame(single_pred, columns=targets)\n",
    "        single_df[\"id_seqpos\"] = [f\"{uid}_{x}\" for x in range(single_df.shape[0])]\n",
    "\n",
    "        preds_ls.append(single_df)\n",
    "\n",
    "preds_df = pd.concat(preds_ls)\n",
    "preds_df.to_csv(\"submission.csv\", index=False)\n",
    "preds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)\n",
    "print(np.mean(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
