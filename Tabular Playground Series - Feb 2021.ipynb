{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file and print head\n",
    "train_ds = pd.read_csv(\"data/Tabular Playground Series - Feb 2021/train.csv\")\n",
    "train_ds.drop([\"id\"], axis=1, inplace=True)\n",
    "train_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get table info\n",
    "train_ds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get count of data by column\n",
    "train_ds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "train_ds.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View target histogram\n",
    "train_ds[\"target\"].plot(kind=\"hist\", bins=30)\n",
    "plt.title(\"Target - Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for data visualization\n",
    "train_ds[\"target\"].plot(kind=\"box\")\n",
    "plt.title(\"Target - Boxplot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers\n",
    "outliers = train_ds[(train_ds[\"target\"] < 5) | (train_ds[\"target\"] > 10)]\n",
    "print(len(outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "train_ds = train_ds[(train_ds[\"target\"] > 5) & (train_ds[\"target\"] < 10)]\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recheck outliers\n",
    "train_ds[\"target\"].plot(kind=\"box\")\n",
    "plt.title(\"Target - Boxplot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all columns with float values. Exclude Strings to plot histogram.\n",
    "feature_list_num = [\n",
    "    \"cont0\",\n",
    "    \"cont1\",\n",
    "    \"cont2\",\n",
    "    \"cont3\",\n",
    "    \"cont4\",\n",
    "    \"cont5\",\n",
    "    \"cont6\",\n",
    "    \"cont7\",\n",
    "    \"cont8\",\n",
    "    \"cont9\",\n",
    "    \"cont10\",\n",
    "    \"cont11\",\n",
    "    \"cont12\",\n",
    "    \"cont13\",\n",
    "]\n",
    "feature_list_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for each numeric feature\n",
    "\n",
    "fig, axs = plt.subplots(5, 3)\n",
    "fig.set_size_inches(8, 8)\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "axs[0, 0].hist(train_ds[feature_list_num[0]], bins=100)  # First histogram at 0x0\n",
    "axs[0, 0].set_title(f\"{feature_list_num[0]}\")\n",
    "\n",
    "axs[0, 1].hist(train_ds[feature_list_num[1]], bins=100)  # Second histogram at 0x1\n",
    "axs[0, 1].set_title(f\"{feature_list_num[1]}\")\n",
    "\n",
    "axs[0, 2].hist(train_ds[feature_list_num[2]], bins=100)\n",
    "axs[0, 2].set_title(f\"{feature_list_num[2]}\")\n",
    "\n",
    "axs[1, 0].hist(train_ds[feature_list_num[3]], bins=100)\n",
    "axs[1, 0].set_title(f\"{feature_list_num[3]}\")\n",
    "\n",
    "axs[1, 1].hist(train_ds[feature_list_num[4]], bins=100)\n",
    "axs[1, 1].set_title(f\"{feature_list_num[4]}\")\n",
    "\n",
    "axs[1, 2].hist(train_ds[feature_list_num[5]], bins=100)\n",
    "axs[1, 2].set_title(f\"{feature_list_num[5]}\")\n",
    "\n",
    "axs[2, 0].hist(train_ds[feature_list_num[6]], bins=100)\n",
    "axs[2, 0].set_title(f\"{feature_list_num[6]}\")\n",
    "\n",
    "axs[2, 1].hist(train_ds[feature_list_num[7]], bins=100)\n",
    "axs[2, 1].set_title(f\"{feature_list_num[7]}\")\n",
    "\n",
    "axs[2, 2].hist(train_ds[feature_list_num[8]], bins=100)\n",
    "axs[2, 2].set_title(f\"{feature_list_num[8]}\")\n",
    "\n",
    "axs[3, 0].hist(train_ds[feature_list_num[9]], bins=100)\n",
    "axs[3, 0].set_title(f\"{feature_list_num[9]}\")\n",
    "\n",
    "axs[3, 1].hist(train_ds[feature_list_num[10]], bins=100)\n",
    "axs[3, 1].set_title(f\"{feature_list_num[10]}\")\n",
    "\n",
    "axs[3, 2].hist(train_ds[feature_list_num[11]], bins=100)\n",
    "axs[3, 2].set_title(f\"{feature_list_num[11]}\")\n",
    "\n",
    "axs[4, 0].hist(train_ds[feature_list_num[12]], bins=100)\n",
    "axs[4, 0].set_title(f\"{feature_list_num[12]}\")\n",
    "\n",
    "axs[4, 1].hist(train_ds[feature_list_num[13]], bins=100)\n",
    "axs[4, 1].set_title(f\"{feature_list_num[13]}\")\n",
    "\n",
    "# Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "for ax in axs.flat:\n",
    "    ax.label_outer()\n",
    "\n",
    "plt.suptitle(\"Histograms of Numerical Features\", size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix\n",
    "sns.heatmap(train_ds.corr(), annot=True, cmap=\"RdYlGn\", linewidths=0.2)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(20, 12)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list_cat = [\n",
    "    \"cat0\",\n",
    "    \"cat1\",\n",
    "    \"cat2\",\n",
    "    \"cat3\",\n",
    "    \"cat4\",\n",
    "    \"cat5\",\n",
    "    \"cat6\",\n",
    "    \"cat7\",\n",
    "    \"cat8\",\n",
    "    \"cat9\",\n",
    "]\n",
    "feature_list_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=train_ds, x=feature_list_cat[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 3)\n",
    "fig.set_size_inches(12, 9)\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "fig.subplots_adjust(wspace=0.4, hspace=0.5)\n",
    "\n",
    "sns.countplot(data=train_ds, x=feature_list_cat[0], ax=axs[0, 0])\n",
    "axs[0, 0].title.set_text(feature_list_cat[0])\n",
    "axs[0, 0].set(xlabel=None)\n",
    "axs[0, 0].set(ylabel=None)\n",
    "\n",
    "sns.countplot(data=train_ds, x=feature_list_cat[1], ax=axs[0, 1])\n",
    "axs[0, 1].title.set_text(feature_list_cat[1])\n",
    "axs[0, 1].set(xlabel=None)\n",
    "axs[0, 1].set(ylabel=None)\n",
    "\n",
    "sns.countplot(data=train_ds, x=feature_list_cat[2], ax=axs[0, 2])\n",
    "axs[0, 2].title.set_text(feature_list_cat[2])\n",
    "axs[0, 2].set(xlabel=None)\n",
    "axs[0, 2].set(ylabel=None)\n",
    "\n",
    "sns.countplot(data=train_ds, x=feature_list_cat[3], ax=axs[1, 0])\n",
    "axs[1, 0].title.set_text(feature_list_cat[3])\n",
    "axs[1, 0].set(xlabel=None)\n",
    "axs[1, 0].set(ylabel=None)\n",
    "\n",
    "sns.countplot(data=train_ds, x=feature_list_cat[4], ax=axs[1, 1])\n",
    "axs[1, 1].title.set_text(feature_list_cat[4])\n",
    "axs[1, 1].set(xlabel=None)\n",
    "axs[1, 1].set(ylabel=None)\n",
    "\n",
    "sns.countplot(data=train_ds, x=feature_list_cat[5], ax=axs[1, 2])\n",
    "axs[1, 2].title.set_text(feature_list_cat[5])\n",
    "axs[1, 2].set(xlabel=None)\n",
    "axs[1, 2].set(ylabel=None)\n",
    "\n",
    "sns.countplot(data=train_ds, x=feature_list_cat[6], ax=axs[2, 0])\n",
    "axs[2, 0].title.set_text(feature_list_cat[6])\n",
    "axs[2, 0].set(xlabel=None)\n",
    "axs[2, 0].set(ylabel=None)\n",
    "\n",
    "sns.countplot(data=train_ds, x=feature_list_cat[7], ax=axs[2, 1])\n",
    "axs[2, 1].title.set_text(feature_list_cat[7])\n",
    "axs[2, 1].set(xlabel=None)\n",
    "axs[2, 1].set(ylabel=None)\n",
    "\n",
    "sns.countplot(data=train_ds, x=feature_list_cat[8], ax=axs[2, 2])\n",
    "axs[2, 2].title.set_text(feature_list_cat[8])\n",
    "axs[2, 2].set(xlabel=None)\n",
    "axs[2, 2].set(ylabel=None)\n",
    "\n",
    "sns.countplot(data=train_ds, x=feature_list_cat[9], ax=axs[3, 0])\n",
    "axs[3, 0].title.set_text(feature_list_cat[9])\n",
    "axs[3, 0].set(xlabel=None)\n",
    "axs[3, 0].set(ylabel=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding to Categorical Columns\n",
    "# encoder = OrdinalEncoder()\n",
    "# train_ds[\"cat0\"] = encoder.fit_transform(train_ds[\"cat0\"].to_numpy().reshape(-1, 1))\n",
    "# train_ds[\"cat1\"] = encoder.fit_transform(train_ds[\"cat1\"].to_numpy().reshape(-1, 1))\n",
    "# train_ds[\"cat2\"] = encoder.fit_transform(train_ds[\"cat2\"].to_numpy().reshape(-1, 1))\n",
    "# train_ds[\"cat3\"] = encoder.fit_transform(train_ds[\"cat3\"].to_numpy().reshape(-1, 1))\n",
    "# train_ds[\"cat4\"] = encoder.fit_transform(train_ds[\"cat4\"].to_numpy().reshape(-1, 1))\n",
    "# train_ds[\"cat5\"] = encoder.fit_transform(train_ds[\"cat5\"].to_numpy().reshape(-1, 1))\n",
    "# train_ds[\"cat6\"] = encoder.fit_transform(train_ds[\"cat6\"].to_numpy().reshape(-1, 1))\n",
    "# train_ds[\"cat7\"] = encoder.fit_transform(train_ds[\"cat7\"].to_numpy().reshape(-1, 1))\n",
    "# train_ds[\"cat8\"] = encoder.fit_transform(train_ds[\"cat8\"].to_numpy().reshape(-1, 1))\n",
    "# train_ds[\"cat9\"] = encoder.fit_transform(train_ds[\"cat9\"].to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot Feature vs Target\n",
    "sns.scatterplot(data=train_ds, x=feature_list_num[0], y=\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 3)\n",
    "fig.set_size_inches(12, 9)\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "fig.subplots_adjust(wspace=0.4, hspace=0.5)\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=train_ds, x=feature_list_num[0], y=\"target\", ax=axs[0, 0], alpha=0.01\n",
    ")\n",
    "axs[0, 0].title.set_text(feature_list_num[0])\n",
    "axs[0, 0].set(xlabel=None)\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=train_ds, x=feature_list_num[1], y=\"target\", ax=axs[0, 1], alpha=0.01\n",
    ")\n",
    "axs[0, 1].title.set_text(feature_list_num[1])\n",
    "axs[0, 1].set(xlabel=None)\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=train_ds, x=feature_list_num[2], y=\"target\", ax=axs[0, 2], alpha=0.01\n",
    ")\n",
    "axs[0, 2].title.set_text(feature_list_num[2])\n",
    "axs[0, 2].set(xlabel=None)\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=train_ds, x=feature_list_num[3], y=\"target\", ax=axs[1, 0], alpha=0.01\n",
    ")\n",
    "axs[1, 0].title.set_text(feature_list_num[3])\n",
    "axs[1, 0].set(xlabel=None)\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=train_ds, x=feature_list_num[4], y=\"target\", ax=axs[1, 1], alpha=0.01\n",
    ")\n",
    "axs[1, 1].title.set_text(feature_list_num[4])\n",
    "axs[1, 1].set(xlabel=None)\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=train_ds, x=feature_list_num[5], y=\"target\", ax=axs[1, 2], alpha=0.01\n",
    ")\n",
    "axs[1, 2].title.set_text(feature_list_num[5])\n",
    "axs[1, 2].set(xlabel=None)\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=train_ds, x=feature_list_num[6], y=\"target\", ax=axs[2, 0], alpha=0.01\n",
    ")\n",
    "axs[2, 0].title.set_text(feature_list_num[6])\n",
    "axs[2, 0].set(xlabel=None)\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=train_ds, x=feature_list_num[7], y=\"target\", ax=axs[2, 1], alpha=0.01\n",
    ")\n",
    "axs[2, 1].title.set_text(feature_list_num[7])\n",
    "axs[2, 1].set(xlabel=None)\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=train_ds, x=feature_list_num[8], y=\"target\", ax=axs[2, 2], alpha=0.01\n",
    ")\n",
    "axs[2, 2].title.set_text(feature_list_num[8])\n",
    "axs[2, 2].set(xlabel=None)\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=train_ds, x=feature_list_num[9], y=\"target\", ax=axs[3, 0], alpha=0.01\n",
    ")\n",
    "axs[3, 0].title.set_text(feature_list_num[9])\n",
    "axs[3, 0].set(xlabel=None)\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=train_ds, x=feature_list_num[10], y=\"target\", ax=axs[3, 1], alpha=0.01\n",
    ")\n",
    "axs[3, 1].title.set_text(feature_list_num[10])\n",
    "axs[3, 1].set(xlabel=None)\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=train_ds, x=feature_list_num[11], y=\"target\", ax=axs[3, 2], alpha=0.01\n",
    ")\n",
    "axs[3, 2].title.set_text(feature_list_num[11])\n",
    "axs[3, 2].set(xlabel=None)\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=train_ds, x=feature_list_num[12], y=\"target\", ax=axs[4, 0], alpha=0.01\n",
    ")\n",
    "axs[4, 0].title.set_text(feature_list_num[12])\n",
    "axs[4, 0].set(xlabel=None)\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=train_ds, x=feature_list_num[13], y=\"target\", ax=axs[4, 1], alpha=0.01\n",
    ")\n",
    "axs[4, 1].title.set_text(feature_list_num[13])\n",
    "axs[4, 1].set(xlabel=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature = train_ds.drop(\"target\", axis=1)\n",
    "target_feature = train_ds[[\"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(all_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(target_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    all_feature, target_feature, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "# Standardize numeric values and one-hot encode categorical values\n",
    "# the Random Forest Regressor\n",
    "\n",
    "\n",
    "# Standardize\n",
    "numeric_transformer = Pipeline(steps=[(\"scaler\", StandardScaler())])\n",
    "\n",
    "# Ordinal Encoding\n",
    "categorical_transformer = OrdinalEncoder()\n",
    "\n",
    "# Combine as Pre-processor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, feature_list_num),\n",
    "        (\"cat\", categorical_transformer, feature_list_cat),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Grid search parameters\n",
    "grid_param = {\n",
    "    \"n_estimators\": [200, 400, 600, 1000, 1500],\n",
    "    \"max_depth\": [3, 5, 10, 50, 100, None],\n",
    "    \"min_samples_leaf\": [1, 2, 4, 6, 10],\n",
    "    \"min_samples_split\": [2, 5, 10, 20],\n",
    "}\n",
    "\n",
    "# Random Forest Regressor\n",
    "# regressor = RandomForestRegressor(n_estimators=400, verbose=True, n_jobs=-1)\n",
    "\n",
    "regressor = RandomForestRegressor(bootstrap=True, max_samples=0.01)\n",
    "\n",
    "# Random Forest Cross Validation\n",
    "regressor_cv = RandomizedSearchCV(\n",
    "    regressor,\n",
    "    n_iter=100,\n",
    "    param_distributions=grid_param,\n",
    "    cv=3,\n",
    "    verbose=10,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Full Prediction Pipeline\n",
    "model = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"regressor\", regressor_cv)])\n",
    "\n",
    "# Fit model\n",
    "model.fit(x_train, np.array(y_train).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(x_train)\n",
    "\n",
    "# Model validation\n",
    "from sklearn.metrics import (\n",
    "    explained_variance_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"explained_variance_score: \"\n",
    "    + str(round(explained_variance_score(y_train, y_train_pred), 4))\n",
    ")\n",
    "print(\"r2_score: \" + str(round(r2_score(y_train, y_train_pred), 4)))\n",
    "print(\n",
    "    \"mean_absolute_error: \" + str(round(mean_absolute_error(y_train, y_train_pred), 4))\n",
    ")\n",
    "print(\n",
    "    \"root_mean_squared_error: \"\n",
    "    + str(round(mean_squared_error(y_train, y_train_pred, squared=False), 4))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best parameters\n",
    "model[\"regressor\"].best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_opt = RandomForestRegressor(\n",
    "    n_estimators=1000,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=1,\n",
    "    max_depth=None,\n",
    "    verbose=10,\n",
    ")\n",
    "\n",
    "model = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"regressor_opt\", regressor_opt)]\n",
    ")\n",
    "\n",
    "model.fit(x_train, np.array(y_train).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(x_train)\n",
    "\n",
    "# Model validation\n",
    "from sklearn.metrics import (\n",
    "    explained_variance_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"explained_variance_score: \"\n",
    "    + str(round(explained_variance_score(y_train, y_train_pred), 4))\n",
    ")\n",
    "print(\"r2_score: \" + str(round(r2_score(y_train, y_train_pred), 4)))\n",
    "print(\n",
    "    \"mean_absolute_error: \" + str(round(mean_absolute_error(y_train, y_train_pred), 4))\n",
    ")\n",
    "print(\n",
    "    \"root_mean_squared_error: \"\n",
    "    + str(round(mean_squared_error(y_train, y_train_pred, squared=False), 4))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(x_test)\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    explained_variance_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    ")\n",
    "\n",
    "print(\"mean_absolute_error :\" + str(round(mean_absolute_error(y_test, y_test_pred), 4)))\n",
    "print(\n",
    "    \"explained_variance_score :\"\n",
    "    + str(round(explained_variance_score(y_test, y_test_pred), 4))\n",
    ")\n",
    "print(\"r2_score :\" + str(round(r2_score(y_test, y_test_pred), 4)))\n",
    "print(\"mean_squared_error :\" + str(round(mean_squared_error(y_test, y_test_pred), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_output = pd.read_csv(\"data/Tabular Playground Series - Feb 2021/test.csv\")\n",
    "x_submit = pd.read_csv(\"data/Tabular Playground Series - Feb 2021/test.csv\")\n",
    "output_id = x_output[\"id\"]\n",
    "x_output = x_output.drop(\"id\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "y_output = model.predict(x_output)\n",
    "x_output[\"id\"] = output_id\n",
    "x_output[\"target\"] = y_output\n",
    "\n",
    "date_time = (\n",
    "    \"date_\"\n",
    "    + str(now.year)\n",
    "    + \"-\"\n",
    "    + str(now.month)\n",
    "    + \"-\"\n",
    "    + str(now.day)\n",
    "    + \"_time_\"\n",
    "    + str(now.hour)\n",
    "    + \"-\"\n",
    "    + str(now.minute)\n",
    ")\n",
    "\n",
    "x_output.to_csv(\n",
    "    f\"data/Tabular Playground Series - Feb 2021/{date_time}_test.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extremly Randomized Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "ordinal = OrdinalEncoder()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"scaler\", scaler, feature_list_num),\n",
    "        (\"ordinal\", ordinal, feature_list_cat),\n",
    "    ]\n",
    ")\n",
    "\n",
    "regressor = ExtraTreesRegressor()\n",
    "\n",
    "# Grid Search Parameters\n",
    "param_grid = {\n",
    "    \"n_estimators\": [200, 400, 600, 1000],\n",
    "    \"max_features\": [2, 5, 10, 15, 20, None],\n",
    "    \"max_depth\": [5, 10, 50, 100, None],\n",
    "    \"min_samples_leaf\": [0.01, 0.05, 0.1, None],\n",
    "    \"min_samples_split\": [2, 5, 10, 20],\n",
    "}\n",
    "\n",
    "random_cv = RandomizedSearchCV(\n",
    "    estimator=regressor,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=100,\n",
    "    n_jobs=-1,\n",
    "    cv=3,\n",
    "    verbose=10,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "model = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"random_cv\", random_cv)])\n",
    "\n",
    "model.fit(x_train, np.array(y_train).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[\"random_cv\"].best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score on Training Data\n",
    "model.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    explained_variance_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"explained_variance_score : \"\n",
    "    + str(round(explained_variance_score(y_test, y_test_pred), 4))\n",
    ")\n",
    "print(\"r2_score : \" + str(round(r2_score(y_test, y_test_pred), 4)))\n",
    "print(\n",
    "    \"mean_absolute_error : \" + str(round(mean_absolute_error(y_test, y_test_pred), 4))\n",
    ")\n",
    "print(\"mean_squared_error : \" + str(round(mean_squared_error(y_test, y_test_pred), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "y_output = model.predict(x_output)\n",
    "x_output[\"id\"] = output_id\n",
    "x_output[\"target\"] = y_output\n",
    "\n",
    "date_time = (\n",
    "    \"date_\"\n",
    "    + str(now.year)\n",
    "    + \"-\"\n",
    "    + str(now.month)\n",
    "    + \"-\"\n",
    "    + str(now.day)\n",
    "    + \"_time_\"\n",
    "    + str(now.hour)\n",
    "    + \"-\"\n",
    "    + str(now.minute)\n",
    ")\n",
    "\n",
    "x_output.to_csv(\n",
    "    f\"data/Tabular Playground Series - Feb 2021/extra-tree-regressor_{date_time}_test.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_output.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost (eXtreme Gradient Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = StandardScaler()\n",
    "\n",
    "ordinal = OrdinalEncoder()\n",
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"scalar\", scalar, feature_list_num),\n",
    "        (\"ordinal\", ordinal, feature_list_cat),\n",
    "    ]\n",
    ")\n",
    "\n",
    "x_preprocessed = preprocessing.fit_transform(all_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_preprocessed, target_feature, test_size=0.3, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# convert to DMatrix format\n",
    "d_train_xgb = xgb.DMatrix(x_train, label=y_train, enable_categorical=True)\n",
    "d_test_xgb = xgb.DMatrix(x_test, label=y_test, enable_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train_xgb, d_test_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model\n",
    "\n",
    "params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    \"max_depth\": 6,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"eta\": 0.3,\n",
    "    \"subsample\": 1,\n",
    "    \"colsample_bytree\": 1,\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    # Other parameters\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "}\n",
    "\n",
    "num_boost_round = 300\n",
    "\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    d_train_xgb,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=[(d_test_xgb, \"Test\")],\n",
    "    early_stopping_rounds=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = xgb.cv(\n",
    "    params,\n",
    "    d_train_xgb,\n",
    "    num_boost_round=num_boost_round,\n",
    "    seed=42,\n",
    "    nfold=5,\n",
    "    metrics={\"rmse\"},\n",
    "    early_stopping_rounds=10,\n",
    ")\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Curve\n",
    "import seaborn as sns\n",
    "\n",
    "temp = cv_results[cv_results[\"train-rmse-mean\"] < 1]\n",
    "sns.lineplot(data=temp, x=temp.index, y=\"train-rmse-mean\", label=\"train error\")\n",
    "sns.lineplot(data=temp, x=temp.index, y=\"test-rmse-mean\", label=\"test error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best parameters for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(2, 9)\n",
    "    for min_child_weight in range(2, 9)\n",
    "]\n",
    "\n",
    "# Define initial best params and RMSE\n",
    "min_rmse = float(\"Inf\")\n",
    "best_params = None\n",
    "for max_depth, min_child_weight in grid_search_params:\n",
    "    print(\n",
    "        \"CV with max_depth={}, min_child_weight={}\".format(max_depth, min_child_weight)\n",
    "    )\n",
    "\n",
    "    # Update our parameters\n",
    "    params[\"max_depth\"] = max_depth\n",
    "    params[\"min_child_weight\"] = min_child_weight\n",
    "\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        d_train_xgb,\n",
    "        num_boost_round=num_boost_round,\n",
    "        seed=42,\n",
    "        nfold=3,\n",
    "        metrics={\"rmse\"},\n",
    "        early_stopping_rounds=10,\n",
    "    )\n",
    "\n",
    "    # Update best RME\n",
    "    mean_rmse = cv_results[\"test-rmse-mean\"].min()\n",
    "    boost_rounds = cv_results[\"test-rmse-mean\"].argmin()\n",
    "    print(\"\\tRMSE {} for {} rounds\".format(mean_rmse, boost_rounds))\n",
    "    if mean_rmse < min_rmse:\n",
    "        min_rmse = mean_rmse\n",
    "        best_params = (max_depth, min_child_weight)\n",
    "\n",
    "print(\"Best params: {}, {}, RMSE: {}\".format(best_params[0], best_params[1], min_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add best parameters\n",
    "params[\"max_depth\"] = 2\n",
    "params[\"min_child_weight\"] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best subsample and colsample\n",
    "grid_search_params = [\n",
    "    (subsample, colsample)\n",
    "    for subsample in [i / 10.0 for i in range(7, 11)]\n",
    "    for colsample in [i / 10.0 for i in range(7, 11)]\n",
    "]\n",
    "\n",
    "min_rmse = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "# We start by the largest values and go down to the smallest\n",
    "for subsample, colsample in reversed(grid_search_params):\n",
    "    print(\"CV with subsample={}, colsample={}\".format(subsample, colsample))\n",
    "\n",
    "    # We update our parameters\n",
    "    params[\"subsample\"] = subsample\n",
    "    params[\"colsample_bytree\"] = colsample\n",
    "\n",
    "    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        d_train_xgb,\n",
    "        num_boost_round=num_boost_round,\n",
    "        seed=42,\n",
    "        nfold=3,\n",
    "        metrics={\"rmse\"},\n",
    "        early_stopping_rounds=10,\n",
    "    )\n",
    "\n",
    "    # Update best score\n",
    "    mean_rmse = cv_results[\"test-rmse-mean\"].min()\n",
    "    boost_rounds = cv_results[\"test-rmse-mean\"].argmin()\n",
    "\n",
    "    print(\"\\tRMSE {} for {} rounds\".format(mean_rmse, boost_rounds))\n",
    "\n",
    "    if mean_rmse < min_rmse:\n",
    "        min_rmse = mean_rmse\n",
    "        best_params = (subsample, colsample)\n",
    "\n",
    "print(\"Best params: {}, {}, RMSE: {}\".format(best_params[0], best_params[1], min_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update parameter\n",
    "params[\"subsample\"] = 1.0\n",
    "params[\"colsample_bytree\"] = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best learning rate eta\n",
    "\n",
    "%time\n",
    "\n",
    "min_rmse= float(\"Inf\")\n",
    "best_params = None\n",
    "for eta in [1, .5, .3, .2, .1, .05, .01, .005]:\n",
    "    print(\"CV with eta={}\".format(eta))\n",
    "    \n",
    "    # We update our parameters\n",
    "    params['eta'] = eta\n",
    "    \n",
    "    # Run and time CV\n",
    "    %time cv_results = xgb.cv(params, d_train_xgb, num_boost_round=num_boost_round, seed=42, nfold=3, metrics=['rmse'],early_stopping_rounds=10)\n",
    "    \n",
    "    # Update best score\n",
    "    mean_rsme = cv_results['test-rmse-mean'].min()\n",
    "    boost_rounds = cv_results['test-rmse-mean'].argmin()\n",
    "    print(\"\\tRMSE {} for {} rounds\\n\".format(mean_rmse, boost_rounds))\n",
    "    if mean_rmse < min_rmse:\n",
    "        min_rmse = mean_rmse\n",
    "        best_params = eta\n",
    "        \n",
    "print(\"Best params: {}, RMSE: {}\".format(best_params, min_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"eta\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final parameter\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = xgb.train(\n",
    "    params,\n",
    "    d_train_xgb,\n",
    "    num_boost_round=num_boost_round,\n",
    "    evals=[(d_test_xgb, \"Test\")],\n",
    "    early_stopping_rounds=10,\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Best RMSE: {:.2f} in {} rounds\".format(\n",
    "        final_model.best_score, final_model.best_iteration + 1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction with final_model\n",
    "x_output_pre = preprocessor.transform(x_submit)\n",
    "\n",
    "y_submit = final_model.predict(xgb.DMatrix(x_output_pre))\n",
    "y_submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export prediction\n",
    "import datetime as datetime\n",
    "\n",
    "export_df = pd.DataFrame()\n",
    "export_df[\"id\"] = pd.read_csv(\"data/Tabular Playground Series - Feb 2021/test.csv\")[\n",
    "    \"id\"\n",
    "]\n",
    "export_df[\"target\"] = y_submit\n",
    "now = datetime.datetime.now()\n",
    "name_add = (\n",
    "    \"date_\"\n",
    "    + str(now.year)\n",
    "    + \"-\"\n",
    "    + str(now.month)\n",
    "    + \"-\"\n",
    "    + str(now.day)\n",
    "    + \"_time_\"\n",
    "    + str(now.hour)\n",
    "    + \"-\"\n",
    "    + str(now.minute)\n",
    ")\n",
    "export_df.to_csv(\n",
    "    f\"data/Tabular Playground Series - Feb 2021/xgboost_tuned_{name_add}.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging and other bootstrapping models (simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "\n",
    "# Standardize\n",
    "numeric_transformer = Pipeline(steps=[(\"scaler\", StandardScaler())])\n",
    "\n",
    "# One-Hot Encoding\n",
    "categorical_transformer = OrdinalEncoder()\n",
    "\n",
    "# Combine as Pre-processor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, feature_list_num),\n",
    "        (\"cat\", categorical_transformer, feature_list_cat),\n",
    "    ]\n",
    ")\n",
    "\n",
    "x_pre = preprocessor.fit_transform(all_feature)\n",
    "x_output_pre = preprocessor.transform(x_submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_pre, target_feature, test_size=0.4, shuffle=True, random_state=0\n",
    ")\n",
    "\n",
    "y_train = np.ravel(y_train)\n",
    "y_test = np.ravel(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Bagging\n",
    "\n",
    "bagging_reg = BaggingRegressor(\n",
    "    DecisionTreeRegressor(),\n",
    "    n_estimators=500,\n",
    "    bootstrap=True,\n",
    "    max_samples=0.3,\n",
    "    # bootstrap_features = True, max_features=1.0,\n",
    "    n_jobs=-1,\n",
    "    verbose=3,\n",
    ")\n",
    "\n",
    "bagging_reg.fit(x_train, y_train)\n",
    "y_pred = bagging_reg.predict(x_test)\n",
    "print(\"RMSE: \" + str(mean_squared_error(y_test, y_pred, squared=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as datetime\n",
    "\n",
    "y_submit = bagging_reg.predict(x_output_pre)\n",
    "\n",
    "export_df = pd.DataFrame()\n",
    "export_df[\"id\"] = pd.read_csv(\"data/Tabular Playground Series - Feb 2021/test.csv\")[\n",
    "    \"id\"\n",
    "]\n",
    "export_df[\"target\"] = y_submit\n",
    "now = datetime.datetime.now()\n",
    "name_add = (\n",
    "    \"date_\"\n",
    "    + str(now.year)\n",
    "    + \"-\"\n",
    "    + str(now.month)\n",
    "    + \"-\"\n",
    "    + str(now.day)\n",
    "    + \"_time_\"\n",
    "    + str(now.hour)\n",
    "    + \"-\"\n",
    "    + str(now.minute)\n",
    ")\n",
    "export_df.to_csv(\n",
    "    f\"data/Tabular Playground Series - Feb 2021/bagging_simple_{name_add}.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasting\n",
    "\n",
    "pasting_reg = BaggingRegressor(\n",
    "    DecisionTreeRegressor(),\n",
    "    n_estimators=500,\n",
    "    bootstrap=False,\n",
    "    max_samples=0.3,\n",
    "    # bootstrap_features = True, max_features=1.0,\n",
    "    n_jobs=-1,\n",
    "    verbose=3,\n",
    ")\n",
    "\n",
    "pasting_reg.fit(x_train, y_train)\n",
    "y_pred = pasting_reg.predict(x_test)\n",
    "print(\"RMSE: \" + str(mean_squared_error(y_test, y_pred, squared=False)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
