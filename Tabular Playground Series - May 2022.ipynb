{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tabular Playground Series - May 2022.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMMsVKVXzftK1tzsbbhdBjT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahug/ds-practice/blob/main/Tabular%20Playground%20Series%20-%20May%202022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tabular Playground Series - May 2022**"
      ],
      "metadata": {
        "id": "TlhqGmiHlzqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset**\n",
        "\n",
        "**Link:** https://www.kaggle.com/competitions/tabular-playground-series-may-2022/data"
      ],
      "metadata": {
        "id": "_Fg0yJZemEm5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unzip**"
      ],
      "metadata": {
        "id": "Yc_LEVkwqA5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/tabular-playground-series-may-2022.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/\")"
      ],
      "metadata": {
        "id": "44Pc9INkmqPm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Dataset**"
      ],
      "metadata": {
        "id": "f1TYgLaFqjlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "kcxIZXvZqn1P"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"/content/train.csv\")\n",
        "test = pd.read_csv(\"/content/test.csv\")"
      ],
      "metadata": {
        "id": "l50HrGwYqloU"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "QR4BqBS-q9l6",
        "outputId": "d35c3550-5352-4b81-bc07-a56ef4e31557"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id      f_00      f_01      f_02      f_03      f_04      f_05      f_06  \\\n",
              "0   0 -1.373246  0.238887 -0.243376  0.567405 -0.647715  0.839326  0.113133   \n",
              "1   1  1.697021 -1.710322 -2.230332 -0.545661  1.113173 -1.552175  0.447825   \n",
              "2   2  1.681726  0.616746 -1.027689  0.810492 -0.609086  0.113965 -0.708660   \n",
              "3   3 -0.118172 -0.587835 -0.804638  2.086822  0.371005 -0.128831 -0.282575   \n",
              "4   4  1.148481 -0.176567 -0.664871 -1.101343  0.467875  0.500117  0.407515   \n",
              "\n",
              "   f_07  f_08  ...      f_22      f_23      f_24      f_25      f_26  \\\n",
              "0     1     5  ... -2.540739  0.766952 -2.730628 -0.208177  1.363402   \n",
              "1     1     3  ...  2.278315 -0.633658 -1.217077 -3.782194 -0.058316   \n",
              "2     1     0  ... -1.385775 -0.520558 -0.009121  2.788536 -3.703488   \n",
              "3     3     2  ...  0.572594 -1.653213  1.686035 -2.533098 -0.608601   \n",
              "4     3     3  ... -3.912929 -1.430366  2.127649 -3.306784  4.371371   \n",
              "\n",
              "         f_27        f_28  f_29  f_30  target  \n",
              "0  ABABDADBAB   67.609153     0     0       0  \n",
              "1  ACACCADCEB  377.096415     0     0       1  \n",
              "2  AAAEABCKAD -195.599702     0     2       1  \n",
              "3  BDBBAACBCB  210.826205     0     0       1  \n",
              "4  BDBCBBCHFE -217.211798     0     1       1  \n",
              "\n",
              "[5 rows x 33 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d8e87976-fc42-431b-bf34-2c55425da407\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>f_00</th>\n",
              "      <th>f_01</th>\n",
              "      <th>f_02</th>\n",
              "      <th>f_03</th>\n",
              "      <th>f_04</th>\n",
              "      <th>f_05</th>\n",
              "      <th>f_06</th>\n",
              "      <th>f_07</th>\n",
              "      <th>f_08</th>\n",
              "      <th>...</th>\n",
              "      <th>f_22</th>\n",
              "      <th>f_23</th>\n",
              "      <th>f_24</th>\n",
              "      <th>f_25</th>\n",
              "      <th>f_26</th>\n",
              "      <th>f_27</th>\n",
              "      <th>f_28</th>\n",
              "      <th>f_29</th>\n",
              "      <th>f_30</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-1.373246</td>\n",
              "      <td>0.238887</td>\n",
              "      <td>-0.243376</td>\n",
              "      <td>0.567405</td>\n",
              "      <td>-0.647715</td>\n",
              "      <td>0.839326</td>\n",
              "      <td>0.113133</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.540739</td>\n",
              "      <td>0.766952</td>\n",
              "      <td>-2.730628</td>\n",
              "      <td>-0.208177</td>\n",
              "      <td>1.363402</td>\n",
              "      <td>ABABDADBAB</td>\n",
              "      <td>67.609153</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1.697021</td>\n",
              "      <td>-1.710322</td>\n",
              "      <td>-2.230332</td>\n",
              "      <td>-0.545661</td>\n",
              "      <td>1.113173</td>\n",
              "      <td>-1.552175</td>\n",
              "      <td>0.447825</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>2.278315</td>\n",
              "      <td>-0.633658</td>\n",
              "      <td>-1.217077</td>\n",
              "      <td>-3.782194</td>\n",
              "      <td>-0.058316</td>\n",
              "      <td>ACACCADCEB</td>\n",
              "      <td>377.096415</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1.681726</td>\n",
              "      <td>0.616746</td>\n",
              "      <td>-1.027689</td>\n",
              "      <td>0.810492</td>\n",
              "      <td>-0.609086</td>\n",
              "      <td>0.113965</td>\n",
              "      <td>-0.708660</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.385775</td>\n",
              "      <td>-0.520558</td>\n",
              "      <td>-0.009121</td>\n",
              "      <td>2.788536</td>\n",
              "      <td>-3.703488</td>\n",
              "      <td>AAAEABCKAD</td>\n",
              "      <td>-195.599702</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>-0.118172</td>\n",
              "      <td>-0.587835</td>\n",
              "      <td>-0.804638</td>\n",
              "      <td>2.086822</td>\n",
              "      <td>0.371005</td>\n",
              "      <td>-0.128831</td>\n",
              "      <td>-0.282575</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.572594</td>\n",
              "      <td>-1.653213</td>\n",
              "      <td>1.686035</td>\n",
              "      <td>-2.533098</td>\n",
              "      <td>-0.608601</td>\n",
              "      <td>BDBBAACBCB</td>\n",
              "      <td>210.826205</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1.148481</td>\n",
              "      <td>-0.176567</td>\n",
              "      <td>-0.664871</td>\n",
              "      <td>-1.101343</td>\n",
              "      <td>0.467875</td>\n",
              "      <td>0.500117</td>\n",
              "      <td>0.407515</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.912929</td>\n",
              "      <td>-1.430366</td>\n",
              "      <td>2.127649</td>\n",
              "      <td>-3.306784</td>\n",
              "      <td>4.371371</td>\n",
              "      <td>BDBCBBCHFE</td>\n",
              "      <td>-217.211798</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 33 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d8e87976-fc42-431b-bf34-2c55425da407')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d8e87976-fc42-431b-bf34-2c55425da407 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d8e87976-fc42-431b-bf34-2c55425da407');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8z0k_b3xrCnY",
        "outputId": "85bfb785-0b43-4b18-85e0-c4f1226920fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 900000 entries, 0 to 899999\n",
            "Data columns (total 33 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   id      900000 non-null  int64  \n",
            " 1   f_00    900000 non-null  float64\n",
            " 2   f_01    900000 non-null  float64\n",
            " 3   f_02    900000 non-null  float64\n",
            " 4   f_03    900000 non-null  float64\n",
            " 5   f_04    900000 non-null  float64\n",
            " 6   f_05    900000 non-null  float64\n",
            " 7   f_06    900000 non-null  float64\n",
            " 8   f_07    900000 non-null  int64  \n",
            " 9   f_08    900000 non-null  int64  \n",
            " 10  f_09    900000 non-null  int64  \n",
            " 11  f_10    900000 non-null  int64  \n",
            " 12  f_11    900000 non-null  int64  \n",
            " 13  f_12    900000 non-null  int64  \n",
            " 14  f_13    900000 non-null  int64  \n",
            " 15  f_14    900000 non-null  int64  \n",
            " 16  f_15    900000 non-null  int64  \n",
            " 17  f_16    900000 non-null  int64  \n",
            " 18  f_17    900000 non-null  int64  \n",
            " 19  f_18    900000 non-null  int64  \n",
            " 20  f_19    900000 non-null  float64\n",
            " 21  f_20    900000 non-null  float64\n",
            " 22  f_21    900000 non-null  float64\n",
            " 23  f_22    900000 non-null  float64\n",
            " 24  f_23    900000 non-null  float64\n",
            " 25  f_24    900000 non-null  float64\n",
            " 26  f_25    900000 non-null  float64\n",
            " 27  f_26    900000 non-null  float64\n",
            " 28  f_27    900000 non-null  object \n",
            " 29  f_28    900000 non-null  float64\n",
            " 30  f_29    900000 non-null  int64  \n",
            " 31  f_30    900000 non-null  int64  \n",
            " 32  target  900000 non-null  int64  \n",
            "dtypes: float64(16), int64(16), object(1)\n",
            "memory usage: 226.6+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.describe(include=\"all\").T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ccPnAqYRrGaU",
        "outputId": "70cfcf7e-7c74-409c-8333-e49c2859a839"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           count  unique         top freq      mean            std  \\\n",
              "id      900000.0     NaN         NaN  NaN  449999.5  259807.765474   \n",
              "f_00    900000.0     NaN         NaN  NaN -0.000286       0.998888   \n",
              "f_01    900000.0     NaN         NaN  NaN  0.001165       0.999193   \n",
              "f_02    900000.0     NaN         NaN  NaN  0.001174       1.000514   \n",
              "f_03    900000.0     NaN         NaN  NaN -0.001368       1.000175   \n",
              "f_04    900000.0     NaN         NaN  NaN -0.000571       1.000167   \n",
              "f_05    900000.0     NaN         NaN  NaN  0.000284       0.999875   \n",
              "f_06    900000.0     NaN         NaN  NaN -0.000709       0.999942   \n",
              "f_07    900000.0     NaN         NaN  NaN   2.03146       1.656172   \n",
              "f_08    900000.0     NaN         NaN  NaN  2.057998       1.590955   \n",
              "f_09    900000.0     NaN         NaN  NaN  2.362431       1.637706   \n",
              "f_10    900000.0     NaN         NaN  NaN  2.177637       1.645953   \n",
              "f_11    900000.0     NaN         NaN  NaN  1.803392       1.537487   \n",
              "f_12    900000.0     NaN         NaN  NaN  2.842373       1.762835   \n",
              "f_13    900000.0     NaN         NaN  NaN  2.239778       1.538426   \n",
              "f_14    900000.0     NaN         NaN  NaN  1.514686       1.359213   \n",
              "f_15    900000.0     NaN         NaN  NaN  2.101132       1.569093   \n",
              "f_16    900000.0     NaN         NaN  NaN  2.096713       1.560169   \n",
              "f_17    900000.0     NaN         NaN  NaN  1.858518       1.467675   \n",
              "f_18    900000.0     NaN         NaN  NaN  2.065131       1.564783   \n",
              "f_19    900000.0     NaN         NaN  NaN  0.308713       2.316026   \n",
              "f_20    900000.0     NaN         NaN  NaN  -0.17873       2.400494   \n",
              "f_21    900000.0     NaN         NaN  NaN -0.156307       2.484706   \n",
              "f_22    900000.0     NaN         NaN  NaN -0.009273       2.450797   \n",
              "f_23    900000.0     NaN         NaN  NaN -0.369459       2.453405   \n",
              "f_24    900000.0     NaN         NaN  NaN -0.342738       2.386941   \n",
              "f_25    900000.0     NaN         NaN  NaN  0.176549       2.416959   \n",
              "f_26    900000.0     NaN         NaN  NaN  0.357591        2.47602   \n",
              "f_27      900000  741354  BBBBBBCJBC   12       NaN            NaN   \n",
              "f_28    900000.0     NaN         NaN  NaN -0.380876     238.773054   \n",
              "f_29    900000.0     NaN         NaN  NaN  0.345661       0.475584   \n",
              "f_30    900000.0     NaN         NaN  NaN  1.002654       0.818989   \n",
              "target  900000.0     NaN         NaN  NaN  0.486488       0.499818   \n",
              "\n",
              "                min         25%       50%         75%          max  \n",
              "id              0.0   224999.75  449999.5   674999.25     899999.0  \n",
              "f_00      -4.599856    -0.67549  0.001144    0.674337     4.749301  \n",
              "f_01      -4.682199   -0.675162  0.002014    0.675021     4.815699  \n",
              "f_02      -4.642676   -0.674369  0.002218    0.677505     4.961982  \n",
              "f_03      -4.658816   -0.676114 -0.002227    0.672544      4.45492  \n",
              "f_04      -4.748501   -0.675909 -0.001662    0.673789     4.948983  \n",
              "f_05      -4.750214   -0.673437 -0.000438    0.675028     4.971881  \n",
              "f_06      -4.842919   -0.674876 -0.001492    0.674749     4.822668  \n",
              "f_07            0.0         1.0       2.0         3.0         15.0  \n",
              "f_08            0.0         1.0       2.0         3.0         16.0  \n",
              "f_09            0.0         1.0       2.0         3.0         14.0  \n",
              "f_10            0.0         1.0       2.0         3.0         14.0  \n",
              "f_11            0.0         1.0       2.0         3.0         13.0  \n",
              "f_12            0.0         2.0       3.0         4.0         16.0  \n",
              "f_13            0.0         1.0       2.0         3.0         12.0  \n",
              "f_14            0.0         0.0       1.0         2.0         14.0  \n",
              "f_15            0.0         1.0       2.0         3.0         14.0  \n",
              "f_16            0.0         1.0       2.0         3.0         15.0  \n",
              "f_17            0.0         1.0       2.0         3.0         14.0  \n",
              "f_18            0.0         1.0       2.0         3.0         13.0  \n",
              "f_19     -11.280941   -1.236061  0.330249    1.880517    12.079667  \n",
              "f_20     -11.257917   -1.804612 -0.190571    1.444508    11.475325  \n",
              "f_21     -13.310146   -1.820063 -0.152668    1.507071    14.455426  \n",
              "f_22      -11.85353   -1.645585   0.03085    1.661676     11.34408  \n",
              "f_23     -12.301097   -2.019739 -0.390966    1.255408      12.2471  \n",
              "f_24     -11.416189   -1.955956 -0.340746    1.266673    12.389844  \n",
              "f_25     -11.918306   -1.440424  0.160912    1.795928    12.529179  \n",
              "f_26     -14.300577   -1.261598  0.404212    2.028219    12.913041  \n",
              "f_27            NaN         NaN       NaN         NaN          NaN  \n",
              "f_28   -1229.753052 -159.427418 -0.519808  158.987357  1229.562577  \n",
              "f_29            0.0         0.0       0.0         1.0          1.0  \n",
              "f_30            0.0         0.0       1.0         2.0          2.0  \n",
              "target          0.0         0.0       0.0         1.0          1.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5cb63fe6-ca67-402d-b448-b6c732bad511\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>unique</th>\n",
              "      <th>top</th>\n",
              "      <th>freq</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>449999.5</td>\n",
              "      <td>259807.765474</td>\n",
              "      <td>0.0</td>\n",
              "      <td>224999.75</td>\n",
              "      <td>449999.5</td>\n",
              "      <td>674999.25</td>\n",
              "      <td>899999.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_00</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.000286</td>\n",
              "      <td>0.998888</td>\n",
              "      <td>-4.599856</td>\n",
              "      <td>-0.67549</td>\n",
              "      <td>0.001144</td>\n",
              "      <td>0.674337</td>\n",
              "      <td>4.749301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_01</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.001165</td>\n",
              "      <td>0.999193</td>\n",
              "      <td>-4.682199</td>\n",
              "      <td>-0.675162</td>\n",
              "      <td>0.002014</td>\n",
              "      <td>0.675021</td>\n",
              "      <td>4.815699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_02</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.001174</td>\n",
              "      <td>1.000514</td>\n",
              "      <td>-4.642676</td>\n",
              "      <td>-0.674369</td>\n",
              "      <td>0.002218</td>\n",
              "      <td>0.677505</td>\n",
              "      <td>4.961982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_03</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.001368</td>\n",
              "      <td>1.000175</td>\n",
              "      <td>-4.658816</td>\n",
              "      <td>-0.676114</td>\n",
              "      <td>-0.002227</td>\n",
              "      <td>0.672544</td>\n",
              "      <td>4.45492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_04</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.000571</td>\n",
              "      <td>1.000167</td>\n",
              "      <td>-4.748501</td>\n",
              "      <td>-0.675909</td>\n",
              "      <td>-0.001662</td>\n",
              "      <td>0.673789</td>\n",
              "      <td>4.948983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_05</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000284</td>\n",
              "      <td>0.999875</td>\n",
              "      <td>-4.750214</td>\n",
              "      <td>-0.673437</td>\n",
              "      <td>-0.000438</td>\n",
              "      <td>0.675028</td>\n",
              "      <td>4.971881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_06</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.000709</td>\n",
              "      <td>0.999942</td>\n",
              "      <td>-4.842919</td>\n",
              "      <td>-0.674876</td>\n",
              "      <td>-0.001492</td>\n",
              "      <td>0.674749</td>\n",
              "      <td>4.822668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_07</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.03146</td>\n",
              "      <td>1.656172</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_08</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.057998</td>\n",
              "      <td>1.590955</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_09</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.362431</td>\n",
              "      <td>1.637706</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>14.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_10</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.177637</td>\n",
              "      <td>1.645953</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>14.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_11</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.803392</td>\n",
              "      <td>1.537487</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_12</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.842373</td>\n",
              "      <td>1.762835</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_13</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.239778</td>\n",
              "      <td>1.538426</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_14</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.514686</td>\n",
              "      <td>1.359213</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>14.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_15</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.101132</td>\n",
              "      <td>1.569093</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>14.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_16</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.096713</td>\n",
              "      <td>1.560169</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_17</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.858518</td>\n",
              "      <td>1.467675</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>14.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_18</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.065131</td>\n",
              "      <td>1.564783</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_19</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.308713</td>\n",
              "      <td>2.316026</td>\n",
              "      <td>-11.280941</td>\n",
              "      <td>-1.236061</td>\n",
              "      <td>0.330249</td>\n",
              "      <td>1.880517</td>\n",
              "      <td>12.079667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_20</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.17873</td>\n",
              "      <td>2.400494</td>\n",
              "      <td>-11.257917</td>\n",
              "      <td>-1.804612</td>\n",
              "      <td>-0.190571</td>\n",
              "      <td>1.444508</td>\n",
              "      <td>11.475325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_21</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.156307</td>\n",
              "      <td>2.484706</td>\n",
              "      <td>-13.310146</td>\n",
              "      <td>-1.820063</td>\n",
              "      <td>-0.152668</td>\n",
              "      <td>1.507071</td>\n",
              "      <td>14.455426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_22</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.009273</td>\n",
              "      <td>2.450797</td>\n",
              "      <td>-11.85353</td>\n",
              "      <td>-1.645585</td>\n",
              "      <td>0.03085</td>\n",
              "      <td>1.661676</td>\n",
              "      <td>11.34408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_23</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.369459</td>\n",
              "      <td>2.453405</td>\n",
              "      <td>-12.301097</td>\n",
              "      <td>-2.019739</td>\n",
              "      <td>-0.390966</td>\n",
              "      <td>1.255408</td>\n",
              "      <td>12.2471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_24</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.342738</td>\n",
              "      <td>2.386941</td>\n",
              "      <td>-11.416189</td>\n",
              "      <td>-1.955956</td>\n",
              "      <td>-0.340746</td>\n",
              "      <td>1.266673</td>\n",
              "      <td>12.389844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_25</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.176549</td>\n",
              "      <td>2.416959</td>\n",
              "      <td>-11.918306</td>\n",
              "      <td>-1.440424</td>\n",
              "      <td>0.160912</td>\n",
              "      <td>1.795928</td>\n",
              "      <td>12.529179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_26</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.357591</td>\n",
              "      <td>2.47602</td>\n",
              "      <td>-14.300577</td>\n",
              "      <td>-1.261598</td>\n",
              "      <td>0.404212</td>\n",
              "      <td>2.028219</td>\n",
              "      <td>12.913041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_27</th>\n",
              "      <td>900000</td>\n",
              "      <td>741354</td>\n",
              "      <td>BBBBBBCJBC</td>\n",
              "      <td>12</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_28</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.380876</td>\n",
              "      <td>238.773054</td>\n",
              "      <td>-1229.753052</td>\n",
              "      <td>-159.427418</td>\n",
              "      <td>-0.519808</td>\n",
              "      <td>158.987357</td>\n",
              "      <td>1229.562577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_29</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.345661</td>\n",
              "      <td>0.475584</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f_30</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.002654</td>\n",
              "      <td>0.818989</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>target</th>\n",
              "      <td>900000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.486488</td>\n",
              "      <td>0.499818</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5cb63fe6-ca67-402d-b448-b6c732bad511')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5cb63fe6-ca67-402d-b448-b6c732bad511 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5cb63fe6-ca67-402d-b448-b6c732bad511');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explore Data**"
      ],
      "metadata": {
        "id": "No2jgz10qOQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.countplot(train[\"target\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "oplX9WpDsZO4",
        "outputId": "f2955ab6-9a28-43fc-f6f4-e4556caecbce"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f98c65af050>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop f_27 as its the only feture with Object type\n",
        "def unique_char(string):\n",
        "    return(len(set(string)))\n",
        "\n",
        "train['unique_length'] = train['f_27'].apply(unique_char)\n",
        "test['unique_length'] = test['f_27'].apply(unique_char)\n",
        "\n",
        "train = train.drop([\"id\", \"f_27\"], axis=1)\n",
        "test = test.drop([\"id\", \"f_27\"], axis=1)"
      ],
      "metadata": {
        "id": "WZjUclctq1H-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spearman Correlation**"
      ],
      "metadata": {
        "id": "yMHhP_ALrGZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s_corr = train.corr(method=\"spearman\")\n",
        "s_corr.style.background_gradient(cmap=\"Blues\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tgIH0Wb7rFkq",
        "outputId": "3f8a2fe7-46b5-4b9f-c595-2535854e712b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f98d493bf90>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_fc7b8_row0_col0, #T_fc7b8_row1_col1, #T_fc7b8_row2_col2, #T_fc7b8_row3_col3, #T_fc7b8_row4_col4, #T_fc7b8_row5_col5, #T_fc7b8_row6_col6, #T_fc7b8_row7_col7, #T_fc7b8_row8_col8, #T_fc7b8_row9_col9, #T_fc7b8_row10_col10, #T_fc7b8_row11_col11, #T_fc7b8_row12_col12, #T_fc7b8_row13_col13, #T_fc7b8_row14_col14, #T_fc7b8_row15_col15, #T_fc7b8_row16_col16, #T_fc7b8_row17_col17, #T_fc7b8_row18_col18, #T_fc7b8_row19_col19, #T_fc7b8_row20_col20, #T_fc7b8_row21_col21, #T_fc7b8_row22_col22, #T_fc7b8_row23_col23, #T_fc7b8_row24_col24, #T_fc7b8_row25_col25, #T_fc7b8_row26_col26, #T_fc7b8_row27_col27, #T_fc7b8_row28_col28, #T_fc7b8_row29_col29, #T_fc7b8_row30_col30, #T_fc7b8_row31_col31 {\n",
              "  background-color: #08306b;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_fc7b8_row0_col1, #T_fc7b8_row0_col3, #T_fc7b8_row0_col4, #T_fc7b8_row0_col5, #T_fc7b8_row0_col6, #T_fc7b8_row1_col0, #T_fc7b8_row1_col2, #T_fc7b8_row1_col3, #T_fc7b8_row1_col4, #T_fc7b8_row1_col5, #T_fc7b8_row1_col6, #T_fc7b8_row2_col1, #T_fc7b8_row2_col3, #T_fc7b8_row2_col4, #T_fc7b8_row2_col5, #T_fc7b8_row2_col6, #T_fc7b8_row3_col0, #T_fc7b8_row3_col1, #T_fc7b8_row3_col2, #T_fc7b8_row3_col6, #T_fc7b8_row4_col0, #T_fc7b8_row4_col1, #T_fc7b8_row4_col2, #T_fc7b8_row4_col3, #T_fc7b8_row4_col5, #T_fc7b8_row4_col6, #T_fc7b8_row5_col0, #T_fc7b8_row5_col1, #T_fc7b8_row5_col2, #T_fc7b8_row5_col4, #T_fc7b8_row5_col6, #T_fc7b8_row6_col0, #T_fc7b8_row6_col1, #T_fc7b8_row6_col2, #T_fc7b8_row6_col3, #T_fc7b8_row6_col4, #T_fc7b8_row6_col5, #T_fc7b8_row7_col0, #T_fc7b8_row7_col1, #T_fc7b8_row7_col3, #T_fc7b8_row7_col5, #T_fc7b8_row7_col6, #T_fc7b8_row7_col10, #T_fc7b8_row7_col11, #T_fc7b8_row7_col17, #T_fc7b8_row7_col27, #T_fc7b8_row7_col31, #T_fc7b8_row8_col0, #T_fc7b8_row8_col1, #T_fc7b8_row8_col2, #T_fc7b8_row8_col3, #T_fc7b8_row8_col4, #T_fc7b8_row8_col5, #T_fc7b8_row8_col6, #T_fc7b8_row8_col27, #T_fc7b8_row9_col0, #T_fc7b8_row9_col1, #T_fc7b8_row9_col3, #T_fc7b8_row9_col5, #T_fc7b8_row9_col6, #T_fc7b8_row9_col13, #T_fc7b8_row9_col18, #T_fc7b8_row9_col27, #T_fc7b8_row10_col0, #T_fc7b8_row10_col1, #T_fc7b8_row10_col3, #T_fc7b8_row10_col4, #T_fc7b8_row10_col5, #T_fc7b8_row10_col6, #T_fc7b8_row10_col27, #T_fc7b8_row11_col1, #T_fc7b8_row11_col2, #T_fc7b8_row11_col4, #T_fc7b8_row11_col5, #T_fc7b8_row11_col6, #T_fc7b8_row11_col8, #T_fc7b8_row11_col15, #T_fc7b8_row11_col16, #T_fc7b8_row11_col27, #T_fc7b8_row12_col0, #T_fc7b8_row12_col1, #T_fc7b8_row12_col3, #T_fc7b8_row12_col27, #T_fc7b8_row12_col28, #T_fc7b8_row13_col0, #T_fc7b8_row13_col1, #T_fc7b8_row13_col2, #T_fc7b8_row13_col3, #T_fc7b8_row13_col4, #T_fc7b8_row13_col5, #T_fc7b8_row13_col6, #T_fc7b8_row13_col27, #T_fc7b8_row14_col0, #T_fc7b8_row14_col1, #T_fc7b8_row14_col2, #T_fc7b8_row14_col3, #T_fc7b8_row14_col5, #T_fc7b8_row14_col6, #T_fc7b8_row14_col16, #T_fc7b8_row15_col0, #T_fc7b8_row15_col1, #T_fc7b8_row15_col2, #T_fc7b8_row15_col3, #T_fc7b8_row15_col4, #T_fc7b8_row15_col5, #T_fc7b8_row15_col6, #T_fc7b8_row15_col27, #T_fc7b8_row16_col0, #T_fc7b8_row16_col1, #T_fc7b8_row16_col2, #T_fc7b8_row16_col3, #T_fc7b8_row16_col4, #T_fc7b8_row16_col5, #T_fc7b8_row16_col6, #T_fc7b8_row16_col14, #T_fc7b8_row16_col27, #T_fc7b8_row17_col0, #T_fc7b8_row17_col1, #T_fc7b8_row17_col2, #T_fc7b8_row17_col3, #T_fc7b8_row17_col4, #T_fc7b8_row17_col5, #T_fc7b8_row17_col6, #T_fc7b8_row17_col7, #T_fc7b8_row17_col27, #T_fc7b8_row18_col0, #T_fc7b8_row18_col1, #T_fc7b8_row18_col2, #T_fc7b8_row18_col3, #T_fc7b8_row18_col4, #T_fc7b8_row18_col5, #T_fc7b8_row18_col6, #T_fc7b8_row18_col27, #T_fc7b8_row19_col0, #T_fc7b8_row19_col1, #T_fc7b8_row19_col2, #T_fc7b8_row19_col3, #T_fc7b8_row19_col5, #T_fc7b8_row19_col20, #T_fc7b8_row19_col24, #T_fc7b8_row19_col27, #T_fc7b8_row19_col30, #T_fc7b8_row20_col0, #T_fc7b8_row20_col1, #T_fc7b8_row20_col2, #T_fc7b8_row20_col3, #T_fc7b8_row20_col4, #T_fc7b8_row20_col5, #T_fc7b8_row20_col6, #T_fc7b8_row20_col27, #T_fc7b8_row21_col0, #T_fc7b8_row21_col1, #T_fc7b8_row21_col2, #T_fc7b8_row21_col3, #T_fc7b8_row21_col4, #T_fc7b8_row21_col5, #T_fc7b8_row21_col6, #T_fc7b8_row21_col22, #T_fc7b8_row21_col27, #T_fc7b8_row22_col0, #T_fc7b8_row22_col1, #T_fc7b8_row22_col2, #T_fc7b8_row22_col3, #T_fc7b8_row22_col6, #T_fc7b8_row22_col27, #T_fc7b8_row23_col0, #T_fc7b8_row23_col1, #T_fc7b8_row23_col2, #T_fc7b8_row23_col3, #T_fc7b8_row23_col4, #T_fc7b8_row23_col5, #T_fc7b8_row23_col6, #T_fc7b8_row23_col25, #T_fc7b8_row23_col27, #T_fc7b8_row23_col29, #T_fc7b8_row24_col1, #T_fc7b8_row24_col2, #T_fc7b8_row24_col4, #T_fc7b8_row24_col5, #T_fc7b8_row24_col6, #T_fc7b8_row24_col19, #T_fc7b8_row24_col27, #T_fc7b8_row25_col0, #T_fc7b8_row25_col1, #T_fc7b8_row25_col2, #T_fc7b8_row25_col4, #T_fc7b8_row25_col6, #T_fc7b8_row25_col23, #T_fc7b8_row25_col27, #T_fc7b8_row26_col0, #T_fc7b8_row26_col1, #T_fc7b8_row26_col2, #T_fc7b8_row26_col3, #T_fc7b8_row26_col4, #T_fc7b8_row26_col6, #T_fc7b8_row26_col27, #T_fc7b8_row28_col0, #T_fc7b8_row28_col1, #T_fc7b8_row28_col2, #T_fc7b8_row28_col3, #T_fc7b8_row28_col4, #T_fc7b8_row28_col5, #T_fc7b8_row28_col6, #T_fc7b8_row28_col9, #T_fc7b8_row28_col12, #T_fc7b8_row28_col27, #T_fc7b8_row29_col0, #T_fc7b8_row29_col1, #T_fc7b8_row29_col2, #T_fc7b8_row29_col3, #T_fc7b8_row29_col4, #T_fc7b8_row29_col5, #T_fc7b8_row29_col6, #T_fc7b8_row29_col21, #T_fc7b8_row29_col27, #T_fc7b8_row30_col3, #T_fc7b8_row30_col4, #T_fc7b8_row30_col6, #T_fc7b8_row31_col0, #T_fc7b8_row31_col1, #T_fc7b8_row31_col2, #T_fc7b8_row31_col3, #T_fc7b8_row31_col4, #T_fc7b8_row31_col5, #T_fc7b8_row31_col6, #T_fc7b8_row31_col26, #T_fc7b8_row31_col27 {\n",
              "  background-color: #f7fbff;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row0_col2, #T_fc7b8_row2_col0, #T_fc7b8_row3_col4, #T_fc7b8_row3_col5, #T_fc7b8_row5_col3, #T_fc7b8_row7_col2, #T_fc7b8_row7_col4, #T_fc7b8_row8_col10, #T_fc7b8_row9_col2, #T_fc7b8_row9_col4, #T_fc7b8_row10_col2, #T_fc7b8_row11_col0, #T_fc7b8_row11_col3, #T_fc7b8_row12_col2, #T_fc7b8_row12_col4, #T_fc7b8_row12_col5, #T_fc7b8_row12_col6, #T_fc7b8_row14_col4, #T_fc7b8_row14_col27, #T_fc7b8_row17_col13, #T_fc7b8_row19_col4, #T_fc7b8_row19_col6, #T_fc7b8_row22_col4, #T_fc7b8_row22_col5, #T_fc7b8_row24_col0, #T_fc7b8_row24_col3, #T_fc7b8_row25_col3, #T_fc7b8_row25_col5, #T_fc7b8_row26_col5 {\n",
              "  background-color: #f6faff;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row0_col7, #T_fc7b8_row0_col17, #T_fc7b8_row0_col21, #T_fc7b8_row1_col17, #T_fc7b8_row1_col21, #T_fc7b8_row2_col7, #T_fc7b8_row2_col17, #T_fc7b8_row2_col21, #T_fc7b8_row3_col7, #T_fc7b8_row3_col17, #T_fc7b8_row3_col21, #T_fc7b8_row4_col7, #T_fc7b8_row4_col17, #T_fc7b8_row4_col21, #T_fc7b8_row5_col7, #T_fc7b8_row5_col17, #T_fc7b8_row5_col21, #T_fc7b8_row6_col7, #T_fc7b8_row6_col17, #T_fc7b8_row6_col21, #T_fc7b8_row9_col12, #T_fc7b8_row12_col21, #T_fc7b8_row13_col21, #T_fc7b8_row14_col21, #T_fc7b8_row16_col21, #T_fc7b8_row17_col14, #T_fc7b8_row17_col21, #T_fc7b8_row18_col21, #T_fc7b8_row19_col17, #T_fc7b8_row20_col17, #T_fc7b8_row20_col25, #T_fc7b8_row21_col7, #T_fc7b8_row21_col17, #T_fc7b8_row22_col7, #T_fc7b8_row22_col17, #T_fc7b8_row24_col7, #T_fc7b8_row25_col17, #T_fc7b8_row26_col17, #T_fc7b8_row27_col7, #T_fc7b8_row27_col17, #T_fc7b8_row27_col21, #T_fc7b8_row29_col7, #T_fc7b8_row29_col17 {\n",
              "  background-color: #ddeaf7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row0_col8, #T_fc7b8_row0_col14, #T_fc7b8_row1_col8, #T_fc7b8_row1_col9, #T_fc7b8_row1_col19, #T_fc7b8_row1_col24, #T_fc7b8_row2_col14, #T_fc7b8_row2_col19, #T_fc7b8_row2_col24, #T_fc7b8_row3_col8, #T_fc7b8_row3_col16, #T_fc7b8_row4_col8, #T_fc7b8_row4_col14, #T_fc7b8_row4_col24, #T_fc7b8_row5_col8, #T_fc7b8_row5_col9, #T_fc7b8_row5_col14, #T_fc7b8_row5_col19, #T_fc7b8_row5_col24, #T_fc7b8_row6_col8, #T_fc7b8_row6_col9, #T_fc7b8_row6_col16, #T_fc7b8_row6_col24, #T_fc7b8_row7_col19, #T_fc7b8_row7_col30, #T_fc7b8_row8_col17, #T_fc7b8_row13_col24, #T_fc7b8_row15_col9, #T_fc7b8_row15_col19, #T_fc7b8_row16_col12, #T_fc7b8_row16_col24, #T_fc7b8_row17_col19, #T_fc7b8_row17_col24, #T_fc7b8_row18_col19, #T_fc7b8_row19_col14, #T_fc7b8_row19_col16, #T_fc7b8_row20_col14, #T_fc7b8_row20_col16, #T_fc7b8_row21_col16, #T_fc7b8_row22_col8, #T_fc7b8_row22_col9, #T_fc7b8_row24_col14, #T_fc7b8_row25_col8, #T_fc7b8_row25_col31, #T_fc7b8_row26_col14, #T_fc7b8_row26_col16, #T_fc7b8_row27_col8, #T_fc7b8_row27_col14, #T_fc7b8_row27_col24, #T_fc7b8_row29_col9 {\n",
              "  background-color: #e6f0f9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row0_col9, #T_fc7b8_row0_col19, #T_fc7b8_row0_col24, #T_fc7b8_row2_col8, #T_fc7b8_row2_col9, #T_fc7b8_row3_col9, #T_fc7b8_row3_col19, #T_fc7b8_row3_col24, #T_fc7b8_row4_col9, #T_fc7b8_row4_col19, #T_fc7b8_row6_col19, #T_fc7b8_row7_col15, #T_fc7b8_row7_col24, #T_fc7b8_row12_col19, #T_fc7b8_row12_col24, #T_fc7b8_row14_col10, #T_fc7b8_row14_col12, #T_fc7b8_row14_col19, #T_fc7b8_row14_col24, #T_fc7b8_row15_col24, #T_fc7b8_row16_col19, #T_fc7b8_row17_col10, #T_fc7b8_row18_col24, #T_fc7b8_row21_col8, #T_fc7b8_row23_col8, #T_fc7b8_row24_col8, #T_fc7b8_row27_col9, #T_fc7b8_row27_col19, #T_fc7b8_row29_col8, #T_fc7b8_row31_col16 {\n",
              "  background-color: #e5eff9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row0_col10, #T_fc7b8_row1_col10, #T_fc7b8_row2_col10, #T_fc7b8_row3_col10, #T_fc7b8_row5_col10, #T_fc7b8_row6_col10, #T_fc7b8_row8_col13, #T_fc7b8_row9_col19, #T_fc7b8_row9_col31, #T_fc7b8_row10_col31, #T_fc7b8_row11_col12, #T_fc7b8_row11_col31, #T_fc7b8_row13_col15, #T_fc7b8_row14_col7, #T_fc7b8_row15_col13, #T_fc7b8_row19_col9, #T_fc7b8_row22_col10, #T_fc7b8_row25_col10, #T_fc7b8_row26_col9, #T_fc7b8_row27_col10, #T_fc7b8_row29_col31, #T_fc7b8_row30_col14 {\n",
              "  background-color: #e7f1fa;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row0_col11, #T_fc7b8_row0_col22, #T_fc7b8_row1_col11, #T_fc7b8_row3_col11, #T_fc7b8_row4_col11, #T_fc7b8_row6_col11, #T_fc7b8_row12_col22, #T_fc7b8_row14_col9, #T_fc7b8_row22_col11, #T_fc7b8_row25_col11, #T_fc7b8_row27_col11, #T_fc7b8_row31_col28 {\n",
              "  background-color: #dfecf7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row0_col12, #T_fc7b8_row0_col28, #T_fc7b8_row1_col12, #T_fc7b8_row1_col28, #T_fc7b8_row2_col28, #T_fc7b8_row3_col28, #T_fc7b8_row4_col28, #T_fc7b8_row5_col28, #T_fc7b8_row7_col9, #T_fc7b8_row7_col13, #T_fc7b8_row9_col24, #T_fc7b8_row12_col8, #T_fc7b8_row15_col8, #T_fc7b8_row19_col12, #T_fc7b8_row20_col12, #T_fc7b8_row20_col28, #T_fc7b8_row21_col12, #T_fc7b8_row22_col12, #T_fc7b8_row23_col9, #T_fc7b8_row24_col9, #T_fc7b8_row24_col12, #T_fc7b8_row25_col12, #T_fc7b8_row25_col28, #T_fc7b8_row26_col12, #T_fc7b8_row27_col12, #T_fc7b8_row27_col28, #T_fc7b8_row28_col7, #T_fc7b8_row29_col12, #T_fc7b8_row29_col28 {\n",
              "  background-color: #e3eef9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row0_col13, #T_fc7b8_row0_col18, #T_fc7b8_row1_col15, #T_fc7b8_row1_col18, #T_fc7b8_row2_col15, #T_fc7b8_row2_col18, #T_fc7b8_row3_col18, #T_fc7b8_row4_col13, #T_fc7b8_row4_col15, #T_fc7b8_row4_col18, #T_fc7b8_row5_col13, #T_fc7b8_row5_col15, #T_fc7b8_row5_col18, #T_fc7b8_row6_col13, #T_fc7b8_row6_col15, #T_fc7b8_row6_col18, #T_fc7b8_row8_col14, #T_fc7b8_row10_col9, #T_fc7b8_row10_col16, #T_fc7b8_row12_col10, #T_fc7b8_row13_col9, #T_fc7b8_row13_col30, #T_fc7b8_row16_col30, #T_fc7b8_row17_col8, #T_fc7b8_row18_col9, #T_fc7b8_row19_col13, #T_fc7b8_row19_col15, #T_fc7b8_row19_col18, #T_fc7b8_row20_col13, #T_fc7b8_row20_col18, #T_fc7b8_row20_col31, #T_fc7b8_row21_col15, #T_fc7b8_row21_col18, #T_fc7b8_row22_col13, #T_fc7b8_row22_col15, #T_fc7b8_row22_col18, #T_fc7b8_row23_col15, #T_fc7b8_row23_col18, #T_fc7b8_row23_col19, #T_fc7b8_row24_col15, #T_fc7b8_row24_col18, #T_fc7b8_row24_col26, #T_fc7b8_row25_col15, #T_fc7b8_row25_col18, #T_fc7b8_row25_col30, #T_fc7b8_row26_col13, #T_fc7b8_row26_col18, #T_fc7b8_row27_col18, #T_fc7b8_row28_col15, #T_fc7b8_row29_col15, #T_fc7b8_row29_col18, #T_fc7b8_row29_col23, #T_fc7b8_row31_col15 {\n",
              "  background-color: #eef5fc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row0_col15, #T_fc7b8_row1_col13, #T_fc7b8_row2_col13, #T_fc7b8_row3_col13, #T_fc7b8_row3_col15, #T_fc7b8_row9_col15, #T_fc7b8_row10_col15, #T_fc7b8_row10_col26, #T_fc7b8_row10_col30, #T_fc7b8_row11_col26, #T_fc7b8_row12_col18, #T_fc7b8_row13_col26, #T_fc7b8_row16_col10, #T_fc7b8_row16_col26, #T_fc7b8_row20_col15, #T_fc7b8_row20_col30, #T_fc7b8_row21_col13, #T_fc7b8_row23_col13, #T_fc7b8_row24_col13, #T_fc7b8_row25_col13, #T_fc7b8_row26_col15, #T_fc7b8_row27_col13, #T_fc7b8_row27_col15, #T_fc7b8_row29_col13, #T_fc7b8_row30_col10 {\n",
              "  background-color: #eff6fc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row0_col16, #T_fc7b8_row1_col14, #T_fc7b8_row1_col16, #T_fc7b8_row2_col16, #T_fc7b8_row3_col14, #T_fc7b8_row4_col16, #T_fc7b8_row5_col16, #T_fc7b8_row6_col14, #T_fc7b8_row8_col19, #T_fc7b8_row10_col24, #T_fc7b8_row11_col24, #T_fc7b8_row13_col18, #T_fc7b8_row14_col28, #T_fc7b8_row15_col12, #T_fc7b8_row17_col30, #T_fc7b8_row18_col13, #T_fc7b8_row19_col8, #T_fc7b8_row19_col10, #T_fc7b8_row19_col31, #T_fc7b8_row20_col8, #T_fc7b8_row20_col9, #T_fc7b8_row20_col10, #T_fc7b8_row21_col14, #T_fc7b8_row22_col14, #T_fc7b8_row22_col16, #T_fc7b8_row23_col14, #T_fc7b8_row23_col16, #T_fc7b8_row24_col16, #T_fc7b8_row25_col9, #T_fc7b8_row25_col14, #T_fc7b8_row25_col16, #T_fc7b8_row26_col8, #T_fc7b8_row26_col10, #T_fc7b8_row27_col16, #T_fc7b8_row28_col19, #T_fc7b8_row28_col31, #T_fc7b8_row29_col14, #T_fc7b8_row29_col16 {\n",
              "  background-color: #e7f0fa;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row0_col20, #T_fc7b8_row5_col20, #T_fc7b8_row6_col20, #T_fc7b8_row7_col20, #T_fc7b8_row7_col28, #T_fc7b8_row10_col12, #T_fc7b8_row12_col20, #T_fc7b8_row15_col20, #T_fc7b8_row15_col31, #T_fc7b8_row18_col20, #T_fc7b8_row18_col30, #T_fc7b8_row20_col22, #T_fc7b8_row23_col26, #T_fc7b8_row28_col14, #T_fc7b8_row28_col20, #T_fc7b8_row31_col7 {\n",
              "  background-color: #eaf2fb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row0_col23, #T_fc7b8_row0_col25, #T_fc7b8_row1_col23, #T_fc7b8_row1_col25, #T_fc7b8_row2_col23, #T_fc7b8_row2_col25, #T_fc7b8_row3_col23, #T_fc7b8_row3_col25, #T_fc7b8_row5_col23, #T_fc7b8_row5_col25, #T_fc7b8_row6_col23, #T_fc7b8_row6_col25, #T_fc7b8_row7_col25, #T_fc7b8_row10_col25, #T_fc7b8_row11_col23, #T_fc7b8_row11_col25, #T_fc7b8_row11_col28, #T_fc7b8_row12_col25, #T_fc7b8_row13_col23, #T_fc7b8_row13_col25, #T_fc7b8_row14_col25, #T_fc7b8_row15_col25, #T_fc7b8_row16_col25, #T_fc7b8_row17_col11, #T_fc7b8_row17_col23, #T_fc7b8_row17_col25, #T_fc7b8_row18_col23, #T_fc7b8_row18_col25, #T_fc7b8_row27_col23, #T_fc7b8_row27_col25 {\n",
              "  background-color: #d3e3f3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row0_col26, #T_fc7b8_row1_col26, #T_fc7b8_row2_col26, #T_fc7b8_row3_col26, #T_fc7b8_row4_col26, #T_fc7b8_row5_col26, #T_fc7b8_row6_col26, #T_fc7b8_row9_col10, #T_fc7b8_row11_col13, #T_fc7b8_row12_col26, #T_fc7b8_row13_col31, #T_fc7b8_row14_col26, #T_fc7b8_row15_col18, #T_fc7b8_row16_col11, #T_fc7b8_row16_col18, #T_fc7b8_row17_col26, #T_fc7b8_row18_col31, #T_fc7b8_row22_col19, #T_fc7b8_row23_col22, #T_fc7b8_row27_col26, #T_fc7b8_row30_col18, #T_fc7b8_row30_col27, #T_fc7b8_row31_col8 {\n",
              "  background-color: #f0f6fd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row0_col27, #T_fc7b8_row16_col7, #T_fc7b8_row27_col0, #T_fc7b8_row31_col23, #T_fc7b8_row31_col30 {\n",
              "  background-color: #d4e4f4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row0_col29, #T_fc7b8_row1_col29, #T_fc7b8_row2_col29, #T_fc7b8_row3_col29, #T_fc7b8_row4_col29, #T_fc7b8_row5_col29, #T_fc7b8_row7_col29, #T_fc7b8_row8_col29, #T_fc7b8_row9_col29, #T_fc7b8_row10_col29, #T_fc7b8_row11_col29, #T_fc7b8_row12_col29, #T_fc7b8_row13_col29, #T_fc7b8_row14_col29, #T_fc7b8_row16_col29, #T_fc7b8_row18_col29, #T_fc7b8_row19_col23, #T_fc7b8_row24_col20, #T_fc7b8_row27_col29, #T_fc7b8_row28_col29, #T_fc7b8_row30_col9, #T_fc7b8_row30_col28 {\n",
              "  background-color: #dae8f6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row0_col30, #T_fc7b8_row1_col7, #T_fc7b8_row1_col30, #T_fc7b8_row10_col21, #T_fc7b8_row11_col18, #T_fc7b8_row11_col21, #T_fc7b8_row16_col8, #T_fc7b8_row19_col7, #T_fc7b8_row20_col7, #T_fc7b8_row22_col30, #T_fc7b8_row23_col17, #T_fc7b8_row24_col17, #T_fc7b8_row25_col7, #T_fc7b8_row26_col7, #T_fc7b8_row26_col29 {\n",
              "  background-color: #deebf7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row0_col31, #T_fc7b8_row1_col31, #T_fc7b8_row2_col31, #T_fc7b8_row3_col31, #T_fc7b8_row4_col31, #T_fc7b8_row5_col31, #T_fc7b8_row6_col31, #T_fc7b8_row8_col20, #T_fc7b8_row9_col20, #T_fc7b8_row17_col12, #T_fc7b8_row25_col21, #T_fc7b8_row27_col31, #T_fc7b8_row28_col17 {\n",
              "  background-color: #eaf3fb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row1_col20, #T_fc7b8_row2_col20, #T_fc7b8_row3_col20, #T_fc7b8_row4_col20, #T_fc7b8_row7_col18, #T_fc7b8_row9_col16, #T_fc7b8_row10_col20, #T_fc7b8_row12_col16, #T_fc7b8_row12_col30, #T_fc7b8_row13_col14, #T_fc7b8_row13_col16, #T_fc7b8_row13_col20, #T_fc7b8_row14_col20, #T_fc7b8_row14_col30, #T_fc7b8_row16_col20, #T_fc7b8_row16_col31, #T_fc7b8_row17_col20, #T_fc7b8_row19_col22, #T_fc7b8_row22_col24, #T_fc7b8_row25_col19, #T_fc7b8_row27_col20, #T_fc7b8_row30_col15, #T_fc7b8_row30_col26, #T_fc7b8_row31_col12 {\n",
              "  background-color: #e9f2fa;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row1_col22, #T_fc7b8_row2_col22, #T_fc7b8_row3_col22, #T_fc7b8_row4_col22, #T_fc7b8_row5_col22, #T_fc7b8_row6_col22, #T_fc7b8_row7_col22, #T_fc7b8_row8_col16, #T_fc7b8_row8_col22, #T_fc7b8_row8_col30, #T_fc7b8_row9_col11, #T_fc7b8_row9_col22, #T_fc7b8_row10_col22, #T_fc7b8_row11_col22, #T_fc7b8_row12_col9, #T_fc7b8_row13_col8, #T_fc7b8_row13_col22, #T_fc7b8_row14_col22, #T_fc7b8_row15_col22, #T_fc7b8_row16_col22, #T_fc7b8_row17_col22, #T_fc7b8_row18_col22, #T_fc7b8_row19_col11, #T_fc7b8_row20_col11, #T_fc7b8_row20_col21, #T_fc7b8_row21_col25, #T_fc7b8_row24_col25, #T_fc7b8_row26_col11, #T_fc7b8_row27_col22, #T_fc7b8_row28_col22, #T_fc7b8_row28_col30, #T_fc7b8_row31_col21 {\n",
              "  background-color: #dfebf7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row1_col27, #T_fc7b8_row4_col23, #T_fc7b8_row4_col25, #T_fc7b8_row8_col25, #T_fc7b8_row9_col25, #T_fc7b8_row10_col23, #T_fc7b8_row13_col7, #T_fc7b8_row14_col23, #T_fc7b8_row16_col23, #T_fc7b8_row21_col30, #T_fc7b8_row26_col25, #T_fc7b8_row27_col1, #T_fc7b8_row28_col25, #T_fc7b8_row29_col19 {\n",
              "  background-color: #d3e4f3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row2_col11, #T_fc7b8_row5_col11, #T_fc7b8_row9_col14, #T_fc7b8_row13_col12, #T_fc7b8_row15_col14, #T_fc7b8_row21_col11, #T_fc7b8_row23_col11, #T_fc7b8_row24_col11, #T_fc7b8_row29_col11 {\n",
              "  background-color: #e0ecf8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row2_col12, #T_fc7b8_row2_col30, #T_fc7b8_row3_col12, #T_fc7b8_row4_col12, #T_fc7b8_row5_col12, #T_fc7b8_row6_col12, #T_fc7b8_row6_col28, #T_fc7b8_row15_col17, #T_fc7b8_row15_col28, #T_fc7b8_row15_col30, #T_fc7b8_row21_col28, #T_fc7b8_row22_col28, #T_fc7b8_row23_col12, #T_fc7b8_row23_col24, #T_fc7b8_row26_col22 {\n",
              "  background-color: #e3eef8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row2_col27, #T_fc7b8_row20_col29, #T_fc7b8_row26_col23, #T_fc7b8_row27_col2 {\n",
              "  background-color: #cde0f1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row3_col27, #T_fc7b8_row27_col3 {\n",
              "  background-color: #afd1e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row3_col30, #T_fc7b8_row4_col10, #T_fc7b8_row4_col30, #T_fc7b8_row6_col30, #T_fc7b8_row10_col13, #T_fc7b8_row11_col20, #T_fc7b8_row12_col14, #T_fc7b8_row14_col15, #T_fc7b8_row15_col10, #T_fc7b8_row15_col11, #T_fc7b8_row16_col9, #T_fc7b8_row18_col16, #T_fc7b8_row21_col10, #T_fc7b8_row22_col31, #T_fc7b8_row23_col10, #T_fc7b8_row24_col10, #T_fc7b8_row24_col29, #T_fc7b8_row26_col19, #T_fc7b8_row29_col10, #T_fc7b8_row31_col14 {\n",
              "  background-color: #e8f1fa;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row4_col27, #T_fc7b8_row24_col30, #T_fc7b8_row27_col4, #T_fc7b8_row28_col8, #T_fc7b8_row31_col17 {\n",
              "  background-color: #d8e7f5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row5_col27, #T_fc7b8_row27_col5 {\n",
              "  background-color: #b9d6ea;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row5_col30, #T_fc7b8_row18_col17, #T_fc7b8_row22_col23, #T_fc7b8_row24_col22, #T_fc7b8_row24_col28, #T_fc7b8_row31_col9, #T_fc7b8_row31_col19 {\n",
              "  background-color: #e2edf8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row6_col27, #T_fc7b8_row18_col7, #T_fc7b8_row19_col21, #T_fc7b8_row23_col30, #T_fc7b8_row27_col6 {\n",
              "  background-color: #d9e7f5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row6_col29, #T_fc7b8_row15_col29, #T_fc7b8_row17_col29, #T_fc7b8_row18_col8 {\n",
              "  background-color: #d9e8f5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row7_col8, #T_fc7b8_row7_col12, #T_fc7b8_row7_col23, #T_fc7b8_row8_col23, #T_fc7b8_row12_col23, #T_fc7b8_row15_col23, #T_fc7b8_row16_col28, #T_fc7b8_row18_col11 {\n",
              "  background-color: #d2e3f3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row7_col14, #T_fc7b8_row7_col26, #T_fc7b8_row8_col9, #T_fc7b8_row8_col11, #T_fc7b8_row8_col26, #T_fc7b8_row12_col31, #T_fc7b8_row15_col26, #T_fc7b8_row17_col28, #T_fc7b8_row18_col15, #T_fc7b8_row18_col26, #T_fc7b8_row30_col2, #T_fc7b8_row30_col5, #T_fc7b8_row30_col20 {\n",
              "  background-color: #f1f7fd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row7_col16, #T_fc7b8_row7_col21, #T_fc7b8_row8_col21, #T_fc7b8_row9_col17, #T_fc7b8_row9_col21, #T_fc7b8_row15_col21, #T_fc7b8_row20_col23, #T_fc7b8_row20_col26, #T_fc7b8_row21_col24, #T_fc7b8_row23_col7, #T_fc7b8_row28_col21, #T_fc7b8_row30_col8, #T_fc7b8_row30_col17, #T_fc7b8_row31_col11, #T_fc7b8_row31_col22 {\n",
              "  background-color: #dceaf6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row8_col7, #T_fc7b8_row11_col10 {\n",
              "  background-color: #cbdef1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row8_col12, #T_fc7b8_row8_col18, #T_fc7b8_row10_col18, #T_fc7b8_row13_col10, #T_fc7b8_row13_col11, #T_fc7b8_row13_col28, #T_fc7b8_row21_col19, #T_fc7b8_row23_col28, #T_fc7b8_row26_col30, #T_fc7b8_row27_col30 {\n",
              "  background-color: #e1edf8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row8_col15, #T_fc7b8_row12_col13, #T_fc7b8_row15_col16, #T_fc7b8_row16_col17, #T_fc7b8_row21_col31, #T_fc7b8_row23_col31, #T_fc7b8_row30_col0, #T_fc7b8_row30_col16, #T_fc7b8_row31_col20 {\n",
              "  background-color: #ecf4fb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row8_col24, #T_fc7b8_row10_col14, #T_fc7b8_row10_col19, #T_fc7b8_row11_col9, #T_fc7b8_row11_col19, #T_fc7b8_row12_col11, #T_fc7b8_row12_col17, #T_fc7b8_row13_col17, #T_fc7b8_row13_col19, #T_fc7b8_row17_col9, #T_fc7b8_row17_col31, #T_fc7b8_row18_col12, #T_fc7b8_row19_col28, #T_fc7b8_row21_col9, #T_fc7b8_row26_col24, #T_fc7b8_row26_col28, #T_fc7b8_row28_col24, #T_fc7b8_row29_col30, #T_fc7b8_row30_col12, #T_fc7b8_row31_col10 {\n",
              "  background-color: #e4eff9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row8_col28, #T_fc7b8_row14_col17, #T_fc7b8_row19_col25, #T_fc7b8_row30_col22, #T_fc7b8_row30_col24 {\n",
              "  background-color: #d6e5f4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row8_col31, #T_fc7b8_row11_col30, #T_fc7b8_row17_col16, #T_fc7b8_row22_col21, #T_fc7b8_row25_col20, #T_fc7b8_row29_col26, #T_fc7b8_row30_col13 {\n",
              "  background-color: #f5fafe;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row9_col7, #T_fc7b8_row9_col30, #T_fc7b8_row10_col17, #T_fc7b8_row11_col14, #T_fc7b8_row30_col7 {\n",
              "  background-color: #dce9f6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row9_col8, #T_fc7b8_row9_col26, #T_fc7b8_row12_col15, #T_fc7b8_row14_col13, #T_fc7b8_row16_col13, #T_fc7b8_row25_col26, #T_fc7b8_row26_col31, #T_fc7b8_row28_col26 {\n",
              "  background-color: #f2f7fd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row9_col23, #T_fc7b8_row28_col23 {\n",
              "  background-color: #d1e2f3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row9_col28, #T_fc7b8_row10_col8, #T_fc7b8_row11_col7, #T_fc7b8_row14_col18, #T_fc7b8_row16_col15, #T_fc7b8_row17_col15, #T_fc7b8_row22_col20, #T_fc7b8_row22_col26, #T_fc7b8_row23_col20, #T_fc7b8_row29_col24, #T_fc7b8_row30_col19 {\n",
              "  background-color: #f5f9fe;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row10_col7, #T_fc7b8_row14_col8, #T_fc7b8_row14_col31, #T_fc7b8_row18_col14, #T_fc7b8_row28_col13, #T_fc7b8_row30_col1, #T_fc7b8_row30_col11, #T_fc7b8_row31_col24 {\n",
              "  background-color: #edf4fc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row10_col11, #T_fc7b8_row30_col23 {\n",
              "  background-color: #c4daee;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row10_col28, #T_fc7b8_row12_col7 {\n",
              "  background-color: #cddff1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row11_col17, #T_fc7b8_row24_col23 {\n",
              "  background-color: #d0e2f2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row14_col11, #T_fc7b8_row26_col20, #T_fc7b8_row30_col29, #T_fc7b8_row30_col31 {\n",
              "  background-color: #d6e6f4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row15_col7, #T_fc7b8_row24_col21, #T_fc7b8_row28_col16 {\n",
              "  background-color: #d5e5f4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row17_col18, #T_fc7b8_row19_col26, #T_fc7b8_row20_col19 {\n",
              "  background-color: #f3f8fe;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row18_col10, #T_fc7b8_row29_col20 {\n",
              "  background-color: #dbe9f6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row18_col28, #T_fc7b8_row26_col21 {\n",
              "  background-color: #c7dcef;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row19_col29 {\n",
              "  background-color: #c9ddf0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row20_col24, #T_fc7b8_row21_col26, #T_fc7b8_row30_col25, #T_fc7b8_row31_col29 {\n",
              "  background-color: #d7e6f5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row21_col20 {\n",
              "  background-color: #ebf3fb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row21_col23 {\n",
              "  background-color: #bed8ec;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row21_col29, #T_fc7b8_row25_col24, #T_fc7b8_row31_col13, #T_fc7b8_row31_col18 {\n",
              "  background-color: #f4f9fe;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row22_col25 {\n",
              "  background-color: #b5d4e9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row22_col29 {\n",
              "  background-color: #8cc0dd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row23_col21, #T_fc7b8_row30_col21 {\n",
              "  background-color: #cadef0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row24_col31 {\n",
              "  background-color: #f2f8fd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row25_col22 {\n",
              "  background-color: #c7dbef;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row25_col29 {\n",
              "  background-color: #b7d4ea;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row28_col10, #T_fc7b8_row28_col11, #T_fc7b8_row28_col18 {\n",
              "  background-color: #d0e1f2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row29_col22 {\n",
              "  background-color: #94c4df;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row29_col25 {\n",
              "  background-color: #add0e6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_fc7b8_row31_col25 {\n",
              "  background-color: #cfe1f2;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_fc7b8_\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th class=\"col_heading level0 col0\" >f_00</th>\n",
              "      <th class=\"col_heading level0 col1\" >f_01</th>\n",
              "      <th class=\"col_heading level0 col2\" >f_02</th>\n",
              "      <th class=\"col_heading level0 col3\" >f_03</th>\n",
              "      <th class=\"col_heading level0 col4\" >f_04</th>\n",
              "      <th class=\"col_heading level0 col5\" >f_05</th>\n",
              "      <th class=\"col_heading level0 col6\" >f_06</th>\n",
              "      <th class=\"col_heading level0 col7\" >f_07</th>\n",
              "      <th class=\"col_heading level0 col8\" >f_08</th>\n",
              "      <th class=\"col_heading level0 col9\" >f_09</th>\n",
              "      <th class=\"col_heading level0 col10\" >f_10</th>\n",
              "      <th class=\"col_heading level0 col11\" >f_11</th>\n",
              "      <th class=\"col_heading level0 col12\" >f_12</th>\n",
              "      <th class=\"col_heading level0 col13\" >f_13</th>\n",
              "      <th class=\"col_heading level0 col14\" >f_14</th>\n",
              "      <th class=\"col_heading level0 col15\" >f_15</th>\n",
              "      <th class=\"col_heading level0 col16\" >f_16</th>\n",
              "      <th class=\"col_heading level0 col17\" >f_17</th>\n",
              "      <th class=\"col_heading level0 col18\" >f_18</th>\n",
              "      <th class=\"col_heading level0 col19\" >f_19</th>\n",
              "      <th class=\"col_heading level0 col20\" >f_20</th>\n",
              "      <th class=\"col_heading level0 col21\" >f_21</th>\n",
              "      <th class=\"col_heading level0 col22\" >f_22</th>\n",
              "      <th class=\"col_heading level0 col23\" >f_23</th>\n",
              "      <th class=\"col_heading level0 col24\" >f_24</th>\n",
              "      <th class=\"col_heading level0 col25\" >f_25</th>\n",
              "      <th class=\"col_heading level0 col26\" >f_26</th>\n",
              "      <th class=\"col_heading level0 col27\" >f_28</th>\n",
              "      <th class=\"col_heading level0 col28\" >f_29</th>\n",
              "      <th class=\"col_heading level0 col29\" >f_30</th>\n",
              "      <th class=\"col_heading level0 col30\" >target</th>\n",
              "      <th class=\"col_heading level0 col31\" >unique_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row0\" class=\"row_heading level0 row0\" >f_00</th>\n",
              "      <td id=\"T_fc7b8_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row0_col1\" class=\"data row0 col1\" >-0.000335</td>\n",
              "      <td id=\"T_fc7b8_row0_col2\" class=\"data row0 col2\" >0.002016</td>\n",
              "      <td id=\"T_fc7b8_row0_col3\" class=\"data row0 col3\" >-0.000427</td>\n",
              "      <td id=\"T_fc7b8_row0_col4\" class=\"data row0 col4\" >-0.000208</td>\n",
              "      <td id=\"T_fc7b8_row0_col5\" class=\"data row0 col5\" >-0.000401</td>\n",
              "      <td id=\"T_fc7b8_row0_col6\" class=\"data row0 col6\" >-0.001356</td>\n",
              "      <td id=\"T_fc7b8_row0_col7\" class=\"data row0 col7\" >0.000548</td>\n",
              "      <td id=\"T_fc7b8_row0_col8\" class=\"data row0 col8\" >-0.002819</td>\n",
              "      <td id=\"T_fc7b8_row0_col9\" class=\"data row0 col9\" >0.000216</td>\n",
              "      <td id=\"T_fc7b8_row0_col10\" class=\"data row0 col10\" >0.000767</td>\n",
              "      <td id=\"T_fc7b8_row0_col11\" class=\"data row0 col11\" >0.001137</td>\n",
              "      <td id=\"T_fc7b8_row0_col12\" class=\"data row0 col12\" >-0.001343</td>\n",
              "      <td id=\"T_fc7b8_row0_col13\" class=\"data row0 col13\" >0.000838</td>\n",
              "      <td id=\"T_fc7b8_row0_col14\" class=\"data row0 col14\" >0.001039</td>\n",
              "      <td id=\"T_fc7b8_row0_col15\" class=\"data row0 col15\" >-0.002240</td>\n",
              "      <td id=\"T_fc7b8_row0_col16\" class=\"data row0 col16\" >-0.000768</td>\n",
              "      <td id=\"T_fc7b8_row0_col17\" class=\"data row0 col17\" >0.000796</td>\n",
              "      <td id=\"T_fc7b8_row0_col18\" class=\"data row0 col18\" >-0.000933</td>\n",
              "      <td id=\"T_fc7b8_row0_col19\" class=\"data row0 col19\" >0.000582</td>\n",
              "      <td id=\"T_fc7b8_row0_col20\" class=\"data row0 col20\" >-0.000479</td>\n",
              "      <td id=\"T_fc7b8_row0_col21\" class=\"data row0 col21\" >0.000649</td>\n",
              "      <td id=\"T_fc7b8_row0_col22\" class=\"data row0 col22\" >-0.001566</td>\n",
              "      <td id=\"T_fc7b8_row0_col23\" class=\"data row0 col23\" >0.000958</td>\n",
              "      <td id=\"T_fc7b8_row0_col24\" class=\"data row0 col24\" >0.001685</td>\n",
              "      <td id=\"T_fc7b8_row0_col25\" class=\"data row0 col25\" >0.000438</td>\n",
              "      <td id=\"T_fc7b8_row0_col26\" class=\"data row0 col26\" >0.000140</td>\n",
              "      <td id=\"T_fc7b8_row0_col27\" class=\"data row0 col27\" >0.173930</td>\n",
              "      <td id=\"T_fc7b8_row0_col28\" class=\"data row0 col28\" >0.000378</td>\n",
              "      <td id=\"T_fc7b8_row0_col29\" class=\"data row0 col29\" >-0.001207</td>\n",
              "      <td id=\"T_fc7b8_row0_col30\" class=\"data row0 col30\" >0.053039</td>\n",
              "      <td id=\"T_fc7b8_row0_col31\" class=\"data row0 col31\" >-0.000236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row1\" class=\"row_heading level0 row1\" >f_01</th>\n",
              "      <td id=\"T_fc7b8_row1_col0\" class=\"data row1 col0\" >-0.000335</td>\n",
              "      <td id=\"T_fc7b8_row1_col1\" class=\"data row1 col1\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row1_col2\" class=\"data row1 col2\" >0.000857</td>\n",
              "      <td id=\"T_fc7b8_row1_col3\" class=\"data row1 col3\" >-0.000356</td>\n",
              "      <td id=\"T_fc7b8_row1_col4\" class=\"data row1 col4\" >-0.001100</td>\n",
              "      <td id=\"T_fc7b8_row1_col5\" class=\"data row1 col5\" >0.000184</td>\n",
              "      <td id=\"T_fc7b8_row1_col6\" class=\"data row1 col6\" >-0.000240</td>\n",
              "      <td id=\"T_fc7b8_row1_col7\" class=\"data row1 col7\" >-0.000925</td>\n",
              "      <td id=\"T_fc7b8_row1_col8\" class=\"data row1 col8\" >-0.000746</td>\n",
              "      <td id=\"T_fc7b8_row1_col9\" class=\"data row1 col9\" >-0.000703</td>\n",
              "      <td id=\"T_fc7b8_row1_col10\" class=\"data row1 col10\" >0.001659</td>\n",
              "      <td id=\"T_fc7b8_row1_col11\" class=\"data row1 col11\" >0.001605</td>\n",
              "      <td id=\"T_fc7b8_row1_col12\" class=\"data row1 col12\" >-0.001396</td>\n",
              "      <td id=\"T_fc7b8_row1_col13\" class=\"data row1 col13\" >-0.000931</td>\n",
              "      <td id=\"T_fc7b8_row1_col14\" class=\"data row1 col14\" >0.000311</td>\n",
              "      <td id=\"T_fc7b8_row1_col15\" class=\"data row1 col15\" >-0.000784</td>\n",
              "      <td id=\"T_fc7b8_row1_col16\" class=\"data row1 col16\" >-0.001258</td>\n",
              "      <td id=\"T_fc7b8_row1_col17\" class=\"data row1 col17\" >-0.000094</td>\n",
              "      <td id=\"T_fc7b8_row1_col18\" class=\"data row1 col18\" >0.001100</td>\n",
              "      <td id=\"T_fc7b8_row1_col19\" class=\"data row1 col19\" >-0.000696</td>\n",
              "      <td id=\"T_fc7b8_row1_col20\" class=\"data row1 col20\" >0.001118</td>\n",
              "      <td id=\"T_fc7b8_row1_col21\" class=\"data row1 col21\" >-0.001339</td>\n",
              "      <td id=\"T_fc7b8_row1_col22\" class=\"data row1 col22\" >-0.000481</td>\n",
              "      <td id=\"T_fc7b8_row1_col23\" class=\"data row1 col23\" >-0.000462</td>\n",
              "      <td id=\"T_fc7b8_row1_col24\" class=\"data row1 col24\" >-0.000800</td>\n",
              "      <td id=\"T_fc7b8_row1_col25\" class=\"data row1 col25\" >-0.000408</td>\n",
              "      <td id=\"T_fc7b8_row1_col26\" class=\"data row1 col26\" >0.001537</td>\n",
              "      <td id=\"T_fc7b8_row1_col27\" class=\"data row1 col27\" >0.179157</td>\n",
              "      <td id=\"T_fc7b8_row1_col28\" class=\"data row1 col28\" >-0.000144</td>\n",
              "      <td id=\"T_fc7b8_row1_col29\" class=\"data row1 col29\" >0.000052</td>\n",
              "      <td id=\"T_fc7b8_row1_col30\" class=\"data row1 col30\" >0.052751</td>\n",
              "      <td id=\"T_fc7b8_row1_col31\" class=\"data row1 col31\" >-0.001385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row2\" class=\"row_heading level0 row2\" >f_02</th>\n",
              "      <td id=\"T_fc7b8_row2_col0\" class=\"data row2 col0\" >0.002016</td>\n",
              "      <td id=\"T_fc7b8_row2_col1\" class=\"data row2 col1\" >0.000857</td>\n",
              "      <td id=\"T_fc7b8_row2_col2\" class=\"data row2 col2\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row2_col3\" class=\"data row2 col3\" >0.000394</td>\n",
              "      <td id=\"T_fc7b8_row2_col4\" class=\"data row2 col4\" >0.000205</td>\n",
              "      <td id=\"T_fc7b8_row2_col5\" class=\"data row2 col5\" >-0.001556</td>\n",
              "      <td id=\"T_fc7b8_row2_col6\" class=\"data row2 col6\" >0.000877</td>\n",
              "      <td id=\"T_fc7b8_row2_col7\" class=\"data row2 col7\" >0.001348</td>\n",
              "      <td id=\"T_fc7b8_row2_col8\" class=\"data row2 col8\" >0.000759</td>\n",
              "      <td id=\"T_fc7b8_row2_col9\" class=\"data row2 col9\" >0.001214</td>\n",
              "      <td id=\"T_fc7b8_row2_col10\" class=\"data row2 col10\" >0.001376</td>\n",
              "      <td id=\"T_fc7b8_row2_col11\" class=\"data row2 col11\" >-0.002289</td>\n",
              "      <td id=\"T_fc7b8_row2_col12\" class=\"data row2 col12\" >0.001972</td>\n",
              "      <td id=\"T_fc7b8_row2_col13\" class=\"data row2 col13\" >-0.002787</td>\n",
              "      <td id=\"T_fc7b8_row2_col14\" class=\"data row2 col14\" >0.000814</td>\n",
              "      <td id=\"T_fc7b8_row2_col15\" class=\"data row2 col15\" >0.000415</td>\n",
              "      <td id=\"T_fc7b8_row2_col16\" class=\"data row2 col16\" >-0.002015</td>\n",
              "      <td id=\"T_fc7b8_row2_col17\" class=\"data row2 col17\" >0.000429</td>\n",
              "      <td id=\"T_fc7b8_row2_col18\" class=\"data row2 col18\" >-0.000059</td>\n",
              "      <td id=\"T_fc7b8_row2_col19\" class=\"data row2 col19\" >-0.000927</td>\n",
              "      <td id=\"T_fc7b8_row2_col20\" class=\"data row2 col20\" >0.000832</td>\n",
              "      <td id=\"T_fc7b8_row2_col21\" class=\"data row2 col21\" >-0.000230</td>\n",
              "      <td id=\"T_fc7b8_row2_col22\" class=\"data row2 col22\" >-0.000603</td>\n",
              "      <td id=\"T_fc7b8_row2_col23\" class=\"data row2 col23\" >0.000476</td>\n",
              "      <td id=\"T_fc7b8_row2_col24\" class=\"data row2 col24\" >0.000065</td>\n",
              "      <td id=\"T_fc7b8_row2_col25\" class=\"data row2 col25\" >-0.000800</td>\n",
              "      <td id=\"T_fc7b8_row2_col26\" class=\"data row2 col26\" >0.000832</td>\n",
              "      <td id=\"T_fc7b8_row2_col27\" class=\"data row2 col27\" >0.211694</td>\n",
              "      <td id=\"T_fc7b8_row2_col28\" class=\"data row2 col28\" >-0.001319</td>\n",
              "      <td id=\"T_fc7b8_row2_col29\" class=\"data row2 col29\" >-0.000089</td>\n",
              "      <td id=\"T_fc7b8_row2_col30\" class=\"data row2 col30\" >0.029921</td>\n",
              "      <td id=\"T_fc7b8_row2_col31\" class=\"data row2 col31\" >-0.000078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row3\" class=\"row_heading level0 row3\" >f_03</th>\n",
              "      <td id=\"T_fc7b8_row3_col0\" class=\"data row3 col0\" >-0.000427</td>\n",
              "      <td id=\"T_fc7b8_row3_col1\" class=\"data row3 col1\" >-0.000356</td>\n",
              "      <td id=\"T_fc7b8_row3_col2\" class=\"data row3 col2\" >0.000394</td>\n",
              "      <td id=\"T_fc7b8_row3_col3\" class=\"data row3 col3\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row3_col4\" class=\"data row3 col4\" >0.001123</td>\n",
              "      <td id=\"T_fc7b8_row3_col5\" class=\"data row3 col5\" >0.002678</td>\n",
              "      <td id=\"T_fc7b8_row3_col6\" class=\"data row3 col6\" >-0.001097</td>\n",
              "      <td id=\"T_fc7b8_row3_col7\" class=\"data row3 col7\" >0.000827</td>\n",
              "      <td id=\"T_fc7b8_row3_col8\" class=\"data row3 col8\" >-0.000061</td>\n",
              "      <td id=\"T_fc7b8_row3_col9\" class=\"data row3 col9\" >0.001070</td>\n",
              "      <td id=\"T_fc7b8_row3_col10\" class=\"data row3 col10\" >-0.000506</td>\n",
              "      <td id=\"T_fc7b8_row3_col11\" class=\"data row3 col11\" >0.001975</td>\n",
              "      <td id=\"T_fc7b8_row3_col12\" class=\"data row3 col12\" >0.000630</td>\n",
              "      <td id=\"T_fc7b8_row3_col13\" class=\"data row3 col13\" >-0.001187</td>\n",
              "      <td id=\"T_fc7b8_row3_col14\" class=\"data row3 col14\" >0.000104</td>\n",
              "      <td id=\"T_fc7b8_row3_col15\" class=\"data row3 col15\" >-0.001986</td>\n",
              "      <td id=\"T_fc7b8_row3_col16\" class=\"data row3 col16\" >0.000627</td>\n",
              "      <td id=\"T_fc7b8_row3_col17\" class=\"data row3 col17\" >0.000523</td>\n",
              "      <td id=\"T_fc7b8_row3_col18\" class=\"data row3 col18\" >0.001095</td>\n",
              "      <td id=\"T_fc7b8_row3_col19\" class=\"data row3 col19\" >0.001529</td>\n",
              "      <td id=\"T_fc7b8_row3_col20\" class=\"data row3 col20\" >0.000655</td>\n",
              "      <td id=\"T_fc7b8_row3_col21\" class=\"data row3 col21\" >-0.000119</td>\n",
              "      <td id=\"T_fc7b8_row3_col22\" class=\"data row3 col22\" >0.000543</td>\n",
              "      <td id=\"T_fc7b8_row3_col23\" class=\"data row3 col23\" >0.001532</td>\n",
              "      <td id=\"T_fc7b8_row3_col24\" class=\"data row3 col24\" >0.002580</td>\n",
              "      <td id=\"T_fc7b8_row3_col25\" class=\"data row3 col25\" >0.002306</td>\n",
              "      <td id=\"T_fc7b8_row3_col26\" class=\"data row3 col26\" >0.000586</td>\n",
              "      <td id=\"T_fc7b8_row3_col27\" class=\"data row3 col27\" >0.322121</td>\n",
              "      <td id=\"T_fc7b8_row3_col28\" class=\"data row3 col28\" >-0.000587</td>\n",
              "      <td id=\"T_fc7b8_row3_col29\" class=\"data row3 col29\" >-0.000019</td>\n",
              "      <td id=\"T_fc7b8_row3_col30\" class=\"data row3 col30\" >-0.000038</td>\n",
              "      <td id=\"T_fc7b8_row3_col31\" class=\"data row3 col31\" >-0.001494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row4\" class=\"row_heading level0 row4\" >f_04</th>\n",
              "      <td id=\"T_fc7b8_row4_col0\" class=\"data row4 col0\" >-0.000208</td>\n",
              "      <td id=\"T_fc7b8_row4_col1\" class=\"data row4 col1\" >-0.001100</td>\n",
              "      <td id=\"T_fc7b8_row4_col2\" class=\"data row4 col2\" >0.000205</td>\n",
              "      <td id=\"T_fc7b8_row4_col3\" class=\"data row4 col3\" >0.001123</td>\n",
              "      <td id=\"T_fc7b8_row4_col4\" class=\"data row4 col4\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row4_col5\" class=\"data row4 col5\" >0.000091</td>\n",
              "      <td id=\"T_fc7b8_row4_col6\" class=\"data row4 col6\" >-0.000447</td>\n",
              "      <td id=\"T_fc7b8_row4_col7\" class=\"data row4 col7\" >0.001211</td>\n",
              "      <td id=\"T_fc7b8_row4_col8\" class=\"data row4 col8\" >0.000121</td>\n",
              "      <td id=\"T_fc7b8_row4_col9\" class=\"data row4 col9\" >0.001633</td>\n",
              "      <td id=\"T_fc7b8_row4_col10\" class=\"data row4 col10\" >-0.003212</td>\n",
              "      <td id=\"T_fc7b8_row4_col11\" class=\"data row4 col11\" >-0.000035</td>\n",
              "      <td id=\"T_fc7b8_row4_col12\" class=\"data row4 col12\" >0.001381</td>\n",
              "      <td id=\"T_fc7b8_row4_col13\" class=\"data row4 col13\" >-0.000509</td>\n",
              "      <td id=\"T_fc7b8_row4_col14\" class=\"data row4 col14\" >0.001740</td>\n",
              "      <td id=\"T_fc7b8_row4_col15\" class=\"data row4 col15\" >0.000669</td>\n",
              "      <td id=\"T_fc7b8_row4_col16\" class=\"data row4 col16\" >-0.000974</td>\n",
              "      <td id=\"T_fc7b8_row4_col17\" class=\"data row4 col17\" >0.000129</td>\n",
              "      <td id=\"T_fc7b8_row4_col18\" class=\"data row4 col18\" >0.000491</td>\n",
              "      <td id=\"T_fc7b8_row4_col19\" class=\"data row4 col19\" >0.000905</td>\n",
              "      <td id=\"T_fc7b8_row4_col20\" class=\"data row4 col20\" >0.000312</td>\n",
              "      <td id=\"T_fc7b8_row4_col21\" class=\"data row4 col21\" >0.000204</td>\n",
              "      <td id=\"T_fc7b8_row4_col22\" class=\"data row4 col22\" >0.001342</td>\n",
              "      <td id=\"T_fc7b8_row4_col23\" class=\"data row4 col23\" >-0.002190</td>\n",
              "      <td id=\"T_fc7b8_row4_col24\" class=\"data row4 col24\" >0.000075</td>\n",
              "      <td id=\"T_fc7b8_row4_col25\" class=\"data row4 col25\" >-0.002170</td>\n",
              "      <td id=\"T_fc7b8_row4_col26\" class=\"data row4 col26\" >-0.000267</td>\n",
              "      <td id=\"T_fc7b8_row4_col27\" class=\"data row4 col27\" >0.156413</td>\n",
              "      <td id=\"T_fc7b8_row4_col28\" class=\"data row4 col28\" >-0.001027</td>\n",
              "      <td id=\"T_fc7b8_row4_col29\" class=\"data row4 col29\" >0.000327</td>\n",
              "      <td id=\"T_fc7b8_row4_col30\" class=\"data row4 col30\" >0.000058</td>\n",
              "      <td id=\"T_fc7b8_row4_col31\" class=\"data row4 col31\" >-0.000354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row5\" class=\"row_heading level0 row5\" >f_05</th>\n",
              "      <td id=\"T_fc7b8_row5_col0\" class=\"data row5 col0\" >-0.000401</td>\n",
              "      <td id=\"T_fc7b8_row5_col1\" class=\"data row5 col1\" >0.000184</td>\n",
              "      <td id=\"T_fc7b8_row5_col2\" class=\"data row5 col2\" >-0.001556</td>\n",
              "      <td id=\"T_fc7b8_row5_col3\" class=\"data row5 col3\" >0.002678</td>\n",
              "      <td id=\"T_fc7b8_row5_col4\" class=\"data row5 col4\" >0.000091</td>\n",
              "      <td id=\"T_fc7b8_row5_col5\" class=\"data row5 col5\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row5_col6\" class=\"data row5 col6\" >0.000241</td>\n",
              "      <td id=\"T_fc7b8_row5_col7\" class=\"data row5 col7\" >0.000062</td>\n",
              "      <td id=\"T_fc7b8_row5_col8\" class=\"data row5 col8\" >-0.000824</td>\n",
              "      <td id=\"T_fc7b8_row5_col9\" class=\"data row5 col9\" >-0.000459</td>\n",
              "      <td id=\"T_fc7b8_row5_col10\" class=\"data row5 col10\" >-0.000877</td>\n",
              "      <td id=\"T_fc7b8_row5_col11\" class=\"data row5 col11\" >-0.000970</td>\n",
              "      <td id=\"T_fc7b8_row5_col12\" class=\"data row5 col12\" >0.003442</td>\n",
              "      <td id=\"T_fc7b8_row5_col13\" class=\"data row5 col13\" >0.001086</td>\n",
              "      <td id=\"T_fc7b8_row5_col14\" class=\"data row5 col14\" >0.000625</td>\n",
              "      <td id=\"T_fc7b8_row5_col15\" class=\"data row5 col15\" >-0.000867</td>\n",
              "      <td id=\"T_fc7b8_row5_col16\" class=\"data row5 col16\" >-0.000053</td>\n",
              "      <td id=\"T_fc7b8_row5_col17\" class=\"data row5 col17\" >0.000406</td>\n",
              "      <td id=\"T_fc7b8_row5_col18\" class=\"data row5 col18\" >0.000159</td>\n",
              "      <td id=\"T_fc7b8_row5_col19\" class=\"data row5 col19\" >-0.001009</td>\n",
              "      <td id=\"T_fc7b8_row5_col20\" class=\"data row5 col20\" >-0.000119</td>\n",
              "      <td id=\"T_fc7b8_row5_col21\" class=\"data row5 col21\" >0.000699</td>\n",
              "      <td id=\"T_fc7b8_row5_col22\" class=\"data row5 col22\" >0.001394</td>\n",
              "      <td id=\"T_fc7b8_row5_col23\" class=\"data row5 col23\" >-0.000894</td>\n",
              "      <td id=\"T_fc7b8_row5_col24\" class=\"data row5 col24\" >-0.002708</td>\n",
              "      <td id=\"T_fc7b8_row5_col25\" class=\"data row5 col25\" >0.001292</td>\n",
              "      <td id=\"T_fc7b8_row5_col26\" class=\"data row5 col26\" >0.001368</td>\n",
              "      <td id=\"T_fc7b8_row5_col27\" class=\"data row5 col27\" >0.289427</td>\n",
              "      <td id=\"T_fc7b8_row5_col28\" class=\"data row5 col28\" >-0.001408</td>\n",
              "      <td id=\"T_fc7b8_row5_col29\" class=\"data row5 col29\" >-0.000722</td>\n",
              "      <td id=\"T_fc7b8_row5_col30\" class=\"data row5 col30\" >0.031144</td>\n",
              "      <td id=\"T_fc7b8_row5_col31\" class=\"data row5 col31\" >0.000392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row6\" class=\"row_heading level0 row6\" >f_06</th>\n",
              "      <td id=\"T_fc7b8_row6_col0\" class=\"data row6 col0\" >-0.001356</td>\n",
              "      <td id=\"T_fc7b8_row6_col1\" class=\"data row6 col1\" >-0.000240</td>\n",
              "      <td id=\"T_fc7b8_row6_col2\" class=\"data row6 col2\" >0.000877</td>\n",
              "      <td id=\"T_fc7b8_row6_col3\" class=\"data row6 col3\" >-0.001097</td>\n",
              "      <td id=\"T_fc7b8_row6_col4\" class=\"data row6 col4\" >-0.000447</td>\n",
              "      <td id=\"T_fc7b8_row6_col5\" class=\"data row6 col5\" >0.000241</td>\n",
              "      <td id=\"T_fc7b8_row6_col6\" class=\"data row6 col6\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row6_col7\" class=\"data row6 col7\" >-0.000262</td>\n",
              "      <td id=\"T_fc7b8_row6_col8\" class=\"data row6 col8\" >-0.000432</td>\n",
              "      <td id=\"T_fc7b8_row6_col9\" class=\"data row6 col9\" >-0.001104</td>\n",
              "      <td id=\"T_fc7b8_row6_col10\" class=\"data row6 col10\" >-0.000198</td>\n",
              "      <td id=\"T_fc7b8_row6_col11\" class=\"data row6 col11\" >0.001057</td>\n",
              "      <td id=\"T_fc7b8_row6_col12\" class=\"data row6 col12\" >0.001361</td>\n",
              "      <td id=\"T_fc7b8_row6_col13\" class=\"data row6 col13\" >-0.000598</td>\n",
              "      <td id=\"T_fc7b8_row6_col14\" class=\"data row6 col14\" >-0.000569</td>\n",
              "      <td id=\"T_fc7b8_row6_col15\" class=\"data row6 col15\" >-0.000673</td>\n",
              "      <td id=\"T_fc7b8_row6_col16\" class=\"data row6 col16\" >0.000766</td>\n",
              "      <td id=\"T_fc7b8_row6_col17\" class=\"data row6 col17\" >0.000842</td>\n",
              "      <td id=\"T_fc7b8_row6_col18\" class=\"data row6 col18\" >-0.000598</td>\n",
              "      <td id=\"T_fc7b8_row6_col19\" class=\"data row6 col19\" >0.001308</td>\n",
              "      <td id=\"T_fc7b8_row6_col20\" class=\"data row6 col20\" >-0.001024</td>\n",
              "      <td id=\"T_fc7b8_row6_col21\" class=\"data row6 col21\" >-0.000362</td>\n",
              "      <td id=\"T_fc7b8_row6_col22\" class=\"data row6 col22\" >-0.000284</td>\n",
              "      <td id=\"T_fc7b8_row6_col23\" class=\"data row6 col23\" >0.000335</td>\n",
              "      <td id=\"T_fc7b8_row6_col24\" class=\"data row6 col24\" >-0.002706</td>\n",
              "      <td id=\"T_fc7b8_row6_col25\" class=\"data row6 col25\" >0.000626</td>\n",
              "      <td id=\"T_fc7b8_row6_col26\" class=\"data row6 col26\" >-0.000080</td>\n",
              "      <td id=\"T_fc7b8_row6_col27\" class=\"data row6 col27\" >0.151197</td>\n",
              "      <td id=\"T_fc7b8_row6_col28\" class=\"data row6 col28\" >0.000902</td>\n",
              "      <td id=\"T_fc7b8_row6_col29\" class=\"data row6 col29\" >0.000935</td>\n",
              "      <td id=\"T_fc7b8_row6_col30\" class=\"data row6 col30\" >-0.000773</td>\n",
              "      <td id=\"T_fc7b8_row6_col31\" class=\"data row6 col31\" >0.000038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row7\" class=\"row_heading level0 row7\" >f_07</th>\n",
              "      <td id=\"T_fc7b8_row7_col0\" class=\"data row7 col0\" >0.000548</td>\n",
              "      <td id=\"T_fc7b8_row7_col1\" class=\"data row7 col1\" >-0.000925</td>\n",
              "      <td id=\"T_fc7b8_row7_col2\" class=\"data row7 col2\" >0.001348</td>\n",
              "      <td id=\"T_fc7b8_row7_col3\" class=\"data row7 col3\" >0.000827</td>\n",
              "      <td id=\"T_fc7b8_row7_col4\" class=\"data row7 col4\" >0.001211</td>\n",
              "      <td id=\"T_fc7b8_row7_col5\" class=\"data row7 col5\" >0.000062</td>\n",
              "      <td id=\"T_fc7b8_row7_col6\" class=\"data row7 col6\" >-0.000262</td>\n",
              "      <td id=\"T_fc7b8_row7_col7\" class=\"data row7 col7\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row7_col8\" class=\"data row7 col8\" >0.109142</td>\n",
              "      <td id=\"T_fc7b8_row7_col9\" class=\"data row7 col9\" >0.010485</td>\n",
              "      <td id=\"T_fc7b8_row7_col10\" class=\"data row7 col10\" >-0.087163</td>\n",
              "      <td id=\"T_fc7b8_row7_col11\" class=\"data row7 col11\" >-0.133388</td>\n",
              "      <td id=\"T_fc7b8_row7_col12\" class=\"data row7 col12\" >0.098123</td>\n",
              "      <td id=\"T_fc7b8_row7_col13\" class=\"data row7 col13\" >0.059017</td>\n",
              "      <td id=\"T_fc7b8_row7_col14\" class=\"data row7 col14\" >-0.058409</td>\n",
              "      <td id=\"T_fc7b8_row7_col15\" class=\"data row7 col15\" >0.049611</td>\n",
              "      <td id=\"T_fc7b8_row7_col16\" class=\"data row7 col16\" >0.054666</td>\n",
              "      <td id=\"T_fc7b8_row7_col17\" class=\"data row7 col17\" >-0.148858</td>\n",
              "      <td id=\"T_fc7b8_row7_col18\" class=\"data row7 col18\" >0.026498</td>\n",
              "      <td id=\"T_fc7b8_row7_col19\" class=\"data row7 col19\" >-0.003231</td>\n",
              "      <td id=\"T_fc7b8_row7_col20\" class=\"data row7 col20\" >-0.003155</td>\n",
              "      <td id=\"T_fc7b8_row7_col21\" class=\"data row7 col21\" >0.001340</td>\n",
              "      <td id=\"T_fc7b8_row7_col22\" class=\"data row7 col22\" >0.000006</td>\n",
              "      <td id=\"T_fc7b8_row7_col23\" class=\"data row7 col23\" >0.005449</td>\n",
              "      <td id=\"T_fc7b8_row7_col24\" class=\"data row7 col24\" >0.003383</td>\n",
              "      <td id=\"T_fc7b8_row7_col25\" class=\"data row7 col25\" >-0.001242</td>\n",
              "      <td id=\"T_fc7b8_row7_col26\" class=\"data row7 col26\" >-0.003957</td>\n",
              "      <td id=\"T_fc7b8_row7_col27\" class=\"data row7 col27\" >0.000336</td>\n",
              "      <td id=\"T_fc7b8_row7_col28\" class=\"data row7 col28\" >-0.034330</td>\n",
              "      <td id=\"T_fc7b8_row7_col29\" class=\"data row7 col29\" >-0.000538</td>\n",
              "      <td id=\"T_fc7b8_row7_col30\" class=\"data row7 col30\" >0.009698</td>\n",
              "      <td id=\"T_fc7b8_row7_col31\" class=\"data row7 col31\" >-0.068810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row8\" class=\"row_heading level0 row8\" >f_08</th>\n",
              "      <td id=\"T_fc7b8_row8_col0\" class=\"data row8 col0\" >-0.002819</td>\n",
              "      <td id=\"T_fc7b8_row8_col1\" class=\"data row8 col1\" >-0.000746</td>\n",
              "      <td id=\"T_fc7b8_row8_col2\" class=\"data row8 col2\" >0.000759</td>\n",
              "      <td id=\"T_fc7b8_row8_col3\" class=\"data row8 col3\" >-0.000061</td>\n",
              "      <td id=\"T_fc7b8_row8_col4\" class=\"data row8 col4\" >0.000121</td>\n",
              "      <td id=\"T_fc7b8_row8_col5\" class=\"data row8 col5\" >-0.000824</td>\n",
              "      <td id=\"T_fc7b8_row8_col6\" class=\"data row8 col6\" >-0.000432</td>\n",
              "      <td id=\"T_fc7b8_row8_col7\" class=\"data row8 col7\" >0.109142</td>\n",
              "      <td id=\"T_fc7b8_row8_col8\" class=\"data row8 col8\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row8_col9\" class=\"data row8 col9\" >-0.063953</td>\n",
              "      <td id=\"T_fc7b8_row8_col10\" class=\"data row8 col10\" >-0.082123</td>\n",
              "      <td id=\"T_fc7b8_row8_col11\" class=\"data row8 col11\" >-0.097914</td>\n",
              "      <td id=\"T_fc7b8_row8_col12\" class=\"data row8 col12\" >0.013528</td>\n",
              "      <td id=\"T_fc7b8_row8_col13\" class=\"data row8 col13\" >0.036971</td>\n",
              "      <td id=\"T_fc7b8_row8_col14\" class=\"data row8 col14\" >-0.041036</td>\n",
              "      <td id=\"T_fc7b8_row8_col15\" class=\"data row8 col15\" >0.010796</td>\n",
              "      <td id=\"T_fc7b8_row8_col16\" class=\"data row8 col16\" >0.040876</td>\n",
              "      <td id=\"T_fc7b8_row8_col17\" class=\"data row8 col17\" >-0.049987</td>\n",
              "      <td id=\"T_fc7b8_row8_col18\" class=\"data row8 col18\" >0.066853</td>\n",
              "      <td id=\"T_fc7b8_row8_col19\" class=\"data row8 col19\" >-0.005749</td>\n",
              "      <td id=\"T_fc7b8_row8_col20\" class=\"data row8 col20\" >-0.005247</td>\n",
              "      <td id=\"T_fc7b8_row8_col21\" class=\"data row8 col21\" >0.003877</td>\n",
              "      <td id=\"T_fc7b8_row8_col22\" class=\"data row8 col22\" >-0.000813</td>\n",
              "      <td id=\"T_fc7b8_row8_col23\" class=\"data row8 col23\" >0.003617</td>\n",
              "      <td id=\"T_fc7b8_row8_col24\" class=\"data row8 col24\" >0.004526</td>\n",
              "      <td id=\"T_fc7b8_row8_col25\" class=\"data row8 col25\" >-0.001997</td>\n",
              "      <td id=\"T_fc7b8_row8_col26\" class=\"data row8 col26\" >-0.004281</td>\n",
              "      <td id=\"T_fc7b8_row8_col27\" class=\"data row8 col27\" >-0.000160</td>\n",
              "      <td id=\"T_fc7b8_row8_col28\" class=\"data row8 col28\" >0.077027</td>\n",
              "      <td id=\"T_fc7b8_row8_col29\" class=\"data row8 col29\" >0.000741</td>\n",
              "      <td id=\"T_fc7b8_row8_col30\" class=\"data row8 col30\" >0.050638</td>\n",
              "      <td id=\"T_fc7b8_row8_col31\" class=\"data row8 col31\" >-0.058358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row9\" class=\"row_heading level0 row9\" >f_09</th>\n",
              "      <td id=\"T_fc7b8_row9_col0\" class=\"data row9 col0\" >0.000216</td>\n",
              "      <td id=\"T_fc7b8_row9_col1\" class=\"data row9 col1\" >-0.000703</td>\n",
              "      <td id=\"T_fc7b8_row9_col2\" class=\"data row9 col2\" >0.001214</td>\n",
              "      <td id=\"T_fc7b8_row9_col3\" class=\"data row9 col3\" >0.001070</td>\n",
              "      <td id=\"T_fc7b8_row9_col4\" class=\"data row9 col4\" >0.001633</td>\n",
              "      <td id=\"T_fc7b8_row9_col5\" class=\"data row9 col5\" >-0.000459</td>\n",
              "      <td id=\"T_fc7b8_row9_col6\" class=\"data row9 col6\" >-0.001104</td>\n",
              "      <td id=\"T_fc7b8_row9_col7\" class=\"data row9 col7\" >0.010485</td>\n",
              "      <td id=\"T_fc7b8_row9_col8\" class=\"data row9 col8\" >-0.063953</td>\n",
              "      <td id=\"T_fc7b8_row9_col9\" class=\"data row9 col9\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row9_col10\" class=\"data row9 col10\" >-0.048707</td>\n",
              "      <td id=\"T_fc7b8_row9_col11\" class=\"data row9 col11\" >0.006844</td>\n",
              "      <td id=\"T_fc7b8_row9_col12\" class=\"data row9 col12\" >0.035096</td>\n",
              "      <td id=\"T_fc7b8_row9_col13\" class=\"data row9 col13\" >-0.045551</td>\n",
              "      <td id=\"T_fc7b8_row9_col14\" class=\"data row9 col14\" >0.033980</td>\n",
              "      <td id=\"T_fc7b8_row9_col15\" class=\"data row9 col15\" >-0.003289</td>\n",
              "      <td id=\"T_fc7b8_row9_col16\" class=\"data row9 col16\" >-0.013508</td>\n",
              "      <td id=\"T_fc7b8_row9_col17\" class=\"data row9 col17\" >0.004246</td>\n",
              "      <td id=\"T_fc7b8_row9_col18\" class=\"data row9 col18\" >-0.049147</td>\n",
              "      <td id=\"T_fc7b8_row9_col19\" class=\"data row9 col19\" >-0.010685</td>\n",
              "      <td id=\"T_fc7b8_row9_col20\" class=\"data row9 col20\" >-0.004410</td>\n",
              "      <td id=\"T_fc7b8_row9_col21\" class=\"data row9 col21\" >0.005610</td>\n",
              "      <td id=\"T_fc7b8_row9_col22\" class=\"data row9 col22\" >-0.000720</td>\n",
              "      <td id=\"T_fc7b8_row9_col23\" class=\"data row9 col23\" >0.010393</td>\n",
              "      <td id=\"T_fc7b8_row9_col24\" class=\"data row9 col24\" >0.010352</td>\n",
              "      <td id=\"T_fc7b8_row9_col25\" class=\"data row9 col25\" >-0.005560</td>\n",
              "      <td id=\"T_fc7b8_row9_col26\" class=\"data row9 col26\" >-0.008910</td>\n",
              "      <td id=\"T_fc7b8_row9_col27\" class=\"data row9 col27\" >0.000076</td>\n",
              "      <td id=\"T_fc7b8_row9_col28\" class=\"data row9 col28\" >-0.098771</td>\n",
              "      <td id=\"T_fc7b8_row9_col29\" class=\"data row9 col29\" >-0.001799</td>\n",
              "      <td id=\"T_fc7b8_row9_col30\" class=\"data row9 col30\" >0.064265</td>\n",
              "      <td id=\"T_fc7b8_row9_col31\" class=\"data row9 col31\" >0.017631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row10\" class=\"row_heading level0 row10\" >f_10</th>\n",
              "      <td id=\"T_fc7b8_row10_col0\" class=\"data row10 col0\" >0.000767</td>\n",
              "      <td id=\"T_fc7b8_row10_col1\" class=\"data row10 col1\" >0.001659</td>\n",
              "      <td id=\"T_fc7b8_row10_col2\" class=\"data row10 col2\" >0.001376</td>\n",
              "      <td id=\"T_fc7b8_row10_col3\" class=\"data row10 col3\" >-0.000506</td>\n",
              "      <td id=\"T_fc7b8_row10_col4\" class=\"data row10 col4\" >-0.003212</td>\n",
              "      <td id=\"T_fc7b8_row10_col5\" class=\"data row10 col5\" >-0.000877</td>\n",
              "      <td id=\"T_fc7b8_row10_col6\" class=\"data row10 col6\" >-0.000198</td>\n",
              "      <td id=\"T_fc7b8_row10_col7\" class=\"data row10 col7\" >-0.087163</td>\n",
              "      <td id=\"T_fc7b8_row10_col8\" class=\"data row10 col8\" >-0.082123</td>\n",
              "      <td id=\"T_fc7b8_row10_col9\" class=\"data row10 col9\" >-0.048707</td>\n",
              "      <td id=\"T_fc7b8_row10_col10\" class=\"data row10 col10\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row10_col11\" class=\"data row10 col11\" >0.158144</td>\n",
              "      <td id=\"T_fc7b8_row10_col12\" class=\"data row10 col12\" >-0.036609</td>\n",
              "      <td id=\"T_fc7b8_row10_col13\" class=\"data row10 col13\" >0.034905</td>\n",
              "      <td id=\"T_fc7b8_row10_col14\" class=\"data row10 col14\" >0.011847</td>\n",
              "      <td id=\"T_fc7b8_row10_col15\" class=\"data row10 col15\" >-0.002882</td>\n",
              "      <td id=\"T_fc7b8_row10_col16\" class=\"data row10 col16\" >-0.043113</td>\n",
              "      <td id=\"T_fc7b8_row10_col17\" class=\"data row10 col17\" >0.010979</td>\n",
              "      <td id=\"T_fc7b8_row10_col18\" class=\"data row10 col18\" >0.065922</td>\n",
              "      <td id=\"T_fc7b8_row10_col19\" class=\"data row10 col19\" >0.006211</td>\n",
              "      <td id=\"T_fc7b8_row10_col20\" class=\"data row10 col20\" >0.003863</td>\n",
              "      <td id=\"T_fc7b8_row10_col21\" class=\"data row10 col21\" >-0.004228</td>\n",
              "      <td id=\"T_fc7b8_row10_col22\" class=\"data row10 col22\" >-0.000269</td>\n",
              "      <td id=\"T_fc7b8_row10_col23\" class=\"data row10 col23\" >-0.002578</td>\n",
              "      <td id=\"T_fc7b8_row10_col24\" class=\"data row10 col24\" >-0.004359</td>\n",
              "      <td id=\"T_fc7b8_row10_col25\" class=\"data row10 col25\" >0.001431</td>\n",
              "      <td id=\"T_fc7b8_row10_col26\" class=\"data row10 col26\" >0.005743</td>\n",
              "      <td id=\"T_fc7b8_row10_col27\" class=\"data row10 col27\" >-0.001150</td>\n",
              "      <td id=\"T_fc7b8_row10_col28\" class=\"data row10 col28\" >0.129957</td>\n",
              "      <td id=\"T_fc7b8_row10_col29\" class=\"data row10 col29\" >-0.002315</td>\n",
              "      <td id=\"T_fc7b8_row10_col30\" class=\"data row10 col30\" >-0.040612</td>\n",
              "      <td id=\"T_fc7b8_row10_col31\" class=\"data row10 col31\" >0.017759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row11\" class=\"row_heading level0 row11\" >f_11</th>\n",
              "      <td id=\"T_fc7b8_row11_col0\" class=\"data row11 col0\" >0.001137</td>\n",
              "      <td id=\"T_fc7b8_row11_col1\" class=\"data row11 col1\" >0.001605</td>\n",
              "      <td id=\"T_fc7b8_row11_col2\" class=\"data row11 col2\" >-0.002289</td>\n",
              "      <td id=\"T_fc7b8_row11_col3\" class=\"data row11 col3\" >0.001975</td>\n",
              "      <td id=\"T_fc7b8_row11_col4\" class=\"data row11 col4\" >-0.000035</td>\n",
              "      <td id=\"T_fc7b8_row11_col5\" class=\"data row11 col5\" >-0.000970</td>\n",
              "      <td id=\"T_fc7b8_row11_col6\" class=\"data row11 col6\" >0.001057</td>\n",
              "      <td id=\"T_fc7b8_row11_col7\" class=\"data row11 col7\" >-0.133388</td>\n",
              "      <td id=\"T_fc7b8_row11_col8\" class=\"data row11 col8\" >-0.097914</td>\n",
              "      <td id=\"T_fc7b8_row11_col9\" class=\"data row11 col9\" >0.006844</td>\n",
              "      <td id=\"T_fc7b8_row11_col10\" class=\"data row11 col10\" >0.158144</td>\n",
              "      <td id=\"T_fc7b8_row11_col11\" class=\"data row11 col11\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row11_col12\" class=\"data row11 col12\" >-0.025318</td>\n",
              "      <td id=\"T_fc7b8_row11_col13\" class=\"data row11 col13\" >-0.005039</td>\n",
              "      <td id=\"T_fc7b8_row11_col14\" class=\"data row11 col14\" >0.056802</td>\n",
              "      <td id=\"T_fc7b8_row11_col15\" class=\"data row11 col15\" >-0.046495</td>\n",
              "      <td id=\"T_fc7b8_row11_col16\" class=\"data row11 col16\" >-0.092317</td>\n",
              "      <td id=\"T_fc7b8_row11_col17\" class=\"data row11 col17\" >0.078684</td>\n",
              "      <td id=\"T_fc7b8_row11_col18\" class=\"data row11 col18\" >0.082832</td>\n",
              "      <td id=\"T_fc7b8_row11_col19\" class=\"data row11 col19\" >0.004794</td>\n",
              "      <td id=\"T_fc7b8_row11_col20\" class=\"data row11 col20\" >0.004746</td>\n",
              "      <td id=\"T_fc7b8_row11_col21\" class=\"data row11 col21\" >-0.004797</td>\n",
              "      <td id=\"T_fc7b8_row11_col22\" class=\"data row11 col22\" >0.002493</td>\n",
              "      <td id=\"T_fc7b8_row11_col23\" class=\"data row11 col23\" >-0.000825</td>\n",
              "      <td id=\"T_fc7b8_row11_col24\" class=\"data row11 col24\" >-0.004515</td>\n",
              "      <td id=\"T_fc7b8_row11_col25\" class=\"data row11 col25\" >0.002085</td>\n",
              "      <td id=\"T_fc7b8_row11_col26\" class=\"data row11 col26\" >0.005425</td>\n",
              "      <td id=\"T_fc7b8_row11_col27\" class=\"data row11 col27\" >0.000491</td>\n",
              "      <td id=\"T_fc7b8_row11_col28\" class=\"data row11 col28\" >0.093383</td>\n",
              "      <td id=\"T_fc7b8_row11_col29\" class=\"data row11 col29\" >-0.002664</td>\n",
              "      <td id=\"T_fc7b8_row11_col30\" class=\"data row11 col30\" >-0.074922</td>\n",
              "      <td id=\"T_fc7b8_row11_col31\" class=\"data row11 col31\" >0.017883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row12\" class=\"row_heading level0 row12\" >f_12</th>\n",
              "      <td id=\"T_fc7b8_row12_col0\" class=\"data row12 col0\" >-0.001343</td>\n",
              "      <td id=\"T_fc7b8_row12_col1\" class=\"data row12 col1\" >-0.001396</td>\n",
              "      <td id=\"T_fc7b8_row12_col2\" class=\"data row12 col2\" >0.001972</td>\n",
              "      <td id=\"T_fc7b8_row12_col3\" class=\"data row12 col3\" >0.000630</td>\n",
              "      <td id=\"T_fc7b8_row12_col4\" class=\"data row12 col4\" >0.001381</td>\n",
              "      <td id=\"T_fc7b8_row12_col5\" class=\"data row12 col5\" >0.003442</td>\n",
              "      <td id=\"T_fc7b8_row12_col6\" class=\"data row12 col6\" >0.001361</td>\n",
              "      <td id=\"T_fc7b8_row12_col7\" class=\"data row12 col7\" >0.098123</td>\n",
              "      <td id=\"T_fc7b8_row12_col8\" class=\"data row12 col8\" >0.013528</td>\n",
              "      <td id=\"T_fc7b8_row12_col9\" class=\"data row12 col9\" >0.035096</td>\n",
              "      <td id=\"T_fc7b8_row12_col10\" class=\"data row12 col10\" >-0.036609</td>\n",
              "      <td id=\"T_fc7b8_row12_col11\" class=\"data row12 col11\" >-0.025318</td>\n",
              "      <td id=\"T_fc7b8_row12_col12\" class=\"data row12 col12\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row12_col13\" class=\"data row12 col13\" >0.013717</td>\n",
              "      <td id=\"T_fc7b8_row12_col14\" class=\"data row12 col14\" >-0.009853</td>\n",
              "      <td id=\"T_fc7b8_row12_col15\" class=\"data row12 col15\" >-0.017417</td>\n",
              "      <td id=\"T_fc7b8_row12_col16\" class=\"data row12 col16\" >-0.013114</td>\n",
              "      <td id=\"T_fc7b8_row12_col17\" class=\"data row12 col17\" >-0.039093</td>\n",
              "      <td id=\"T_fc7b8_row12_col18\" class=\"data row12 col18\" >-0.006136</td>\n",
              "      <td id=\"T_fc7b8_row12_col19\" class=\"data row12 col19\" >0.000286</td>\n",
              "      <td id=\"T_fc7b8_row12_col20\" class=\"data row12 col20\" >-0.001601</td>\n",
              "      <td id=\"T_fc7b8_row12_col21\" class=\"data row12 col21\" >-0.001400</td>\n",
              "      <td id=\"T_fc7b8_row12_col22\" class=\"data row12 col22\" >-0.001321</td>\n",
              "      <td id=\"T_fc7b8_row12_col23\" class=\"data row12 col23\" >0.003820</td>\n",
              "      <td id=\"T_fc7b8_row12_col24\" class=\"data row12 col24\" >0.000350</td>\n",
              "      <td id=\"T_fc7b8_row12_col25\" class=\"data row12 col25\" >-0.001149</td>\n",
              "      <td id=\"T_fc7b8_row12_col26\" class=\"data row12 col26\" >-0.000477</td>\n",
              "      <td id=\"T_fc7b8_row12_col27\" class=\"data row12 col27\" >0.000580</td>\n",
              "      <td id=\"T_fc7b8_row12_col28\" class=\"data row12 col28\" >-0.112364</td>\n",
              "      <td id=\"T_fc7b8_row12_col29\" class=\"data row12 col29\" >-0.002644</td>\n",
              "      <td id=\"T_fc7b8_row12_col30\" class=\"data row12 col30\" >-0.004650</td>\n",
              "      <td id=\"T_fc7b8_row12_col31\" class=\"data row12 col31\" >-0.032927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row13\" class=\"row_heading level0 row13\" >f_13</th>\n",
              "      <td id=\"T_fc7b8_row13_col0\" class=\"data row13 col0\" >0.000838</td>\n",
              "      <td id=\"T_fc7b8_row13_col1\" class=\"data row13 col1\" >-0.000931</td>\n",
              "      <td id=\"T_fc7b8_row13_col2\" class=\"data row13 col2\" >-0.002787</td>\n",
              "      <td id=\"T_fc7b8_row13_col3\" class=\"data row13 col3\" >-0.001187</td>\n",
              "      <td id=\"T_fc7b8_row13_col4\" class=\"data row13 col4\" >-0.000509</td>\n",
              "      <td id=\"T_fc7b8_row13_col5\" class=\"data row13 col5\" >0.001086</td>\n",
              "      <td id=\"T_fc7b8_row13_col6\" class=\"data row13 col6\" >-0.000598</td>\n",
              "      <td id=\"T_fc7b8_row13_col7\" class=\"data row13 col7\" >0.059017</td>\n",
              "      <td id=\"T_fc7b8_row13_col8\" class=\"data row13 col8\" >0.036971</td>\n",
              "      <td id=\"T_fc7b8_row13_col9\" class=\"data row13 col9\" >-0.045551</td>\n",
              "      <td id=\"T_fc7b8_row13_col10\" class=\"data row13 col10\" >0.034905</td>\n",
              "      <td id=\"T_fc7b8_row13_col11\" class=\"data row13 col11\" >-0.005039</td>\n",
              "      <td id=\"T_fc7b8_row13_col12\" class=\"data row13 col12\" >0.013717</td>\n",
              "      <td id=\"T_fc7b8_row13_col13\" class=\"data row13 col13\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row13_col14\" class=\"data row13 col14\" >-0.016223</td>\n",
              "      <td id=\"T_fc7b8_row13_col15\" class=\"data row13 col15\" >0.036557</td>\n",
              "      <td id=\"T_fc7b8_row13_col16\" class=\"data row13 col16\" >-0.014038</td>\n",
              "      <td id=\"T_fc7b8_row13_col17\" class=\"data row13 col17\" >-0.037716</td>\n",
              "      <td id=\"T_fc7b8_row13_col18\" class=\"data row13 col18\" >0.040916</td>\n",
              "      <td id=\"T_fc7b8_row13_col19\" class=\"data row13 col19\" >0.004873</td>\n",
              "      <td id=\"T_fc7b8_row13_col20\" class=\"data row13 col20\" >0.001792</td>\n",
              "      <td id=\"T_fc7b8_row13_col21\" class=\"data row13 col21\" >-0.001776</td>\n",
              "      <td id=\"T_fc7b8_row13_col22\" class=\"data row13 col22\" >0.000536</td>\n",
              "      <td id=\"T_fc7b8_row13_col23\" class=\"data row13 col23\" >-0.001372</td>\n",
              "      <td id=\"T_fc7b8_row13_col24\" class=\"data row13 col24\" >-0.003143</td>\n",
              "      <td id=\"T_fc7b8_row13_col25\" class=\"data row13 col25\" >-0.000754</td>\n",
              "      <td id=\"T_fc7b8_row13_col26\" class=\"data row13 col26\" >0.004221</td>\n",
              "      <td id=\"T_fc7b8_row13_col27\" class=\"data row13 col27\" >-0.001171</td>\n",
              "      <td id=\"T_fc7b8_row13_col28\" class=\"data row13 col28\" >0.010649</td>\n",
              "      <td id=\"T_fc7b8_row13_col29\" class=\"data row13 col29\" >-0.001137</td>\n",
              "      <td id=\"T_fc7b8_row13_col30\" class=\"data row13 col30\" >-0.033999</td>\n",
              "      <td id=\"T_fc7b8_row13_col31\" class=\"data row13 col31\" >-0.027220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row14\" class=\"row_heading level0 row14\" >f_14</th>\n",
              "      <td id=\"T_fc7b8_row14_col0\" class=\"data row14 col0\" >0.001039</td>\n",
              "      <td id=\"T_fc7b8_row14_col1\" class=\"data row14 col1\" >0.000311</td>\n",
              "      <td id=\"T_fc7b8_row14_col2\" class=\"data row14 col2\" >0.000814</td>\n",
              "      <td id=\"T_fc7b8_row14_col3\" class=\"data row14 col3\" >0.000104</td>\n",
              "      <td id=\"T_fc7b8_row14_col4\" class=\"data row14 col4\" >0.001740</td>\n",
              "      <td id=\"T_fc7b8_row14_col5\" class=\"data row14 col5\" >0.000625</td>\n",
              "      <td id=\"T_fc7b8_row14_col6\" class=\"data row14 col6\" >-0.000569</td>\n",
              "      <td id=\"T_fc7b8_row14_col7\" class=\"data row14 col7\" >-0.058409</td>\n",
              "      <td id=\"T_fc7b8_row14_col8\" class=\"data row14 col8\" >-0.041036</td>\n",
              "      <td id=\"T_fc7b8_row14_col9\" class=\"data row14 col9\" >0.033980</td>\n",
              "      <td id=\"T_fc7b8_row14_col10\" class=\"data row14 col10\" >0.011847</td>\n",
              "      <td id=\"T_fc7b8_row14_col11\" class=\"data row14 col11\" >0.056802</td>\n",
              "      <td id=\"T_fc7b8_row14_col12\" class=\"data row14 col12\" >-0.009853</td>\n",
              "      <td id=\"T_fc7b8_row14_col13\" class=\"data row14 col13\" >-0.016223</td>\n",
              "      <td id=\"T_fc7b8_row14_col14\" class=\"data row14 col14\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row14_col15\" class=\"data row14 col15\" >0.031184</td>\n",
              "      <td id=\"T_fc7b8_row14_col16\" class=\"data row14 col16\" >-0.093436</td>\n",
              "      <td id=\"T_fc7b8_row14_col17\" class=\"data row14 col17\" >0.047769</td>\n",
              "      <td id=\"T_fc7b8_row14_col18\" class=\"data row14 col18\" >-0.036617</td>\n",
              "      <td id=\"T_fc7b8_row14_col19\" class=\"data row14 col19\" >0.003080</td>\n",
              "      <td id=\"T_fc7b8_row14_col20\" class=\"data row14 col20\" >0.000760</td>\n",
              "      <td id=\"T_fc7b8_row14_col21\" class=\"data row14 col21\" >-0.001688</td>\n",
              "      <td id=\"T_fc7b8_row14_col22\" class=\"data row14 col22\" >-0.000289</td>\n",
              "      <td id=\"T_fc7b8_row14_col23\" class=\"data row14 col23\" >-0.002676</td>\n",
              "      <td id=\"T_fc7b8_row14_col24\" class=\"data row14 col24\" >0.000821</td>\n",
              "      <td id=\"T_fc7b8_row14_col25\" class=\"data row14 col25\" >-0.001142</td>\n",
              "      <td id=\"T_fc7b8_row14_col26\" class=\"data row14 col26\" >0.001857</td>\n",
              "      <td id=\"T_fc7b8_row14_col27\" class=\"data row14 col27\" >0.001518</td>\n",
              "      <td id=\"T_fc7b8_row14_col28\" class=\"data row14 col28\" >-0.019842</td>\n",
              "      <td id=\"T_fc7b8_row14_col29\" class=\"data row14 col29\" >-0.001154</td>\n",
              "      <td id=\"T_fc7b8_row14_col30\" class=\"data row14 col30\" >-0.007883</td>\n",
              "      <td id=\"T_fc7b8_row14_col31\" class=\"data row14 col31\" >-0.011617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row15\" class=\"row_heading level0 row15\" >f_15</th>\n",
              "      <td id=\"T_fc7b8_row15_col0\" class=\"data row15 col0\" >-0.002240</td>\n",
              "      <td id=\"T_fc7b8_row15_col1\" class=\"data row15 col1\" >-0.000784</td>\n",
              "      <td id=\"T_fc7b8_row15_col2\" class=\"data row15 col2\" >0.000415</td>\n",
              "      <td id=\"T_fc7b8_row15_col3\" class=\"data row15 col3\" >-0.001986</td>\n",
              "      <td id=\"T_fc7b8_row15_col4\" class=\"data row15 col4\" >0.000669</td>\n",
              "      <td id=\"T_fc7b8_row15_col5\" class=\"data row15 col5\" >-0.000867</td>\n",
              "      <td id=\"T_fc7b8_row15_col6\" class=\"data row15 col6\" >-0.000673</td>\n",
              "      <td id=\"T_fc7b8_row15_col7\" class=\"data row15 col7\" >0.049611</td>\n",
              "      <td id=\"T_fc7b8_row15_col8\" class=\"data row15 col8\" >0.010796</td>\n",
              "      <td id=\"T_fc7b8_row15_col9\" class=\"data row15 col9\" >-0.003289</td>\n",
              "      <td id=\"T_fc7b8_row15_col10\" class=\"data row15 col10\" >-0.002882</td>\n",
              "      <td id=\"T_fc7b8_row15_col11\" class=\"data row15 col11\" >-0.046495</td>\n",
              "      <td id=\"T_fc7b8_row15_col12\" class=\"data row15 col12\" >-0.017417</td>\n",
              "      <td id=\"T_fc7b8_row15_col13\" class=\"data row15 col13\" >0.036557</td>\n",
              "      <td id=\"T_fc7b8_row15_col14\" class=\"data row15 col14\" >0.031184</td>\n",
              "      <td id=\"T_fc7b8_row15_col15\" class=\"data row15 col15\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row15_col16\" class=\"data row15 col16\" >-0.030841</td>\n",
              "      <td id=\"T_fc7b8_row15_col17\" class=\"data row15 col17\" >-0.031416</td>\n",
              "      <td id=\"T_fc7b8_row15_col18\" class=\"data row15 col18\" >-0.010115</td>\n",
              "      <td id=\"T_fc7b8_row15_col19\" class=\"data row15 col19\" >-0.001261</td>\n",
              "      <td id=\"T_fc7b8_row15_col20\" class=\"data row15 col20\" >-0.002196</td>\n",
              "      <td id=\"T_fc7b8_row15_col21\" class=\"data row15 col21\" >0.002054</td>\n",
              "      <td id=\"T_fc7b8_row15_col22\" class=\"data row15 col22\" >0.000327</td>\n",
              "      <td id=\"T_fc7b8_row15_col23\" class=\"data row15 col23\" >0.005402</td>\n",
              "      <td id=\"T_fc7b8_row15_col24\" class=\"data row15 col24\" >0.002660</td>\n",
              "      <td id=\"T_fc7b8_row15_col25\" class=\"data row15 col25\" >-0.001395</td>\n",
              "      <td id=\"T_fc7b8_row15_col26\" class=\"data row15 col26\" >-0.003119</td>\n",
              "      <td id=\"T_fc7b8_row15_col27\" class=\"data row15 col27\" >-0.002529</td>\n",
              "      <td id=\"T_fc7b8_row15_col28\" class=\"data row15 col28\" >0.002563</td>\n",
              "      <td id=\"T_fc7b8_row15_col29\" class=\"data row15 col29\" >0.001558</td>\n",
              "      <td id=\"T_fc7b8_row15_col30\" class=\"data row15 col30\" >0.029299</td>\n",
              "      <td id=\"T_fc7b8_row15_col31\" class=\"data row15 col31\" >0.003933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row16\" class=\"row_heading level0 row16\" >f_16</th>\n",
              "      <td id=\"T_fc7b8_row16_col0\" class=\"data row16 col0\" >-0.000768</td>\n",
              "      <td id=\"T_fc7b8_row16_col1\" class=\"data row16 col1\" >-0.001258</td>\n",
              "      <td id=\"T_fc7b8_row16_col2\" class=\"data row16 col2\" >-0.002015</td>\n",
              "      <td id=\"T_fc7b8_row16_col3\" class=\"data row16 col3\" >0.000627</td>\n",
              "      <td id=\"T_fc7b8_row16_col4\" class=\"data row16 col4\" >-0.000974</td>\n",
              "      <td id=\"T_fc7b8_row16_col5\" class=\"data row16 col5\" >-0.000053</td>\n",
              "      <td id=\"T_fc7b8_row16_col6\" class=\"data row16 col6\" >0.000766</td>\n",
              "      <td id=\"T_fc7b8_row16_col7\" class=\"data row16 col7\" >0.054666</td>\n",
              "      <td id=\"T_fc7b8_row16_col8\" class=\"data row16 col8\" >0.040876</td>\n",
              "      <td id=\"T_fc7b8_row16_col9\" class=\"data row16 col9\" >-0.013508</td>\n",
              "      <td id=\"T_fc7b8_row16_col10\" class=\"data row16 col10\" >-0.043113</td>\n",
              "      <td id=\"T_fc7b8_row16_col11\" class=\"data row16 col11\" >-0.092317</td>\n",
              "      <td id=\"T_fc7b8_row16_col12\" class=\"data row16 col12\" >-0.013114</td>\n",
              "      <td id=\"T_fc7b8_row16_col13\" class=\"data row16 col13\" >-0.014038</td>\n",
              "      <td id=\"T_fc7b8_row16_col14\" class=\"data row16 col14\" >-0.093436</td>\n",
              "      <td id=\"T_fc7b8_row16_col15\" class=\"data row16 col15\" >-0.030841</td>\n",
              "      <td id=\"T_fc7b8_row16_col16\" class=\"data row16 col16\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row16_col17\" class=\"data row16 col17\" >-0.082780</td>\n",
              "      <td id=\"T_fc7b8_row16_col18\" class=\"data row16 col18\" >-0.009191</td>\n",
              "      <td id=\"T_fc7b8_row16_col19\" class=\"data row16 col19\" >0.002222</td>\n",
              "      <td id=\"T_fc7b8_row16_col20\" class=\"data row16 col20\" >0.001713</td>\n",
              "      <td id=\"T_fc7b8_row16_col21\" class=\"data row16 col21\" >0.000824</td>\n",
              "      <td id=\"T_fc7b8_row16_col22\" class=\"data row16 col22\" >-0.000323</td>\n",
              "      <td id=\"T_fc7b8_row16_col23\" class=\"data row16 col23\" >-0.002612</td>\n",
              "      <td id=\"T_fc7b8_row16_col24\" class=\"data row16 col24\" >-0.003071</td>\n",
              "      <td id=\"T_fc7b8_row16_col25\" class=\"data row16 col25\" >0.000451</td>\n",
              "      <td id=\"T_fc7b8_row16_col26\" class=\"data row16 col26\" >0.004094</td>\n",
              "      <td id=\"T_fc7b8_row16_col27\" class=\"data row16 col27\" >-0.001033</td>\n",
              "      <td id=\"T_fc7b8_row16_col28\" class=\"data row16 col28\" >0.097340</td>\n",
              "      <td id=\"T_fc7b8_row16_col29\" class=\"data row16 col29\" >-0.000131</td>\n",
              "      <td id=\"T_fc7b8_row16_col30\" class=\"data row16 col30\" >-0.032506</td>\n",
              "      <td id=\"T_fc7b8_row16_col31\" class=\"data row16 col31\" >0.007388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row17\" class=\"row_heading level0 row17\" >f_17</th>\n",
              "      <td id=\"T_fc7b8_row17_col0\" class=\"data row17 col0\" >0.000796</td>\n",
              "      <td id=\"T_fc7b8_row17_col1\" class=\"data row17 col1\" >-0.000094</td>\n",
              "      <td id=\"T_fc7b8_row17_col2\" class=\"data row17 col2\" >0.000429</td>\n",
              "      <td id=\"T_fc7b8_row17_col3\" class=\"data row17 col3\" >0.000523</td>\n",
              "      <td id=\"T_fc7b8_row17_col4\" class=\"data row17 col4\" >0.000129</td>\n",
              "      <td id=\"T_fc7b8_row17_col5\" class=\"data row17 col5\" >0.000406</td>\n",
              "      <td id=\"T_fc7b8_row17_col6\" class=\"data row17 col6\" >0.000842</td>\n",
              "      <td id=\"T_fc7b8_row17_col7\" class=\"data row17 col7\" >-0.148858</td>\n",
              "      <td id=\"T_fc7b8_row17_col8\" class=\"data row17 col8\" >-0.049987</td>\n",
              "      <td id=\"T_fc7b8_row17_col9\" class=\"data row17 col9\" >0.004246</td>\n",
              "      <td id=\"T_fc7b8_row17_col10\" class=\"data row17 col10\" >0.010979</td>\n",
              "      <td id=\"T_fc7b8_row17_col11\" class=\"data row17 col11\" >0.078684</td>\n",
              "      <td id=\"T_fc7b8_row17_col12\" class=\"data row17 col12\" >-0.039093</td>\n",
              "      <td id=\"T_fc7b8_row17_col13\" class=\"data row17 col13\" >-0.037716</td>\n",
              "      <td id=\"T_fc7b8_row17_col14\" class=\"data row17 col14\" >0.047769</td>\n",
              "      <td id=\"T_fc7b8_row17_col15\" class=\"data row17 col15\" >-0.031416</td>\n",
              "      <td id=\"T_fc7b8_row17_col16\" class=\"data row17 col16\" >-0.082780</td>\n",
              "      <td id=\"T_fc7b8_row17_col17\" class=\"data row17 col17\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row17_col18\" class=\"data row17 col18\" >-0.025496</td>\n",
              "      <td id=\"T_fc7b8_row17_col19\" class=\"data row17 col19\" >-0.000106</td>\n",
              "      <td id=\"T_fc7b8_row17_col20\" class=\"data row17 col20\" >0.001341</td>\n",
              "      <td id=\"T_fc7b8_row17_col21\" class=\"data row17 col21\" >-0.000362</td>\n",
              "      <td id=\"T_fc7b8_row17_col22\" class=\"data row17 col22\" >0.000126</td>\n",
              "      <td id=\"T_fc7b8_row17_col23\" class=\"data row17 col23\" >-0.001260</td>\n",
              "      <td id=\"T_fc7b8_row17_col24\" class=\"data row17 col24\" >-0.001185</td>\n",
              "      <td id=\"T_fc7b8_row17_col25\" class=\"data row17 col25\" >-0.000012</td>\n",
              "      <td id=\"T_fc7b8_row17_col26\" class=\"data row17 col26\" >0.002307</td>\n",
              "      <td id=\"T_fc7b8_row17_col27\" class=\"data row17 col27\" >0.001190</td>\n",
              "      <td id=\"T_fc7b8_row17_col28\" class=\"data row17 col28\" >-0.075579</td>\n",
              "      <td id=\"T_fc7b8_row17_col29\" class=\"data row17 col29\" >0.001542</td>\n",
              "      <td id=\"T_fc7b8_row17_col30\" class=\"data row17 col30\" >0.006195</td>\n",
              "      <td id=\"T_fc7b8_row17_col31\" class=\"data row17 col31\" >0.031436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row18\" class=\"row_heading level0 row18\" >f_18</th>\n",
              "      <td id=\"T_fc7b8_row18_col0\" class=\"data row18 col0\" >-0.000933</td>\n",
              "      <td id=\"T_fc7b8_row18_col1\" class=\"data row18 col1\" >0.001100</td>\n",
              "      <td id=\"T_fc7b8_row18_col2\" class=\"data row18 col2\" >-0.000059</td>\n",
              "      <td id=\"T_fc7b8_row18_col3\" class=\"data row18 col3\" >0.001095</td>\n",
              "      <td id=\"T_fc7b8_row18_col4\" class=\"data row18 col4\" >0.000491</td>\n",
              "      <td id=\"T_fc7b8_row18_col5\" class=\"data row18 col5\" >0.000159</td>\n",
              "      <td id=\"T_fc7b8_row18_col6\" class=\"data row18 col6\" >-0.000598</td>\n",
              "      <td id=\"T_fc7b8_row18_col7\" class=\"data row18 col7\" >0.026498</td>\n",
              "      <td id=\"T_fc7b8_row18_col8\" class=\"data row18 col8\" >0.066853</td>\n",
              "      <td id=\"T_fc7b8_row18_col9\" class=\"data row18 col9\" >-0.049147</td>\n",
              "      <td id=\"T_fc7b8_row18_col10\" class=\"data row18 col10\" >0.065922</td>\n",
              "      <td id=\"T_fc7b8_row18_col11\" class=\"data row18 col11\" >0.082832</td>\n",
              "      <td id=\"T_fc7b8_row18_col12\" class=\"data row18 col12\" >-0.006136</td>\n",
              "      <td id=\"T_fc7b8_row18_col13\" class=\"data row18 col13\" >0.040916</td>\n",
              "      <td id=\"T_fc7b8_row18_col14\" class=\"data row18 col14\" >-0.036617</td>\n",
              "      <td id=\"T_fc7b8_row18_col15\" class=\"data row18 col15\" >-0.010115</td>\n",
              "      <td id=\"T_fc7b8_row18_col16\" class=\"data row18 col16\" >-0.009191</td>\n",
              "      <td id=\"T_fc7b8_row18_col17\" class=\"data row18 col17\" >-0.025496</td>\n",
              "      <td id=\"T_fc7b8_row18_col18\" class=\"data row18 col18\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row18_col19\" class=\"data row18 col19\" >-0.002679</td>\n",
              "      <td id=\"T_fc7b8_row18_col20\" class=\"data row18 col20\" >-0.001964</td>\n",
              "      <td id=\"T_fc7b8_row18_col21\" class=\"data row18 col21\" >-0.001250</td>\n",
              "      <td id=\"T_fc7b8_row18_col22\" class=\"data row18 col22\" >0.001148</td>\n",
              "      <td id=\"T_fc7b8_row18_col23\" class=\"data row18 col23\" >0.002887</td>\n",
              "      <td id=\"T_fc7b8_row18_col24\" class=\"data row18 col24\" >0.002835</td>\n",
              "      <td id=\"T_fc7b8_row18_col25\" class=\"data row18 col25\" >0.000342</td>\n",
              "      <td id=\"T_fc7b8_row18_col26\" class=\"data row18 col26\" >-0.002298</td>\n",
              "      <td id=\"T_fc7b8_row18_col27\" class=\"data row18 col27\" >-0.001695</td>\n",
              "      <td id=\"T_fc7b8_row18_col28\" class=\"data row18 col28\" >0.160595</td>\n",
              "      <td id=\"T_fc7b8_row18_col29\" class=\"data row18 col29\" >-0.000467</td>\n",
              "      <td id=\"T_fc7b8_row18_col30\" class=\"data row18 col30\" >-0.012037</td>\n",
              "      <td id=\"T_fc7b8_row18_col31\" class=\"data row18 col31\" >-0.030131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row19\" class=\"row_heading level0 row19\" >f_19</th>\n",
              "      <td id=\"T_fc7b8_row19_col0\" class=\"data row19 col0\" >0.000582</td>\n",
              "      <td id=\"T_fc7b8_row19_col1\" class=\"data row19 col1\" >-0.000696</td>\n",
              "      <td id=\"T_fc7b8_row19_col2\" class=\"data row19 col2\" >-0.000927</td>\n",
              "      <td id=\"T_fc7b8_row19_col3\" class=\"data row19 col3\" >0.001529</td>\n",
              "      <td id=\"T_fc7b8_row19_col4\" class=\"data row19 col4\" >0.000905</td>\n",
              "      <td id=\"T_fc7b8_row19_col5\" class=\"data row19 col5\" >-0.001009</td>\n",
              "      <td id=\"T_fc7b8_row19_col6\" class=\"data row19 col6\" >0.001308</td>\n",
              "      <td id=\"T_fc7b8_row19_col7\" class=\"data row19 col7\" >-0.003231</td>\n",
              "      <td id=\"T_fc7b8_row19_col8\" class=\"data row19 col8\" >-0.005749</td>\n",
              "      <td id=\"T_fc7b8_row19_col9\" class=\"data row19 col9\" >-0.010685</td>\n",
              "      <td id=\"T_fc7b8_row19_col10\" class=\"data row19 col10\" >0.006211</td>\n",
              "      <td id=\"T_fc7b8_row19_col11\" class=\"data row19 col11\" >0.004794</td>\n",
              "      <td id=\"T_fc7b8_row19_col12\" class=\"data row19 col12\" >0.000286</td>\n",
              "      <td id=\"T_fc7b8_row19_col13\" class=\"data row19 col13\" >0.004873</td>\n",
              "      <td id=\"T_fc7b8_row19_col14\" class=\"data row19 col14\" >0.003080</td>\n",
              "      <td id=\"T_fc7b8_row19_col15\" class=\"data row19 col15\" >-0.001261</td>\n",
              "      <td id=\"T_fc7b8_row19_col16\" class=\"data row19 col16\" >0.002222</td>\n",
              "      <td id=\"T_fc7b8_row19_col17\" class=\"data row19 col17\" >-0.000106</td>\n",
              "      <td id=\"T_fc7b8_row19_col18\" class=\"data row19 col18\" >-0.002679</td>\n",
              "      <td id=\"T_fc7b8_row19_col19\" class=\"data row19 col19\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row19_col20\" class=\"data row19 col20\" >-0.075614</td>\n",
              "      <td id=\"T_fc7b8_row19_col21\" class=\"data row19 col21\" >0.025064</td>\n",
              "      <td id=\"T_fc7b8_row19_col22\" class=\"data row19 col22\" >-0.058058</td>\n",
              "      <td id=\"T_fc7b8_row19_col23\" class=\"data row19 col23\" >-0.047755</td>\n",
              "      <td id=\"T_fc7b8_row19_col24\" class=\"data row19 col24\" >-0.098508</td>\n",
              "      <td id=\"T_fc7b8_row19_col25\" class=\"data row19 col25\" >-0.017172</td>\n",
              "      <td id=\"T_fc7b8_row19_col26\" class=\"data row19 col26\" >-0.016280</td>\n",
              "      <td id=\"T_fc7b8_row19_col27\" class=\"data row19 col27\" >0.000565</td>\n",
              "      <td id=\"T_fc7b8_row19_col28\" class=\"data row19 col28\" >-0.006560</td>\n",
              "      <td id=\"T_fc7b8_row19_col29\" class=\"data row19 col29\" >0.102278</td>\n",
              "      <td id=\"T_fc7b8_row19_col30\" class=\"data row19 col30\" >-0.084182</td>\n",
              "      <td id=\"T_fc7b8_row19_col31\" class=\"data row19 col31\" >0.021167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row20\" class=\"row_heading level0 row20\" >f_20</th>\n",
              "      <td id=\"T_fc7b8_row20_col0\" class=\"data row20 col0\" >-0.000479</td>\n",
              "      <td id=\"T_fc7b8_row20_col1\" class=\"data row20 col1\" >0.001118</td>\n",
              "      <td id=\"T_fc7b8_row20_col2\" class=\"data row20 col2\" >0.000832</td>\n",
              "      <td id=\"T_fc7b8_row20_col3\" class=\"data row20 col3\" >0.000655</td>\n",
              "      <td id=\"T_fc7b8_row20_col4\" class=\"data row20 col4\" >0.000312</td>\n",
              "      <td id=\"T_fc7b8_row20_col5\" class=\"data row20 col5\" >-0.000119</td>\n",
              "      <td id=\"T_fc7b8_row20_col6\" class=\"data row20 col6\" >-0.001024</td>\n",
              "      <td id=\"T_fc7b8_row20_col7\" class=\"data row20 col7\" >-0.003155</td>\n",
              "      <td id=\"T_fc7b8_row20_col8\" class=\"data row20 col8\" >-0.005247</td>\n",
              "      <td id=\"T_fc7b8_row20_col9\" class=\"data row20 col9\" >-0.004410</td>\n",
              "      <td id=\"T_fc7b8_row20_col10\" class=\"data row20 col10\" >0.003863</td>\n",
              "      <td id=\"T_fc7b8_row20_col11\" class=\"data row20 col11\" >0.004746</td>\n",
              "      <td id=\"T_fc7b8_row20_col12\" class=\"data row20 col12\" >-0.001601</td>\n",
              "      <td id=\"T_fc7b8_row20_col13\" class=\"data row20 col13\" >0.001792</td>\n",
              "      <td id=\"T_fc7b8_row20_col14\" class=\"data row20 col14\" >0.000760</td>\n",
              "      <td id=\"T_fc7b8_row20_col15\" class=\"data row20 col15\" >-0.002196</td>\n",
              "      <td id=\"T_fc7b8_row20_col16\" class=\"data row20 col16\" >0.001713</td>\n",
              "      <td id=\"T_fc7b8_row20_col17\" class=\"data row20 col17\" >0.001341</td>\n",
              "      <td id=\"T_fc7b8_row20_col18\" class=\"data row20 col18\" >-0.001964</td>\n",
              "      <td id=\"T_fc7b8_row20_col19\" class=\"data row20 col19\" >-0.075614</td>\n",
              "      <td id=\"T_fc7b8_row20_col20\" class=\"data row20 col20\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row20_col21\" class=\"data row20 col21\" >-0.011829</td>\n",
              "      <td id=\"T_fc7b8_row20_col22\" class=\"data row20 col22\" >-0.062852</td>\n",
              "      <td id=\"T_fc7b8_row20_col23\" class=\"data row20 col23\" >-0.062563</td>\n",
              "      <td id=\"T_fc7b8_row20_col24\" class=\"data row20 col24\" >0.080303</td>\n",
              "      <td id=\"T_fc7b8_row20_col25\" class=\"data row20 col25\" >-0.067204</td>\n",
              "      <td id=\"T_fc7b8_row20_col26\" class=\"data row20 col26\" >0.103919</td>\n",
              "      <td id=\"T_fc7b8_row20_col27\" class=\"data row20 col27\" >0.000860</td>\n",
              "      <td id=\"T_fc7b8_row20_col28\" class=\"data row20 col28\" >-0.003450</td>\n",
              "      <td id=\"T_fc7b8_row20_col29\" class=\"data row20 col29\" >0.077995</td>\n",
              "      <td id=\"T_fc7b8_row20_col30\" class=\"data row20 col30\" >-0.037852</td>\n",
              "      <td id=\"T_fc7b8_row20_col31\" class=\"data row20 col31\" >-0.015024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row21\" class=\"row_heading level0 row21\" >f_21</th>\n",
              "      <td id=\"T_fc7b8_row21_col0\" class=\"data row21 col0\" >0.000649</td>\n",
              "      <td id=\"T_fc7b8_row21_col1\" class=\"data row21 col1\" >-0.001339</td>\n",
              "      <td id=\"T_fc7b8_row21_col2\" class=\"data row21 col2\" >-0.000230</td>\n",
              "      <td id=\"T_fc7b8_row21_col3\" class=\"data row21 col3\" >-0.000119</td>\n",
              "      <td id=\"T_fc7b8_row21_col4\" class=\"data row21 col4\" >0.000204</td>\n",
              "      <td id=\"T_fc7b8_row21_col5\" class=\"data row21 col5\" >0.000699</td>\n",
              "      <td id=\"T_fc7b8_row21_col6\" class=\"data row21 col6\" >-0.000362</td>\n",
              "      <td id=\"T_fc7b8_row21_col7\" class=\"data row21 col7\" >0.001340</td>\n",
              "      <td id=\"T_fc7b8_row21_col8\" class=\"data row21 col8\" >0.003877</td>\n",
              "      <td id=\"T_fc7b8_row21_col9\" class=\"data row21 col9\" >0.005610</td>\n",
              "      <td id=\"T_fc7b8_row21_col10\" class=\"data row21 col10\" >-0.004228</td>\n",
              "      <td id=\"T_fc7b8_row21_col11\" class=\"data row21 col11\" >-0.004797</td>\n",
              "      <td id=\"T_fc7b8_row21_col12\" class=\"data row21 col12\" >-0.001400</td>\n",
              "      <td id=\"T_fc7b8_row21_col13\" class=\"data row21 col13\" >-0.001776</td>\n",
              "      <td id=\"T_fc7b8_row21_col14\" class=\"data row21 col14\" >-0.001688</td>\n",
              "      <td id=\"T_fc7b8_row21_col15\" class=\"data row21 col15\" >0.002054</td>\n",
              "      <td id=\"T_fc7b8_row21_col16\" class=\"data row21 col16\" >0.000824</td>\n",
              "      <td id=\"T_fc7b8_row21_col17\" class=\"data row21 col17\" >-0.000362</td>\n",
              "      <td id=\"T_fc7b8_row21_col18\" class=\"data row21 col18\" >-0.001250</td>\n",
              "      <td id=\"T_fc7b8_row21_col19\" class=\"data row21 col19\" >0.025064</td>\n",
              "      <td id=\"T_fc7b8_row21_col20\" class=\"data row21 col20\" >-0.011829</td>\n",
              "      <td id=\"T_fc7b8_row21_col21\" class=\"data row21 col21\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row21_col22\" class=\"data row21 col22\" >-0.138952</td>\n",
              "      <td id=\"T_fc7b8_row21_col23\" class=\"data row21 col23\" >0.110653</td>\n",
              "      <td id=\"T_fc7b8_row21_col24\" class=\"data row21 col24\" >0.049758</td>\n",
              "      <td id=\"T_fc7b8_row21_col25\" class=\"data row21 col25\" >-0.076074</td>\n",
              "      <td id=\"T_fc7b8_row21_col26\" class=\"data row21 col26\" >0.129474</td>\n",
              "      <td id=\"T_fc7b8_row21_col27\" class=\"data row21 col27\" >-0.000030</td>\n",
              "      <td id=\"T_fc7b8_row21_col28\" class=\"data row21 col28\" >0.002821</td>\n",
              "      <td id=\"T_fc7b8_row21_col29\" class=\"data row21 col29\" >-0.151830</td>\n",
              "      <td id=\"T_fc7b8_row21_col30\" class=\"data row21 col30\" >0.111262</td>\n",
              "      <td id=\"T_fc7b8_row21_col31\" class=\"data row21 col31\" >-0.010326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row22\" class=\"row_heading level0 row22\" >f_22</th>\n",
              "      <td id=\"T_fc7b8_row22_col0\" class=\"data row22 col0\" >-0.001566</td>\n",
              "      <td id=\"T_fc7b8_row22_col1\" class=\"data row22 col1\" >-0.000481</td>\n",
              "      <td id=\"T_fc7b8_row22_col2\" class=\"data row22 col2\" >-0.000603</td>\n",
              "      <td id=\"T_fc7b8_row22_col3\" class=\"data row22 col3\" >0.000543</td>\n",
              "      <td id=\"T_fc7b8_row22_col4\" class=\"data row22 col4\" >0.001342</td>\n",
              "      <td id=\"T_fc7b8_row22_col5\" class=\"data row22 col5\" >0.001394</td>\n",
              "      <td id=\"T_fc7b8_row22_col6\" class=\"data row22 col6\" >-0.000284</td>\n",
              "      <td id=\"T_fc7b8_row22_col7\" class=\"data row22 col7\" >0.000006</td>\n",
              "      <td id=\"T_fc7b8_row22_col8\" class=\"data row22 col8\" >-0.000813</td>\n",
              "      <td id=\"T_fc7b8_row22_col9\" class=\"data row22 col9\" >-0.000720</td>\n",
              "      <td id=\"T_fc7b8_row22_col10\" class=\"data row22 col10\" >-0.000269</td>\n",
              "      <td id=\"T_fc7b8_row22_col11\" class=\"data row22 col11\" >0.002493</td>\n",
              "      <td id=\"T_fc7b8_row22_col12\" class=\"data row22 col12\" >-0.001321</td>\n",
              "      <td id=\"T_fc7b8_row22_col13\" class=\"data row22 col13\" >0.000536</td>\n",
              "      <td id=\"T_fc7b8_row22_col14\" class=\"data row22 col14\" >-0.000289</td>\n",
              "      <td id=\"T_fc7b8_row22_col15\" class=\"data row22 col15\" >0.000327</td>\n",
              "      <td id=\"T_fc7b8_row22_col16\" class=\"data row22 col16\" >-0.000323</td>\n",
              "      <td id=\"T_fc7b8_row22_col17\" class=\"data row22 col17\" >0.000126</td>\n",
              "      <td id=\"T_fc7b8_row22_col18\" class=\"data row22 col18\" >0.001148</td>\n",
              "      <td id=\"T_fc7b8_row22_col19\" class=\"data row22 col19\" >-0.058058</td>\n",
              "      <td id=\"T_fc7b8_row22_col20\" class=\"data row22 col20\" >-0.062852</td>\n",
              "      <td id=\"T_fc7b8_row22_col21\" class=\"data row22 col21\" >-0.138952</td>\n",
              "      <td id=\"T_fc7b8_row22_col22\" class=\"data row22 col22\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row22_col23\" class=\"data row22 col23\" >-0.096265</td>\n",
              "      <td id=\"T_fc7b8_row22_col24\" class=\"data row22 col24\" >-0.018768</td>\n",
              "      <td id=\"T_fc7b8_row22_col25\" class=\"data row22 col25\" >0.145493</td>\n",
              "      <td id=\"T_fc7b8_row22_col26\" class=\"data row22 col26\" >-0.021610</td>\n",
              "      <td id=\"T_fc7b8_row22_col27\" class=\"data row22 col27\" >-0.000017</td>\n",
              "      <td id=\"T_fc7b8_row22_col28\" class=\"data row22 col28\" >0.000880</td>\n",
              "      <td id=\"T_fc7b8_row22_col29\" class=\"data row22 col29\" >0.318088</td>\n",
              "      <td id=\"T_fc7b8_row22_col30\" class=\"data row22 col30\" >0.052677</td>\n",
              "      <td id=\"T_fc7b8_row22_col31\" class=\"data row22 col31\" >0.012876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row23\" class=\"row_heading level0 row23\" >f_23</th>\n",
              "      <td id=\"T_fc7b8_row23_col0\" class=\"data row23 col0\" >0.000958</td>\n",
              "      <td id=\"T_fc7b8_row23_col1\" class=\"data row23 col1\" >-0.000462</td>\n",
              "      <td id=\"T_fc7b8_row23_col2\" class=\"data row23 col2\" >0.000476</td>\n",
              "      <td id=\"T_fc7b8_row23_col3\" class=\"data row23 col3\" >0.001532</td>\n",
              "      <td id=\"T_fc7b8_row23_col4\" class=\"data row23 col4\" >-0.002190</td>\n",
              "      <td id=\"T_fc7b8_row23_col5\" class=\"data row23 col5\" >-0.000894</td>\n",
              "      <td id=\"T_fc7b8_row23_col6\" class=\"data row23 col6\" >0.000335</td>\n",
              "      <td id=\"T_fc7b8_row23_col7\" class=\"data row23 col7\" >0.005449</td>\n",
              "      <td id=\"T_fc7b8_row23_col8\" class=\"data row23 col8\" >0.003617</td>\n",
              "      <td id=\"T_fc7b8_row23_col9\" class=\"data row23 col9\" >0.010393</td>\n",
              "      <td id=\"T_fc7b8_row23_col10\" class=\"data row23 col10\" >-0.002578</td>\n",
              "      <td id=\"T_fc7b8_row23_col11\" class=\"data row23 col11\" >-0.000825</td>\n",
              "      <td id=\"T_fc7b8_row23_col12\" class=\"data row23 col12\" >0.003820</td>\n",
              "      <td id=\"T_fc7b8_row23_col13\" class=\"data row23 col13\" >-0.001372</td>\n",
              "      <td id=\"T_fc7b8_row23_col14\" class=\"data row23 col14\" >-0.002676</td>\n",
              "      <td id=\"T_fc7b8_row23_col15\" class=\"data row23 col15\" >0.005402</td>\n",
              "      <td id=\"T_fc7b8_row23_col16\" class=\"data row23 col16\" >-0.002612</td>\n",
              "      <td id=\"T_fc7b8_row23_col17\" class=\"data row23 col17\" >-0.001260</td>\n",
              "      <td id=\"T_fc7b8_row23_col18\" class=\"data row23 col18\" >0.002887</td>\n",
              "      <td id=\"T_fc7b8_row23_col19\" class=\"data row23 col19\" >-0.047755</td>\n",
              "      <td id=\"T_fc7b8_row23_col20\" class=\"data row23 col20\" >-0.062563</td>\n",
              "      <td id=\"T_fc7b8_row23_col21\" class=\"data row23 col21\" >0.110653</td>\n",
              "      <td id=\"T_fc7b8_row23_col22\" class=\"data row23 col22\" >-0.096265</td>\n",
              "      <td id=\"T_fc7b8_row23_col23\" class=\"data row23 col23\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row23_col24\" class=\"data row23 col24\" >0.014379</td>\n",
              "      <td id=\"T_fc7b8_row23_col25\" class=\"data row23 col25\" >-0.227175</td>\n",
              "      <td id=\"T_fc7b8_row23_col26\" class=\"data row23 col26\" >0.032537</td>\n",
              "      <td id=\"T_fc7b8_row23_col27\" class=\"data row23 col27\" >0.000345</td>\n",
              "      <td id=\"T_fc7b8_row23_col28\" class=\"data row23 col28\" >0.009890</td>\n",
              "      <td id=\"T_fc7b8_row23_col29\" class=\"data row23 col29\" >-0.173252</td>\n",
              "      <td id=\"T_fc7b8_row23_col30\" class=\"data row23 col30\" >0.084513</td>\n",
              "      <td id=\"T_fc7b8_row23_col31\" class=\"data row23 col31\" >-0.009736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row24\" class=\"row_heading level0 row24\" >f_24</th>\n",
              "      <td id=\"T_fc7b8_row24_col0\" class=\"data row24 col0\" >0.001685</td>\n",
              "      <td id=\"T_fc7b8_row24_col1\" class=\"data row24 col1\" >-0.000800</td>\n",
              "      <td id=\"T_fc7b8_row24_col2\" class=\"data row24 col2\" >0.000065</td>\n",
              "      <td id=\"T_fc7b8_row24_col3\" class=\"data row24 col3\" >0.002580</td>\n",
              "      <td id=\"T_fc7b8_row24_col4\" class=\"data row24 col4\" >0.000075</td>\n",
              "      <td id=\"T_fc7b8_row24_col5\" class=\"data row24 col5\" >-0.002708</td>\n",
              "      <td id=\"T_fc7b8_row24_col6\" class=\"data row24 col6\" >-0.002706</td>\n",
              "      <td id=\"T_fc7b8_row24_col7\" class=\"data row24 col7\" >0.003383</td>\n",
              "      <td id=\"T_fc7b8_row24_col8\" class=\"data row24 col8\" >0.004526</td>\n",
              "      <td id=\"T_fc7b8_row24_col9\" class=\"data row24 col9\" >0.010352</td>\n",
              "      <td id=\"T_fc7b8_row24_col10\" class=\"data row24 col10\" >-0.004359</td>\n",
              "      <td id=\"T_fc7b8_row24_col11\" class=\"data row24 col11\" >-0.004515</td>\n",
              "      <td id=\"T_fc7b8_row24_col12\" class=\"data row24 col12\" >0.000350</td>\n",
              "      <td id=\"T_fc7b8_row24_col13\" class=\"data row24 col13\" >-0.003143</td>\n",
              "      <td id=\"T_fc7b8_row24_col14\" class=\"data row24 col14\" >0.000821</td>\n",
              "      <td id=\"T_fc7b8_row24_col15\" class=\"data row24 col15\" >0.002660</td>\n",
              "      <td id=\"T_fc7b8_row24_col16\" class=\"data row24 col16\" >-0.003071</td>\n",
              "      <td id=\"T_fc7b8_row24_col17\" class=\"data row24 col17\" >-0.001185</td>\n",
              "      <td id=\"T_fc7b8_row24_col18\" class=\"data row24 col18\" >0.002835</td>\n",
              "      <td id=\"T_fc7b8_row24_col19\" class=\"data row24 col19\" >-0.098508</td>\n",
              "      <td id=\"T_fc7b8_row24_col20\" class=\"data row24 col20\" >0.080303</td>\n",
              "      <td id=\"T_fc7b8_row24_col21\" class=\"data row24 col21\" >0.049758</td>\n",
              "      <td id=\"T_fc7b8_row24_col22\" class=\"data row24 col22\" >-0.018768</td>\n",
              "      <td id=\"T_fc7b8_row24_col23\" class=\"data row24 col23\" >0.014379</td>\n",
              "      <td id=\"T_fc7b8_row24_col24\" class=\"data row24 col24\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row24_col25\" class=\"data row24 col25\" >-0.078116</td>\n",
              "      <td id=\"T_fc7b8_row24_col26\" class=\"data row24 col26\" >0.007149</td>\n",
              "      <td id=\"T_fc7b8_row24_col27\" class=\"data row24 col27\" >-0.001578</td>\n",
              "      <td id=\"T_fc7b8_row24_col28\" class=\"data row24 col28\" >0.008417</td>\n",
              "      <td id=\"T_fc7b8_row24_col29\" class=\"data row24 col29\" >-0.085034</td>\n",
              "      <td id=\"T_fc7b8_row24_col30\" class=\"data row24 col30\" >0.089448</td>\n",
              "      <td id=\"T_fc7b8_row24_col31\" class=\"data row24 col31\" >-0.042639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row25\" class=\"row_heading level0 row25\" >f_25</th>\n",
              "      <td id=\"T_fc7b8_row25_col0\" class=\"data row25 col0\" >0.000438</td>\n",
              "      <td id=\"T_fc7b8_row25_col1\" class=\"data row25 col1\" >-0.000408</td>\n",
              "      <td id=\"T_fc7b8_row25_col2\" class=\"data row25 col2\" >-0.000800</td>\n",
              "      <td id=\"T_fc7b8_row25_col3\" class=\"data row25 col3\" >0.002306</td>\n",
              "      <td id=\"T_fc7b8_row25_col4\" class=\"data row25 col4\" >-0.002170</td>\n",
              "      <td id=\"T_fc7b8_row25_col5\" class=\"data row25 col5\" >0.001292</td>\n",
              "      <td id=\"T_fc7b8_row25_col6\" class=\"data row25 col6\" >0.000626</td>\n",
              "      <td id=\"T_fc7b8_row25_col7\" class=\"data row25 col7\" >-0.001242</td>\n",
              "      <td id=\"T_fc7b8_row25_col8\" class=\"data row25 col8\" >-0.001997</td>\n",
              "      <td id=\"T_fc7b8_row25_col9\" class=\"data row25 col9\" >-0.005560</td>\n",
              "      <td id=\"T_fc7b8_row25_col10\" class=\"data row25 col10\" >0.001431</td>\n",
              "      <td id=\"T_fc7b8_row25_col11\" class=\"data row25 col11\" >0.002085</td>\n",
              "      <td id=\"T_fc7b8_row25_col12\" class=\"data row25 col12\" >-0.001149</td>\n",
              "      <td id=\"T_fc7b8_row25_col13\" class=\"data row25 col13\" >-0.000754</td>\n",
              "      <td id=\"T_fc7b8_row25_col14\" class=\"data row25 col14\" >-0.001142</td>\n",
              "      <td id=\"T_fc7b8_row25_col15\" class=\"data row25 col15\" >-0.001395</td>\n",
              "      <td id=\"T_fc7b8_row25_col16\" class=\"data row25 col16\" >0.000451</td>\n",
              "      <td id=\"T_fc7b8_row25_col17\" class=\"data row25 col17\" >-0.000012</td>\n",
              "      <td id=\"T_fc7b8_row25_col18\" class=\"data row25 col18\" >0.000342</td>\n",
              "      <td id=\"T_fc7b8_row25_col19\" class=\"data row25 col19\" >-0.017172</td>\n",
              "      <td id=\"T_fc7b8_row25_col20\" class=\"data row25 col20\" >-0.067204</td>\n",
              "      <td id=\"T_fc7b8_row25_col21\" class=\"data row25 col21\" >-0.076074</td>\n",
              "      <td id=\"T_fc7b8_row25_col22\" class=\"data row25 col22\" >0.145493</td>\n",
              "      <td id=\"T_fc7b8_row25_col23\" class=\"data row25 col23\" >-0.227175</td>\n",
              "      <td id=\"T_fc7b8_row25_col24\" class=\"data row25 col24\" >-0.078116</td>\n",
              "      <td id=\"T_fc7b8_row25_col25\" class=\"data row25 col25\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row25_col26\" class=\"data row25 col26\" >-0.005253</td>\n",
              "      <td id=\"T_fc7b8_row25_col27\" class=\"data row25 col27\" >0.000579</td>\n",
              "      <td id=\"T_fc7b8_row25_col28\" class=\"data row25 col28\" >-0.002416</td>\n",
              "      <td id=\"T_fc7b8_row25_col29\" class=\"data row25 col29\" >0.176906</td>\n",
              "      <td id=\"T_fc7b8_row25_col30\" class=\"data row25 col30\" >-0.030622</td>\n",
              "      <td id=\"T_fc7b8_row25_col31\" class=\"data row25 col31\" >0.025510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row26\" class=\"row_heading level0 row26\" >f_26</th>\n",
              "      <td id=\"T_fc7b8_row26_col0\" class=\"data row26 col0\" >0.000140</td>\n",
              "      <td id=\"T_fc7b8_row26_col1\" class=\"data row26 col1\" >0.001537</td>\n",
              "      <td id=\"T_fc7b8_row26_col2\" class=\"data row26 col2\" >0.000832</td>\n",
              "      <td id=\"T_fc7b8_row26_col3\" class=\"data row26 col3\" >0.000586</td>\n",
              "      <td id=\"T_fc7b8_row26_col4\" class=\"data row26 col4\" >-0.000267</td>\n",
              "      <td id=\"T_fc7b8_row26_col5\" class=\"data row26 col5\" >0.001368</td>\n",
              "      <td id=\"T_fc7b8_row26_col6\" class=\"data row26 col6\" >-0.000080</td>\n",
              "      <td id=\"T_fc7b8_row26_col7\" class=\"data row26 col7\" >-0.003957</td>\n",
              "      <td id=\"T_fc7b8_row26_col8\" class=\"data row26 col8\" >-0.004281</td>\n",
              "      <td id=\"T_fc7b8_row26_col9\" class=\"data row26 col9\" >-0.008910</td>\n",
              "      <td id=\"T_fc7b8_row26_col10\" class=\"data row26 col10\" >0.005743</td>\n",
              "      <td id=\"T_fc7b8_row26_col11\" class=\"data row26 col11\" >0.005425</td>\n",
              "      <td id=\"T_fc7b8_row26_col12\" class=\"data row26 col12\" >-0.000477</td>\n",
              "      <td id=\"T_fc7b8_row26_col13\" class=\"data row26 col13\" >0.004221</td>\n",
              "      <td id=\"T_fc7b8_row26_col14\" class=\"data row26 col14\" >0.001857</td>\n",
              "      <td id=\"T_fc7b8_row26_col15\" class=\"data row26 col15\" >-0.003119</td>\n",
              "      <td id=\"T_fc7b8_row26_col16\" class=\"data row26 col16\" >0.004094</td>\n",
              "      <td id=\"T_fc7b8_row26_col17\" class=\"data row26 col17\" >0.002307</td>\n",
              "      <td id=\"T_fc7b8_row26_col18\" class=\"data row26 col18\" >-0.002298</td>\n",
              "      <td id=\"T_fc7b8_row26_col19\" class=\"data row26 col19\" >-0.016280</td>\n",
              "      <td id=\"T_fc7b8_row26_col20\" class=\"data row26 col20\" >0.103919</td>\n",
              "      <td id=\"T_fc7b8_row26_col21\" class=\"data row26 col21\" >0.129474</td>\n",
              "      <td id=\"T_fc7b8_row26_col22\" class=\"data row26 col22\" >-0.021610</td>\n",
              "      <td id=\"T_fc7b8_row26_col23\" class=\"data row26 col23\" >0.032537</td>\n",
              "      <td id=\"T_fc7b8_row26_col24\" class=\"data row26 col24\" >0.007149</td>\n",
              "      <td id=\"T_fc7b8_row26_col25\" class=\"data row26 col25\" >-0.005253</td>\n",
              "      <td id=\"T_fc7b8_row26_col26\" class=\"data row26 col26\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row26_col27\" class=\"data row26 col27\" >0.000065</td>\n",
              "      <td id=\"T_fc7b8_row26_col28\" class=\"data row26 col28\" >-0.007037</td>\n",
              "      <td id=\"T_fc7b8_row26_col29\" class=\"data row26 col29\" >-0.025536</td>\n",
              "      <td id=\"T_fc7b8_row26_col30\" class=\"data row26 col30\" >0.036712</td>\n",
              "      <td id=\"T_fc7b8_row26_col31\" class=\"data row26 col31\" >-0.037533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row27\" class=\"row_heading level0 row27\" >f_28</th>\n",
              "      <td id=\"T_fc7b8_row27_col0\" class=\"data row27 col0\" >0.173930</td>\n",
              "      <td id=\"T_fc7b8_row27_col1\" class=\"data row27 col1\" >0.179157</td>\n",
              "      <td id=\"T_fc7b8_row27_col2\" class=\"data row27 col2\" >0.211694</td>\n",
              "      <td id=\"T_fc7b8_row27_col3\" class=\"data row27 col3\" >0.322121</td>\n",
              "      <td id=\"T_fc7b8_row27_col4\" class=\"data row27 col4\" >0.156413</td>\n",
              "      <td id=\"T_fc7b8_row27_col5\" class=\"data row27 col5\" >0.289427</td>\n",
              "      <td id=\"T_fc7b8_row27_col6\" class=\"data row27 col6\" >0.151197</td>\n",
              "      <td id=\"T_fc7b8_row27_col7\" class=\"data row27 col7\" >0.000336</td>\n",
              "      <td id=\"T_fc7b8_row27_col8\" class=\"data row27 col8\" >-0.000160</td>\n",
              "      <td id=\"T_fc7b8_row27_col9\" class=\"data row27 col9\" >0.000076</td>\n",
              "      <td id=\"T_fc7b8_row27_col10\" class=\"data row27 col10\" >-0.001150</td>\n",
              "      <td id=\"T_fc7b8_row27_col11\" class=\"data row27 col11\" >0.000491</td>\n",
              "      <td id=\"T_fc7b8_row27_col12\" class=\"data row27 col12\" >0.000580</td>\n",
              "      <td id=\"T_fc7b8_row27_col13\" class=\"data row27 col13\" >-0.001171</td>\n",
              "      <td id=\"T_fc7b8_row27_col14\" class=\"data row27 col14\" >0.001518</td>\n",
              "      <td id=\"T_fc7b8_row27_col15\" class=\"data row27 col15\" >-0.002529</td>\n",
              "      <td id=\"T_fc7b8_row27_col16\" class=\"data row27 col16\" >-0.001033</td>\n",
              "      <td id=\"T_fc7b8_row27_col17\" class=\"data row27 col17\" >0.001190</td>\n",
              "      <td id=\"T_fc7b8_row27_col18\" class=\"data row27 col18\" >-0.001695</td>\n",
              "      <td id=\"T_fc7b8_row27_col19\" class=\"data row27 col19\" >0.000565</td>\n",
              "      <td id=\"T_fc7b8_row27_col20\" class=\"data row27 col20\" >0.000860</td>\n",
              "      <td id=\"T_fc7b8_row27_col21\" class=\"data row27 col21\" >-0.000030</td>\n",
              "      <td id=\"T_fc7b8_row27_col22\" class=\"data row27 col22\" >-0.000017</td>\n",
              "      <td id=\"T_fc7b8_row27_col23\" class=\"data row27 col23\" >0.000345</td>\n",
              "      <td id=\"T_fc7b8_row27_col24\" class=\"data row27 col24\" >-0.001578</td>\n",
              "      <td id=\"T_fc7b8_row27_col25\" class=\"data row27 col25\" >0.000579</td>\n",
              "      <td id=\"T_fc7b8_row27_col26\" class=\"data row27 col26\" >0.000065</td>\n",
              "      <td id=\"T_fc7b8_row27_col27\" class=\"data row27 col27\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row27_col28\" class=\"data row27 col28\" >-0.001840</td>\n",
              "      <td id=\"T_fc7b8_row27_col29\" class=\"data row27 col29\" >0.000510</td>\n",
              "      <td id=\"T_fc7b8_row27_col30\" class=\"data row27 col30\" >0.034991</td>\n",
              "      <td id=\"T_fc7b8_row27_col31\" class=\"data row27 col31\" >0.000594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row28\" class=\"row_heading level0 row28\" >f_29</th>\n",
              "      <td id=\"T_fc7b8_row28_col0\" class=\"data row28 col0\" >0.000378</td>\n",
              "      <td id=\"T_fc7b8_row28_col1\" class=\"data row28 col1\" >-0.000144</td>\n",
              "      <td id=\"T_fc7b8_row28_col2\" class=\"data row28 col2\" >-0.001319</td>\n",
              "      <td id=\"T_fc7b8_row28_col3\" class=\"data row28 col3\" >-0.000587</td>\n",
              "      <td id=\"T_fc7b8_row28_col4\" class=\"data row28 col4\" >-0.001027</td>\n",
              "      <td id=\"T_fc7b8_row28_col5\" class=\"data row28 col5\" >-0.001408</td>\n",
              "      <td id=\"T_fc7b8_row28_col6\" class=\"data row28 col6\" >0.000902</td>\n",
              "      <td id=\"T_fc7b8_row28_col7\" class=\"data row28 col7\" >-0.034330</td>\n",
              "      <td id=\"T_fc7b8_row28_col8\" class=\"data row28 col8\" >0.077027</td>\n",
              "      <td id=\"T_fc7b8_row28_col9\" class=\"data row28 col9\" >-0.098771</td>\n",
              "      <td id=\"T_fc7b8_row28_col10\" class=\"data row28 col10\" >0.129957</td>\n",
              "      <td id=\"T_fc7b8_row28_col11\" class=\"data row28 col11\" >0.093383</td>\n",
              "      <td id=\"T_fc7b8_row28_col12\" class=\"data row28 col12\" >-0.112364</td>\n",
              "      <td id=\"T_fc7b8_row28_col13\" class=\"data row28 col13\" >0.010649</td>\n",
              "      <td id=\"T_fc7b8_row28_col14\" class=\"data row28 col14\" >-0.019842</td>\n",
              "      <td id=\"T_fc7b8_row28_col15\" class=\"data row28 col15\" >0.002563</td>\n",
              "      <td id=\"T_fc7b8_row28_col16\" class=\"data row28 col16\" >0.097340</td>\n",
              "      <td id=\"T_fc7b8_row28_col17\" class=\"data row28 col17\" >-0.075579</td>\n",
              "      <td id=\"T_fc7b8_row28_col18\" class=\"data row28 col18\" >0.160595</td>\n",
              "      <td id=\"T_fc7b8_row28_col19\" class=\"data row28 col19\" >-0.006560</td>\n",
              "      <td id=\"T_fc7b8_row28_col20\" class=\"data row28 col20\" >-0.003450</td>\n",
              "      <td id=\"T_fc7b8_row28_col21\" class=\"data row28 col21\" >0.002821</td>\n",
              "      <td id=\"T_fc7b8_row28_col22\" class=\"data row28 col22\" >0.000880</td>\n",
              "      <td id=\"T_fc7b8_row28_col23\" class=\"data row28 col23\" >0.009890</td>\n",
              "      <td id=\"T_fc7b8_row28_col24\" class=\"data row28 col24\" >0.008417</td>\n",
              "      <td id=\"T_fc7b8_row28_col25\" class=\"data row28 col25\" >-0.002416</td>\n",
              "      <td id=\"T_fc7b8_row28_col26\" class=\"data row28 col26\" >-0.007037</td>\n",
              "      <td id=\"T_fc7b8_row28_col27\" class=\"data row28 col27\" >-0.001840</td>\n",
              "      <td id=\"T_fc7b8_row28_col28\" class=\"data row28 col28\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row28_col29\" class=\"data row28 col29\" >-0.000031</td>\n",
              "      <td id=\"T_fc7b8_row28_col30\" class=\"data row28 col30\" >0.051306</td>\n",
              "      <td id=\"T_fc7b8_row28_col31\" class=\"data row28 col31\" >0.020229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row29\" class=\"row_heading level0 row29\" >f_30</th>\n",
              "      <td id=\"T_fc7b8_row29_col0\" class=\"data row29 col0\" >-0.001207</td>\n",
              "      <td id=\"T_fc7b8_row29_col1\" class=\"data row29 col1\" >0.000052</td>\n",
              "      <td id=\"T_fc7b8_row29_col2\" class=\"data row29 col2\" >-0.000089</td>\n",
              "      <td id=\"T_fc7b8_row29_col3\" class=\"data row29 col3\" >-0.000019</td>\n",
              "      <td id=\"T_fc7b8_row29_col4\" class=\"data row29 col4\" >0.000327</td>\n",
              "      <td id=\"T_fc7b8_row29_col5\" class=\"data row29 col5\" >-0.000722</td>\n",
              "      <td id=\"T_fc7b8_row29_col6\" class=\"data row29 col6\" >0.000935</td>\n",
              "      <td id=\"T_fc7b8_row29_col7\" class=\"data row29 col7\" >-0.000538</td>\n",
              "      <td id=\"T_fc7b8_row29_col8\" class=\"data row29 col8\" >0.000741</td>\n",
              "      <td id=\"T_fc7b8_row29_col9\" class=\"data row29 col9\" >-0.001799</td>\n",
              "      <td id=\"T_fc7b8_row29_col10\" class=\"data row29 col10\" >-0.002315</td>\n",
              "      <td id=\"T_fc7b8_row29_col11\" class=\"data row29 col11\" >-0.002664</td>\n",
              "      <td id=\"T_fc7b8_row29_col12\" class=\"data row29 col12\" >-0.002644</td>\n",
              "      <td id=\"T_fc7b8_row29_col13\" class=\"data row29 col13\" >-0.001137</td>\n",
              "      <td id=\"T_fc7b8_row29_col14\" class=\"data row29 col14\" >-0.001154</td>\n",
              "      <td id=\"T_fc7b8_row29_col15\" class=\"data row29 col15\" >0.001558</td>\n",
              "      <td id=\"T_fc7b8_row29_col16\" class=\"data row29 col16\" >-0.000131</td>\n",
              "      <td id=\"T_fc7b8_row29_col17\" class=\"data row29 col17\" >0.001542</td>\n",
              "      <td id=\"T_fc7b8_row29_col18\" class=\"data row29 col18\" >-0.000467</td>\n",
              "      <td id=\"T_fc7b8_row29_col19\" class=\"data row29 col19\" >0.102278</td>\n",
              "      <td id=\"T_fc7b8_row29_col20\" class=\"data row29 col20\" >0.077995</td>\n",
              "      <td id=\"T_fc7b8_row29_col21\" class=\"data row29 col21\" >-0.151830</td>\n",
              "      <td id=\"T_fc7b8_row29_col22\" class=\"data row29 col22\" >0.318088</td>\n",
              "      <td id=\"T_fc7b8_row29_col23\" class=\"data row29 col23\" >-0.173252</td>\n",
              "      <td id=\"T_fc7b8_row29_col24\" class=\"data row29 col24\" >-0.085034</td>\n",
              "      <td id=\"T_fc7b8_row29_col25\" class=\"data row29 col25\" >0.176906</td>\n",
              "      <td id=\"T_fc7b8_row29_col26\" class=\"data row29 col26\" >-0.025536</td>\n",
              "      <td id=\"T_fc7b8_row29_col27\" class=\"data row29 col27\" >0.000510</td>\n",
              "      <td id=\"T_fc7b8_row29_col28\" class=\"data row29 col28\" >-0.000031</td>\n",
              "      <td id=\"T_fc7b8_row29_col29\" class=\"data row29 col29\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row29_col30\" class=\"data row29 col30\" >0.020730</td>\n",
              "      <td id=\"T_fc7b8_row29_col31\" class=\"data row29 col31\" >0.015434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row30\" class=\"row_heading level0 row30\" >target</th>\n",
              "      <td id=\"T_fc7b8_row30_col0\" class=\"data row30 col0\" >0.053039</td>\n",
              "      <td id=\"T_fc7b8_row30_col1\" class=\"data row30 col1\" >0.052751</td>\n",
              "      <td id=\"T_fc7b8_row30_col2\" class=\"data row30 col2\" >0.029921</td>\n",
              "      <td id=\"T_fc7b8_row30_col3\" class=\"data row30 col3\" >-0.000038</td>\n",
              "      <td id=\"T_fc7b8_row30_col4\" class=\"data row30 col4\" >0.000058</td>\n",
              "      <td id=\"T_fc7b8_row30_col5\" class=\"data row30 col5\" >0.031144</td>\n",
              "      <td id=\"T_fc7b8_row30_col6\" class=\"data row30 col6\" >-0.000773</td>\n",
              "      <td id=\"T_fc7b8_row30_col7\" class=\"data row30 col7\" >0.009698</td>\n",
              "      <td id=\"T_fc7b8_row30_col8\" class=\"data row30 col8\" >0.050638</td>\n",
              "      <td id=\"T_fc7b8_row30_col9\" class=\"data row30 col9\" >0.064265</td>\n",
              "      <td id=\"T_fc7b8_row30_col10\" class=\"data row30 col10\" >-0.040612</td>\n",
              "      <td id=\"T_fc7b8_row30_col11\" class=\"data row30 col11\" >-0.074922</td>\n",
              "      <td id=\"T_fc7b8_row30_col12\" class=\"data row30 col12\" >-0.004650</td>\n",
              "      <td id=\"T_fc7b8_row30_col13\" class=\"data row30 col13\" >-0.033999</td>\n",
              "      <td id=\"T_fc7b8_row30_col14\" class=\"data row30 col14\" >-0.007883</td>\n",
              "      <td id=\"T_fc7b8_row30_col15\" class=\"data row30 col15\" >0.029299</td>\n",
              "      <td id=\"T_fc7b8_row30_col16\" class=\"data row30 col16\" >-0.032506</td>\n",
              "      <td id=\"T_fc7b8_row30_col17\" class=\"data row30 col17\" >0.006195</td>\n",
              "      <td id=\"T_fc7b8_row30_col18\" class=\"data row30 col18\" >-0.012037</td>\n",
              "      <td id=\"T_fc7b8_row30_col19\" class=\"data row30 col19\" >-0.084182</td>\n",
              "      <td id=\"T_fc7b8_row30_col20\" class=\"data row30 col20\" >-0.037852</td>\n",
              "      <td id=\"T_fc7b8_row30_col21\" class=\"data row30 col21\" >0.111262</td>\n",
              "      <td id=\"T_fc7b8_row30_col22\" class=\"data row30 col22\" >0.052677</td>\n",
              "      <td id=\"T_fc7b8_row30_col23\" class=\"data row30 col23\" >0.084513</td>\n",
              "      <td id=\"T_fc7b8_row30_col24\" class=\"data row30 col24\" >0.089448</td>\n",
              "      <td id=\"T_fc7b8_row30_col25\" class=\"data row30 col25\" >-0.030622</td>\n",
              "      <td id=\"T_fc7b8_row30_col26\" class=\"data row30 col26\" >0.036712</td>\n",
              "      <td id=\"T_fc7b8_row30_col27\" class=\"data row30 col27\" >0.034991</td>\n",
              "      <td id=\"T_fc7b8_row30_col28\" class=\"data row30 col28\" >0.051306</td>\n",
              "      <td id=\"T_fc7b8_row30_col29\" class=\"data row30 col29\" >0.020730</td>\n",
              "      <td id=\"T_fc7b8_row30_col30\" class=\"data row30 col30\" >1.000000</td>\n",
              "      <td id=\"T_fc7b8_row30_col31\" class=\"data row30 col31\" >0.108902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc7b8_level0_row31\" class=\"row_heading level0 row31\" >unique_length</th>\n",
              "      <td id=\"T_fc7b8_row31_col0\" class=\"data row31 col0\" >-0.000236</td>\n",
              "      <td id=\"T_fc7b8_row31_col1\" class=\"data row31 col1\" >-0.001385</td>\n",
              "      <td id=\"T_fc7b8_row31_col2\" class=\"data row31 col2\" >-0.000078</td>\n",
              "      <td id=\"T_fc7b8_row31_col3\" class=\"data row31 col3\" >-0.001494</td>\n",
              "      <td id=\"T_fc7b8_row31_col4\" class=\"data row31 col4\" >-0.000354</td>\n",
              "      <td id=\"T_fc7b8_row31_col5\" class=\"data row31 col5\" >0.000392</td>\n",
              "      <td id=\"T_fc7b8_row31_col6\" class=\"data row31 col6\" >0.000038</td>\n",
              "      <td id=\"T_fc7b8_row31_col7\" class=\"data row31 col7\" >-0.068810</td>\n",
              "      <td id=\"T_fc7b8_row31_col8\" class=\"data row31 col8\" >-0.058358</td>\n",
              "      <td id=\"T_fc7b8_row31_col9\" class=\"data row31 col9\" >0.017631</td>\n",
              "      <td id=\"T_fc7b8_row31_col10\" class=\"data row31 col10\" >0.017759</td>\n",
              "      <td id=\"T_fc7b8_row31_col11\" class=\"data row31 col11\" >0.017883</td>\n",
              "      <td id=\"T_fc7b8_row31_col12\" class=\"data row31 col12\" >-0.032927</td>\n",
              "      <td id=\"T_fc7b8_row31_col13\" class=\"data row31 col13\" >-0.027220</td>\n",
              "      <td id=\"T_fc7b8_row31_col14\" class=\"data row31 col14\" >-0.011617</td>\n",
              "      <td id=\"T_fc7b8_row31_col15\" class=\"data row31 col15\" >0.003933</td>\n",
              "      <td id=\"T_fc7b8_row31_col16\" class=\"data row31 col16\" >0.007388</td>\n",
              "      <td id=\"T_fc7b8_row31_col17\" class=\"data row31 col17\" >0.031436</td>\n",
              "      <td id=\"T_fc7b8_row31_col18\" class=\"data row31 col18\" >-0.030131</td>\n",
              "      <td id=\"T_fc7b8_row31_col19\" class=\"data row31 col19\" >0.021167</td>\n",
              "      <td id=\"T_fc7b8_row31_col20\" class=\"data row31 col20\" >-0.015024</td>\n",
              "      <td id=\"T_fc7b8_row31_col21\" class=\"data row31 col21\" >-0.010326</td>\n",
              "      <td id=\"T_fc7b8_row31_col22\" class=\"data row31 col22\" >0.012876</td>\n",
              "      <td id=\"T_fc7b8_row31_col23\" class=\"data row31 col23\" >-0.009736</td>\n",
              "      <td id=\"T_fc7b8_row31_col24\" class=\"data row31 col24\" >-0.042639</td>\n",
              "      <td id=\"T_fc7b8_row31_col25\" class=\"data row31 col25\" >0.025510</td>\n",
              "      <td id=\"T_fc7b8_row31_col26\" class=\"data row31 col26\" >-0.037533</td>\n",
              "      <td id=\"T_fc7b8_row31_col27\" class=\"data row31 col27\" >0.000594</td>\n",
              "      <td id=\"T_fc7b8_row31_col28\" class=\"data row31 col28\" >0.020229</td>\n",
              "      <td id=\"T_fc7b8_row31_col29\" class=\"data row31 col29\" >0.015434</td>\n",
              "      <td id=\"T_fc7b8_row31_col30\" class=\"data row31 col30\" >0.108902</td>\n",
              "      <td id=\"T_fc7b8_row31_col31\" class=\"data row31 col31\" >1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(data=train, x=\"f_01\", hue=\"target\", kde=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "4t6igESVryBO",
        "outputId": "a6cab2b6-51ec-45bf-f80b-b6a78033754c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f98d67c4890>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEHCAYAAABfkmooAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3hdV53/+/f3NPVuSVYs2XKLaxzHdhKnUJKQQoAE7gA3YWYSBobMQJg7DFxmAsxc+kz4Db+hDD9gEggkkJBKiNNxYjuOe+9VtmxLsmT1duou6/5xjm25SrYlbZXv63nO43PWOXufz/FzpK/22mutLcYYlFJKqfPxeR1AKaXU0KfFQimlVK+0WCillOqVFgullFK90mKhlFKqVwGvAwyEMWPGmMrKSq9jKKXUsLJx48ZmY0zx2Z4bkcWisrKSDRs2eB1DKaWGFRE5fK7ntBtKKaVUr7RYKKWU6pUWC6WUUr0akecslFLKK5ZlUVtbSywW8zrKOaWnp1NeXk4wGOzzNloslFKqH9XW1pKTk0NlZSUi4nWcMxhjaGlpoba2lokTJ/Z5O+2GUkqpfhSLxSgqKhqShQJARCgqKrrgIx8tFkop1c+GaqE47mLyabFQSinVKy0WSg0QYwzhcBi9ZowCaG9v5+c///mAv8+f/vQndu3a1e/71WKh1ACJRCL8xwtriEQiXkdRQ8CFFgtjDK7rXvD7aLFQahgKpWd6HUENEQ899BAHDhxg7ty5/NM//RO33HIL8+bN44orruCll14C4NChQ0ybNo377ruP2bNnU1NTw3e/+12mTZvGjTfeyL333ssPf/hDAA4cOMAdd9zB/Pnzec973sOePXtYtWoVixYt4qtf/Spz587lwIED/ZZfh84qpdQgePjhh9mxYwdbtmzBtm0ikQi5ubk0NzezcOFC7rrrLgD279/P448/zsKFC1m/fj0vvPACW7duxbIs5s2bx/z58wF44IEH+OUvf8nUqVNZu3YtX/jCF1iyZAl33XUXH/7wh/n4xz/er/m1WCil1CAzxvD1r3+d5cuX4/P5qKur49ixYwBMmDCBhQsXArBy5Uruvvtu0tPTSU9P5yMf+QgA3d3drFq1ik984hMn9hmPxwc0sxYLpQaBMYZIJEJGRgbRaJTMzMwhP7xSDZwnn3ySpqYmNm7cSDAYpLKy8sS8h6ysrF63d12X/Px8tmzZMtBRT9BzFkr1s3A4TDgcBsB2DYdaImypbuTfn19Nc3OznvQepXJycujq6gKgo6ODkpISgsEgS5cu5fDhs68MfsMNN/Dyyy8Ti8Xo7u7mlVdeASA3N5eJEyfy3HPPAck/RrZu3XrG+/QnPbJQqh+dGC7rD/HbNbU8vb2dJ7asAyAvzcfkbUcJpWd4nFJ5oaioiBtuuIHZs2dz9dVXs2fPHq644goWLFjA9OnTz7rN1VdfzV133cWcOXMoLS3liiuuIC8vD0genXz+85/ne9/7HpZlcc8993DllVdyzz338LnPfY6f/vSnPP/880yePLlf8stIHAO+YMECoxc/Ul4Ih8N8+TdL2NWdTk17ggl5Qf7munJe2XiQA51CWxzml4V44m+vJzMzk0gkol1SI8zu3buZMWNGv+2vu7ub7OxsIpEI733ve3nkkUeYN2/eJe/3bDlFZKMxZsHZXq/dUEr1o211nSw56qOxM87PPz6Nmydl874JGUwvzuADFT4qsoWN9Qme33xU52GoPnnggQeYO3cu8+bN4y/+4i/6pVBcDO2GUqofGGNYf+AYn3tyKyEf3DwhnQXjc1m16wj//cZWQmnp+H3CdWVg1wvff30/00uzdR6G6tVTTz3ldQRggI8sROSQiGwXkS0isiHVVigii0Vkf+rfglS7iMhPRaRKRLaJyLwe+7k/9fr9InL/QGZW6kIYY+ju7mbHwVru/+1GctL83DQOMgOkTnIbQmnpJ17vE2FhKQTE8JXnd2K7I68bWI1Mg9ENdZMxZm6PfrCHgLeNMVOBt1OPAT4ITE3dHgB+AcniAnwTuBa4Bvjm8QKjlNcikQjfemoZ9z2+BcuF/333VDIDYCVi/PcbW7Ft64xt0vzCtWN91HbE2Xw0OvihlboIXpyzuBt4PHX/ceCjPdqfMElrgHwRKQNuBxYbY1qNMW3AYuCOwQ6t1Lns7wrSlvBxQ3k6U4pPdiv1PKI4XWmm8KHp+exojLG5ukkXG1RD3kAXCwP8WUQ2isgDqbZSY0x96n4DUJq6Pw6o6bFtbartXO2nEJEHRGSDiGxoamrqz8+g1Dntb+xme5PN+BxhQv6FnQL8+4WlBP3w1T/u1JPcasgb6GJxozFmHskupgdF5L09nzTJP6f65U8qY8wjxpgFxpgFxcXF/bFLpXr1n28dIOCHecUXNvTVGIPPiTOrKEB9BN7d26BHFyNUxfgJiEi/3SrGT+j1Pd944w2mTZvGlClTePjhh/vlcwzoaChjTF3q30YReZHkOYdjIlJmjKlPdTM1pl5eB1T02Lw81VYHvP+09mUDmVup3hhjWLarjpUH2phXGiA9YE5MyOv5948xYNk2wYAfy3YIBgKIgJWI8z9v72ZybhZ7WuBfF+3hxstLyc7O9u5DqQFRW3OE//rz3n7b35dvm3be5x3H4cEHH2Tx4sWUl5efmNg3c+bMS3rfATuyEJEsEck5fh+4DdgBLAKOj2i6H3gpdX8RcF9qVNRCoCPVXfUmcJuIFKRObN+WalPKE8YYGhsb+epzW0j3GyblJK85YMWPn9S2T7zWsm1i1euJxWJEq9cTjUZJWBbGQCAtnYBPmFUkNMdgeVWrVx9JjSDr1q1jypQpTJo0iVAoxD333HNiCfRLMZDdUKXAChHZCqwDXjXGvAE8DNwqIvuBD6QeA7wGHASqgEeBLwAYY1qB7wLrU7fvpNqU8kQkEuGhZ9bTHBNmFfkI+E52QYXS0jEGHDtZEACCfj8hE6NCGkk7tITwwXXEY1Gc1EipSXlCTkj4ydKDuDqUVl2iuro6KipOdtKUl5dTV1d3yfsdsG4oY8xB4MqztLcAt5yl3QAPnmNfjwGP9XdGpS7Wvk4/aX6XiblnnquwbZtYzRb8UxaS6XZxh28x1zf/F740A2lQ647hdetj7AhdjzFg2w5XlqazoibMK9uOcsvUfF0CRA05OoNbqQt0oClMXbfLrEI55aiip6DfT7lVzWdbf0i2r4NliVnsN+UU+rq5wb+bz0UfZbe9mt+Zv6Oz7gDjpr2HqcVZ/GjxXtZsdfnGx6/r01LVSp1u3Lhx1NScHEBaW1vLuHFnDCC9YLo2lFIX6HfravELTM0/tVAYAwnLBgwzfdV8sfk7OBLgB4l7eca5iU3uVN6yr+I/7L/k6bR7mGLt4ctNX6fM14qI8OD7KqluiVIT0b/h1MW7+uqr2b9/P9XV1SQSCZ5++ukTV+G7FPqtVOoCtEcSvLztGJV5ftJP++k5fjL7ioI4nwu+TJ1bxM9zH6IzXAM4J15n8PG270b2dFl8Ofs1vhh8kf+xrmdhxWyml2azpT5CR2eXdkWNEOUV43sdwXSh+zufQCDAz372M26//XYcx+Ezn/kMs2bNuuT31SMLpS7AM+triNku0wvP/nfWlEADfxt9lBp3DD+Of5TmuoO4xjnra2sZy0/id5NOnM80fIf/s2gVfz23gK6Eyz8+vUkn6o0QNUcOY4zpt1vNkbNfKKmnO++8k3379nHgwAG+8Y1v9Mvn0GKhVB85ruGJ1Ye5siyTbL99xvN5Tguf9S+i1cngx5GPECGdQODsP2KWkxwJVWuK+ZX9EYqdej7W8j+s33mAwnTY2WJo7+z/q50pdbG0WCjVR2/taqCuPUpaog3LPrVYGNdwT9svScPivyMfJMy514WyHIdE3U4cJzlMdp+p4OX0jzIvtpobYm8zu9BHxIaXdjQP6OdR6kJosVCqF8dnZv/63YNkBgyXZZ35Y7Og802mWbt43rqRGivvRCE4l6D/5LkIy3F4uamM7c4EPtr1JFc4uylKh9+uPUrMOnsXllKDTYuFUr2IRCL885OrWHe4nWmFQU4fLZvjtPGRrj+w2xnPu87FnUj0B/z8JnErnSaLv2n7EXNyozR2W/xh3ZF++ARKXTotFkr1QVWnkOYXppxlZdk72p8iYGz+YN8MXPzopTAZ/Mq6nULp4u/iv2FuWSY/X3aAuK1HF8p7WiyU6kU4YXOgNc5Nk3NJO1ErDAnLorR7N/PDy3kn64M0m/xLfq+Dbhlv2PNZGF/BzOgGmrrivLy1vvcNlRpgWiyUOg9jDH/aVIvtQrSj5cSV7xzbpqtqLbc3/ooOk8krgdv7Z6194GX7Gg47xXwt/hMmFQR5dHkVruv2097VYKscX96vS5RXji/v9T0/85nPUFJSwuzZs/vtc+ikPKXOIxKJ8Mvl1eSGhLG5oVOemx5oYGagluesG+ms3X3OYbIXysXPY7Fb+EbWC3wq8iTfa/sky3Yf5eZZvf+SUEPP4Zo6zJJ/77f9yc1f7/U1n/70p/niF7/Ifffd12/vq0cWSp3H3mPdtMaFyXlyxmzqOwNr6TQZvGPP7rdCcdyheA7PRq7mr3iVDJ/NsxuP9uv+1cj23ve+l8LCwn7dpxYLpc7j+c31+AQqT1tdttQ+ygz/Ed6yr8IiOCDvvTQxhxaTx0d9K3h7TxPNXbEBeR+l+kKLhVLnEE3YvLytgfE5PtL8pxaLa2IrcIywyp4xYO/v4uM55/3c73sdy4UnV+4bsPdSqjdaLJQ6hz9tPExX3GFi7mlPuC4LYqvY6U6ki8wBzbDbLiPhCnPkAC9ubdAT3cozWiyUOodnNh4lJySUZJzaPjG8iXzTziq7/1YSPZ/Fznw+6V/GoTaLzdWNvW+g1ADQ0VBKncXmI21sqe1kwdggIqf+Nb8wsZpuk8Z2d+KgZNnnjuOmwFZ8uPxpwyHmTSrVpcuHkQkV4/o0gulC9tebe++9l2XLltHc3Ex5eTnf/va3+exnP3tJ76vFQqmzeGzlIXLS/EzO90OPrp90N8xcewvLrRnY+AcpjbDZmcJC3y5e2TaFf76zm5ycnEF6b3WpDh2pHfT3/MMf/tDv+9RuKKVOU98R5bXt9dw1ewzB035C5oRXEcRmlTV9UDNtdiZxm28DbU4624+0DOp7KwVaLJQ6wxOrkxeraW9qODFj+7j54eXU+S7jsFs8qJksgpSaZny4vLmjDmP6a764Un2jxUKpHpraOnlyzWHeP6WAguxTz2wXJY4yIbGPlb5ruJQFAy/WHnc882Q/i7fX6lX0hrihXswvJp8WC6U4ec2KRVvr6YzZEG4946jiqq6lOEZYdiyz1+tVDIS97jiu8+3iqJ1NU1d80N9f9U16ejotLS1DtmAYY2hpaSE9/dwX6DobPcGtFMk1oP79+dWsaID8kEtx1qk/GmJcFkTfZZdbSViyPPkry+CjVNoAWLGvgcqx/bucg+of5eXl1NbW0tTU5HWUc0pPT6e8/MLWGtNioVRKmx3icFsX15T6zxiaOjm2g3y3jeec6zxKl9Ti5lJKK8u2JPjL98zQIbRDUDAYZOLEwRlWPZi0G0qplN3NcbJDPsbnnPkLeH54GRHJYrs7yYNkJ+1xxnKjbzsrjxraO7s8zaJGFy0WatQzxlDT1M7h9gS3TMrCf9pf62lOhNmRdWxMuxbL44NxFz8VHCNKGmv26Uq0avBosVCjXiQS4WvPb8Y10N7UgGXbpzw/q3sFQSyWtRTiGu8vceq6EMRmzfa9XkdRo4gWC6WAoxE/uSEoyg6d8dyCyLvUu4XU+sZ6kOxMB0wZC3x7WXFEh8+qwaPFQo16LeEEjRGXiuwzz1UUWfVMsvaxxpmJF3MrzsYmwDRfHQdiueyr0YUF1eAY8GIhIn4R2Swir6QeTxSRtSJSJSLPiEgo1Z6WelyVer6yxz6+lmrfKyK3D3RmNbq8vbcZA5Sf5cT2vPA7uAhrncFd3qM32ZIAYOmadR4nUaPFYBxZ/COwu8fjHwA/MsZMAdqA40shfhZoS7X/KPU6RGQmcA8wC7gD+LmIDNYKbmoUeGtPEzkhIf+0HigxLvO6l7PTrqDNZJx9Y48ctfMpl0ZW7dcjCzU4BrRYiEg58CHgV6nHAtwMPJ96yePAR1P37049JvX8LanX3w08bYyJG2OqgSrgmoHMrUaPaMJh/aF2xmX7TpmzYAyMD2+jwG1hnTvLw4RnF3b9zKaaDZ15JCzvT7qrkW+gjyx+DPwzcHyN5yKg3RhzfLhJLXB8cfZxQA1A6vmO1OtPtJ9lmxNE5AER2SAiG4byzEk1tKw71ErCMVyWferBqmXbzGt6kQgZbHEne5Tu/PLoIkw6Gzev9zqKGgUGrFiIyIeBRmPMxoF6j56MMY8YYxYYYxYUFw/uiqBq+Fq+r4mQXyjJPG1uhRvhKl8VaxOTSJihcWL7dBE3SACbZeu3Dtl1iNTIMZBHFjcAd4nIIeBpkt1PPwHyReT4zKZyoC51vw6oAEg9nwe09Gw/yzZKXTRjDMt2N1AQcsHtObfCMCe8ipDYrHVnepavN3E3yBXBo7xTB5Fw2Os4aoQbsGJhjPmaMabcGFNJ8gT1EmPMXwJLgY+nXnY/8FLq/qLUY1LPLzHJP5cWAfekRktNBKYCOgREXbJDx9o40BJlbNapPwaObTO/400a3HyqTalH6XoXj8fJjx1lj1tOa80er+OoEc6LeRb/AnxZRKpInpP4dar910BRqv3LwEMAxpidwLPALuAN4EFjhsA0WjXsrTqYXMF1bNap3UxFTiOX+4+yypnBUJlbcS7H063ZsNbTHGrkG5SFbowxy4BlqfsHOctoJmNMDPjEObb/PvD9gUuoRqNVB1tJ93PGkNl5seQv3nXONA9SXRjXssn1h1lZ3cmd4TCZmZm6Eq0aEDqDW406xhi6urtZfbCNsmz/GUNmr4yt44BbRpvJ8TBl3wgwxVfPish4Wp77sl5BTw0YLRZq1IlEIvzLk6tpjViUnXa+Ii9cTYVbw0Z7ikfpLly2z6aZfBo7tVCogaPFQo1KjfHkvIqy0+ZXzLeTI703OsOnWGT4kiO51rZleZxEjWRaLNSoVNdlMbU4g4zAqf37c63N7LfLaCfbo2QXId7FWJpZHZ+AdB/zOo0aobRYqFEnbjs0dtvMLcsATk5my7ebqHBr2WwPv0tiFvoibHIvx937htdR1AilxUKNOjuOduEYqKlvxLatE+3To5sA2GpXepTs4qWLRYR09u7Y4HUUNUJpsVCjzvrD7QCU5Z46ZnZmdAMNUsIxU+BFrEtSQPJ63OuOWhBp9TiNGom0WKhRZ+ORDvLThDT/yfMVISfCpNhONobH4jjDb52lgB0lny7Wu5cT2/YnXStK9TstFmpUsRyXzTUdlGae+tWfGNlKAIet9niPkl26XCKsc2cQXfYjnW+h+p0WCzVqGGNYX9VA1HIpOW1+xeXx7cRMkCp76K4F1ZtC6aabDGqiAUjowoKqf2mxUKNGJBLh4ZeSJ7GLQqcuLzYtvoN9bgXOMP6RyHOT52LWO5fjO/i2x2nUSDN8fzKUuggtCT+5IUjvMb+i0D7GGOcYu93h2wUFkE6CDBNlpTsb2f2y13HUCKPFQo0ajmtojLgUZ5y6FtTE7s0Aw75YQHJU1Dp3BoHD74Jj9b6BUn2kxUKNGvuOdWO5UJxxss2ybSa1LqfFzabezfUuXD8ppJNuMjgQz8UcWeN1HDWCaLFQo8bKqkYASnocWfiMwwx/Dbvc8Qz1a1f0RQGdAKx2ZxLZ+qIOoVX9RouFGjU213aRFYTM4MmiUGEdJFMSI6ILCiCDOOkkWCVzCex8XofQqn6jxUKNCq5r2FzbfcpRBcC0+HZcA3ucco+S9S8BiqSLDc5UQokOpO2Q15HUCKHFQo0KVU3ddMTsU85XAFwe30GNKSVMxtk3HIYKfd20ulkcNGU4e17TrijVL7RYqFFh7cEWAEoyTx5ZpLkRJlj7R0wX1HGFqfMWK2Uevo2PaVeU6hdaLNSosKa6lZLsIFk9rjo/ObYTP+6IKxZpdjchk2AVc8iMNkCi2+tIagTQYqFGPGMM66pbmTM2k54jnqbEthEnjQPuZd6FGwACFPm6WZ+YCMbFf+hdryOpEUCLhRrxqpvDNHXFaW5uwrKTlyA1BqZGtrDHLsMagV36+W47LSabw5ThP/Bnr+OoEUCLhRrx1lYnr+8wNufk9StyY3UUu42p+RUjz/H5Fsv9C/EfeidZHZW6BFos1Ii3qqqZMVlBcoInf2FOi28HGLHFIpsoQWzWmFn4uhugcZfXkdQwp8VCjWiua1hZ1UyWJHAc+0T7tPh2mt1cGk2+h+kGjgCFdLHZShXDKl2FVl0aLRZqRNt7rIvWiMVlPS6h6jM2UxM72O1OYCQs8XEueU4b9XY2NXnzMQe0WKhLo8VCjWgrq5oBGJvlP9E2Pr6fdBNLFYuRK99NnbeITobDqyGh8y3UxdNioUa0FVXNTCzKIKvHelBTY9tw8LHHHRlLfJxLDmEyxGKDzEScOBxe6XUkNYz1qViIyA19aVNqKInbDuuqW5lfngOcPLk9NbqFaqeUsAl6F24wGMMUfyPro+MwgXQ9b6EuSV+PLP67j21KDRnrq1uJJBwajh3DtpMXAsp0Oym3qkd8FxSAbVtEY3FqE1kcHHMT5sASryOpYey8xUJErhORrwDFIvLlHrdvAf5etk0XkXUislVEdorIt1PtE0VkrYhUicgzIhJKtaelHlelnq/ssa+vpdr3isjtl/iZ1SixeOdRfBjGZp/8qk6JbMOHYadT4WGywVPiSy718WZLCdK8FzpqPU6khqvejixCQDYQAHJ63DqBj/eybRy42RhzJTAXuENEFgI/AH5kjJkCtAGfTb3+s0Bbqv1HqdchIjOBe4BZwB3Az0XkvIVKKYB397dSmuUn4EuerzAGLo9vJ2LSOGRKPE43ODJNlHxflOe75yQbtCtKXaTA+Z40xrwDvCMivzXGHL6QHZvkusjHVzALpm4GuBn4VKr9ceBbwC+Au1P3AZ4HfiYikmp/2hgTB6pFpAq4Blh9IXnU6HKkJczBlgjzSwMcP19hWxbTY5vZbSowo2Rsh2NZpNNBjb+YeEYp/r1v4p93H8kfLaX6rq8/MWki8oiI/FlElhy/9baRiPhFZAvQCCwGDgDtxpjjs6NqgXGp++OAGoDU8x1AUc/2s2zT870eEJENIrKhqampjx9LjUTGGF7ZWA1AaYZ7on2sW0+hL8xOZ2TO2j6XYl8XFgFWJaYiVYuJdHd6HUkNQ+c9sujhOeCXwK8Ap687N8Y4wFwRyQdeBKZfcMK+v9cjwCMACxYs0IVwRrFIJMLv1hwiOyjk9BjwNC22FRi5S3ycSxHtAKz0zeMmawW++i2Q8z6PU6nhpq/FwjbG/OJi38QY0y4iS4HrgHwRCaSOHsqButTL6oAKoFZEAkAe0NKj/bie2yh1hrjt0BgVJuXJKd0tM+xd1DsFtJkcD9MNvjRscomw1pqMAfyHlsHlWizUhelrN9TLIvIFESkTkcLjt/NtICLFqSMKRCQDuBXYDSzl5Mnx+4GXUvcXpR6Ten5J6rzHIuCe1GipicBUYF0fc6tRaP3hDhwDZVknC4XfSTDV2c+OUTIK6nRjfJ3sjhXRkTYuuQqtUheor0cWx3+Jf7VHmwEmnWebMuDx1MglH/CsMeYVEdkFPC0i3wM2A79Ovf7XwO9SJ7BbSY6AwhizU0SeBXYBNvBgqntLqbN6t6oFv0BJj8tqV0a2EcJia2J8L4O+R6Zi6eQgY1kZWMid9S9CtA0yCryOpYaRPhULY8zEC92xMWYbcNVZ2g+SHM10ensM+MQ59vV94PsXmkGNPsYY3tnXTGmW78SQWYAZ8S3ETYC99mX4RmGxKKCLIA7vOrP4kHkBc/AdZNZHvY6lhpE+FQsRue9s7caYJ/o3jlIXzxjD1qpaatrjzCsWTqwoawwz4lvYaY3DJkDovHsZmYydIJcuVoUvwwmGcPe+SVCLhboAfe2GurrH/XTgFmAToMVCDRmRSITvLdoKCKWZJ48qiu2jFDlNvGy9x7twQ0CJv5vdVjlHcqYz/uBSjOsivtEx30Rduj59U4wx/9Dj9jlgHsmZ3UoNKS2JAGl+6HH5CqZHNwGwzRqdJ7ePK5IOAJa48/F31xOt3eZxIjWcXOyfFWHggs9jKDWQjDE0hB1KM08dMjstuoX6QDmto2zI7Oky7U5CJsEKayoA/upl3gZSw0pflyh/WUQWpW6vAntJTrJTasg41BIlap86CirNjVAZ383utLneBRsiBBjj62JzbCzxYD4ceJvk6HSletfXcxY/7HHfBg4bY3T5SjWkrDvcBnDK+YopsR0EcNgRuILkiOzRrdjXxVaniM2BuVxT8y6Rzlay8oq8jqWGgb6es3gH2ENyxdkCIDGQoZS6GGur28kMCNk9lviYHt1EVDLZdbQDY9xzbzxKjCF53mK5mYvPOPhqdX6r6pu+dkN9kuSs6U8AnwTWikhvS5QrNWhc17DuUBslGZw8X2EM06Jb2BOahfhH44DZMwXsCNkmwsrYRIz4kkt/KNUHfe2G+gZwtTGmEZJLeQBvkVxKXCnP7T3WSVvUZmrpyS6osYnD5LptbOnMw9VJ/ycU+zrZGSumrWA8+dW69Ifqm76OhvIdLxQpLRewrVID7p3dDQAUBpNdTcbAlK5kF8vOUXAJ1QtR6LZi42e5LMDXvBs6672OpIaBvv7Cf0NE3hSRT4vIp4FXgdcGLpZSF2bdoTayApCZOl9h2TbTulZxyC2hi0xvww0xBXTiw2FpYmayQa/Nrfqgt2twTxGRG4wxXwX+B5iTuq0mde0IpbzmuIb1h9tPGTKb6XYx2dfADkePKk7nx1BoOnmrewJ2ehH23je9jqSGgd6OLH5M8nrbGGP+aIz5sjHmyyTnWPx4oMMp1Re7jnbSFXco7VEspsW34xPDdqfSs1xD2RjaCZPB3sL34z+8HFwdKabOr7diUWqM2X56Y6qtckASKXWBVh1oBqA4VSyMgRnRjXSZDA6bUg+TDV3Hr5732tFcJNqGqd/icSI11PVWLPLP81zGeZ5TatCsOtDCxMJ0Ml8uiR4AAB6hSURBVALJ2ciOFWdGbDPbnQkYpJetR6csYqSbOH+MzAHA2v2Gx4nUUNdbsdggIp87vVFE/hbYODCRlOo7y3FZV92Cz45i2zYAlYl9ZEmcbY4uX3YuQrIrqslXRGdgjM63UL3qbZ7Fl4AXReQvOVkcFgAh4GMDGUypvthW207UchlXmkZyJRqYFd+MZfzscsd7G26IK6KdWkpZKfO5o+4tiLRC5nmvlqxGsfMeWRhjjhljrge+DRxK3b5tjLnOGNMw8PGUOr9VVS0AlGae/CrPjG1ivzuO+Ki8zFHfFdGBGMMydy5iHNj7uteR1BDW18uqLgWWDnAWpS7Yu1XNTC/NJi3gEInDGLueUqeeZe77vY425AVxyDVdbEiMxy0Yh2/3IrjqL72OpYYonYWthq3OmMWmw20snJADJE9uz4hsAGCrzq/okyLTxkErn/pxd2IOLCHcdkyXLVdnpcVCDUvGGJburMN2DYdrj2LbFsbAzPgW6twiWkyu1xGHhSLTjkF4sy6IOAnCz36eSCTidSw1BGmxUMNSJBLhvxfvISvkoywvOYo7mOhksrWPbToRr89yTTdZEmedNQk7vYjcyCGvI6khSouFGpaMMdSHDfMuy8KXmkoxw9pOQFy2uTpktq8EmB1qZF1XEa2+QkKdh8GKeh1LDUFaLNSwdLA5Qthy6e5sw7YtAGZbW+ky6VS7Omv7QswJHaPVTmNn+jx8xsZ/SJctV2fSYqGGpRUHkpdILc9LDo/1GYeZ1na225UY/Vr3mTGGKSSvkLzSno7rS8O/X4fQqjPpT5UallYcaCUvJGQFk31QE+J7yTIRtuqs7Qti2xZvNI9hQqiTDd1jiOaMx1/1JsaOex1NDTFaLNSwE4nbbDjcTln2ya/v9OgmbPzssMo9TDY8BQJBZoeOsSOSz9FgJb54J/E9b3kdSw0xWizUsLN8z1ESjqE04+R8gOmRDexOjCXiBD1MNjzZlkVdayR59Tx7Bq4EtCtKnUGLhRp23q1qxS+cuNhRUeIopU49W+1KT3MNZ2OCUXy4bIqWEssZT2D/6+DqdcvVSQNWLESkQkSWisguEdkpIv+Yai8UkcUisj/1b0GqXUTkpyJSJSLbRGRej33dn3r9fhG5f6Ayq6HPGMM7+1sozfLhT42ZnRFeA8DGhC4ceLH8GMZIF5sjxURzJiKRZqhZ63UsNYQM5JGFDXzFGDMTWAg8KCIzgYeAt40xU4G3U48BPghMTd0eAH4ByeICfBO4FrgG+ObxAqNGn70NXdS2x6jIOfnVnRNbT7VbSqub7WGy4a9YOjhqZXMweDnGnwa7FnkdSQ0hA1YsjDH1xphNqftdwG5gHHA38HjqZY8DH03dvxt4wiStAfJFpAy4HVhsjGk1xrQBi4E7Biq3Gtpe35Yc5jk2I3kZ0Dy7mfHWQbY4U7yMNSKMkQ4A1oRLcCrfi7vrJYxeblWlDMo5CxGpBK4C1pK8VGt96qkG4PgMqnFATY/NalNt52o//T0eEJENIrKhqampX/OrocEYw9u7GylKFzICyS6oWdH1AGx1tVhcqjS7m3QTY0VLHq1jb8TXdZS2nUt0YUEFDEKxEJFs4AXgS8aYzp7PmeS3sF++icaYR4wxC4wxC4qLi/tjl2qIOdzYxo6GMJdlJR8bAzPDazjqL+eY0Z7JS5W8el4HG8JjqNq1BYNg3v6WLiyogAEuFiISJFkonjTG/DHVfCzVvUTq38ZUex1Q0WPz8lTbudrVKPPO/uSFjsZlJ48q0uItTIrvZUNkLK7RkTv9YQztOBKgWsYRySgjN3IkWZXVqDeQo6EE+DWw2xjzXz2eWgQcH9F0P/BSj/b7UqOiFgIdqe6qN4HbRKQgdWL7tlSbGmWW7m0hKyikVvhgVnwTPjFsNdoF1V8K6UAwbI6U0J01nmCiA2ne63UsNQQM5JHFDcBfAzeLyJbU7U7gYeBWEdkPfCD1GOA14CBQBTwKfAHAGNMKfBdYn7p9J9WmRpFowmF1dRvlOT6Sf4ckR0E1uXnUmjEepxs5gjgUSDebo2PozhyPAQL7X/M6lhoC+nRZ1YthjFlBshv0bG45y+sN8OA59vUY8Fj/pVPDzbv7m4jbLuXZQcAlzY0wNb6Dpe5czv01UxdjjHSyP34ZzSaXsvQSgntegVv/zetYymM6g1sNC29sryPoMxSlJc9NTI9sIoDDZmeyx8lGnkKnBYOwpqOA7szxBFv2YFoOeB1LeUyLhRryLMfl7T3NXJblw5fqgpoZWUObm8VBt8TjdCNPLmFCWGyMlNAUTI5St7a+4HEq5TUtFmrIW1XVTEfMpiI1CirgxJke28pWdwpGu6D6nQCl0sHGSAlRfw7xtDH4973qdSzlMS0WashbtLmGgBiKQskuqEmRzaSRYJNeu2LAlEg7YTfIzmgB4eyJ+Bu2QNshr2MpD2mxUEOa5bgs3tPMuCzBn/q2zomto9uks989YyK/6icFTgs+47Cqq4RwdmWyceefPM2kvKXFQg1pK/c30xmzqchOTgwLunFmxzayxZmCq1/fAePHpdjXxcZoGQl/NlbJHMwuLRajmf60qSHtpc01yVFQIRuAGdENpJsY65zpHicb+cb62ml2MtnWmUWbHUKObobWaq9jKY9osVBDVsJ2eXtvM+U5Afyp89hzulfQ6mazzx3rbbhRoJQ2/Li80zUWp3BqslGPLkYtLRZqyFpZ1URnzGZCbvJrmul2MiO+lQ3uNB0FNQjEjlHgtvFO11ji/iyskjk421/QVWhHKS0Wash6aXMtQZ+hODURb054NX4c1tpTPU42eox1m2lzM1nZlE5jLID/2Haidbu8jqU8oMVCDUkxy+GtPU1U5ATw+wRjYF50NUfdQl0LahCNMa34cFjePY543iQAAntf9jiV8oIWCzUkvbX7GN1xh4l5ya9oTryBKc5+1jmXo2tBDR5jxSg1bbzbfRnd/lziGSX497zU+4ZqxNFioYakFzYcIcN/ci2oBYl1AKlioQbTZW4jETfIu50lRHKn4G/ahWnY4XUsNci0WKghpzWc4N2qVirzgvhEMK5hQWwlVc5YWkye1/FGnRyrlQwT47XWcbSmj8cgWJue8jqWGmRaLNSQ8+q2o9iuYWJ+8utZGqviMtPAyvg0j5ONTgKM97ewM1bEEaeAWHY5gd0vgut6HU0NIi0Wash5YWMNuUFDjj85Ee/qyHIs42etpVfE80q5rwUfhjfbxhHJm4qv6ygcWeV1LDWItFioIeVwS5gttZ1MKggiIviNzVXRVWyyJhA1aV7HG7XSSXBl+jH+3HEZXVkTcAOZmG3Peh1LDSItFmrIMMbw1Mr9AFRkJbs4pkU3k226tQvKY7Zl4UbaabXTeae1iK60sZidL2KsmNfR1CDRYqGGjHA4zFPraynOgKxgcnjsvPA7dEouO+xyj9OpEmknZBK83FxGZ/YkfPFO4jte8TqWGiRaLNSQsbO+iy5LqMxNFopMp5Pp0U2sSUzC0RUmPOfDMI4mNkTHssdU4PgzYNvTuvzHKKHFQg0ZL28/hk84cUW8K8MrCeCwzp3lcTJ1XAXHQGBxVyUd2ZNIq36LaOMhr2OpQaDFQg0JtuPy+s5GyrN9hPzJ5T2u6l7GEWcMNW6h1/FUSgZxSqSDP3dU0Jx9OYIhsPMZr2OpQaDFQg0Jb+2opSVsMTHPD0Bh9BDj7UOscWZ4nEydrtytp91NZ3H3JGKZZfi3/B7jOl7HUgNMi4UaEv60uY6gz1CSnvylc3V0OY4RXd5jCBpDOxnEeb1jAg1pE/F31hDbs9jrWGqAabFQngvHbZZVtTM+x4ffJ/iMw/zoSna5lXSR6XU8dRoBJvia2BEbw3cbbsDxpRHc+nuvY6kBpsVCee7PuxqI2e6JUVBTYtvIc9tZ48z0OJk6lwpfC35cDvnKacmYiH//GxBu9jqWGkBaLJTnXtxUx9icIEWpCdrXdr1Fl+SwzZ3obTB1TiEsFqTVcsQpoC79csS1MFt0ccGRTIuF8tTR9igrqprJJYLtOORZTcyIbWR5fBoJo9etGKpsy8LqasEiwGuRaYRDJZh1j4Ke6B6xtFgoTz27oQZjYEpBEIAFnYvBwLvubI+Tqd6MCcbIIcKfuyrpzJ+Fr+MI7F7kdSw1QLRYKM84ruGZ9TVcOyGX7KAhaOJcF13CdncibSbH63iqFwKUmwYO2wWsdmbgFkyClT8BndE9Ig1YsRCRx0SkUUR29GgrFJHFIrI/9W9Bql1E5KciUiUi20RkXo9t7k+9fr+I3D9QedXge2dfI/UdMXyxDmzb5trIMrLdLhbb872OpvroMprx4/BqZyXdcz4NRzfDoRVex1IDYCCPLH4L3HFa20PA28aYqcDbqccAHwSmpm4PAL+AZHEBvglcC1wDfPN4gVHD369XVFOSE2JCYQbi2ryv61WqnDL2u2O9jqb6KIDDZbSwvLuMvQeqcdMLsZf/l64XNQINWLEwxiwHWk9rvht4PHX/ceCjPdqfMElrgHwRKQNuBxYbY1qNMW3AYs4sQGoY2l7bzsqqFv7qmnL8ArO7V1BkWnjTntf7xmpIKbNqsYyfxZ0VNATLCVQvIXp4k9exVD8b7HMWpcaY+tT9BqA0dX8cUNPjdbWptnO1n0FEHhCRDSKyoampqX9Tq373i6X7CfjgpoogViLBB7peos4pYLsOlx12MqwO8twOXmodT1fBTFwJEFz/C69jqX7m2QlukzxO7bdjVWPMI8aYBcaYBcXFxf21W9XPjDFsP9zIG7samVrg5zdLdzAlvJGKQCuvx67CoMNlh6NKc4xjdibLw+MJ508nsOuPmNZqr2OpfjTYxeJYqnuJ1L+NqfY6oKLH68pTbedqV8NUJBLhC0+sQzBMyzOEQmncHF5Ei5vFGmuq1/HURSqwGsk0UZ5pruRY9kwMYC/9gdexVD8a7GKxCDg+oul+4KUe7felRkUtBDpS3VVvAreJSEHqxPZtqTY1TO1u6KImLEwr8JEeECbHdzA5sZfXY1fi4Pc6nrpIAkwONFIVz2dl91jC+dMI7HxOjy5GkIEcOvsHYDUwTURqReSzwMPArSKyH/hA6jHAa8BBoAp4FPgCgDGmFfgusD51+06qTQ1Dxhh+9FYVIR9MLxAwhtvan6XNV8g7cV2KfLgrlxaK/BF+VD+bw6FpGGOwl/0vr2OpfhIYqB0bY+49x1O3nOW1BnjwHPt5DHisH6Mpj6zYW8/K6g7mjBFCfmFadDMTEvt4In4TCePDp6crhjUfLndl7OQ33VezIj6RCfnTyd7xLOamf0EKKr2Opy6RzuBWg8IYw0+XHiTdD5fnJ48qbm1/hhZ/MeuMLu0xEtiWRXVLhGyi/L5lGg3ZMzGuq+cuRggtFmpQvL6jnvWHO7iiOIBfhGndqym3qnk94249VzGCpIVCTPMfpdbK4c3wVMKFMwhsfwYa93gdTV0iLRZqwEUSNt95eRd5IcPEHBfbSnBry++pd/J5pz6Ia3Sl0pGklDYmBdt4vHESTfnzMcFM7Ne/prO6hzktFmrA/WxJFQ2dca4tS8MnwsLIEsp9LSyyr8MXGLDTZsojjm2RGz5Ck53J4w0TOBKYRKB6CfEdL3sdTV0CLRZqQB1s6uaR5QcZn+VSmOaQ4Xbzwa7n2OeOY5M72et4aoCUBiOUSDvPtE1hZ3A2iWAeobe+Domw19HURdJioQaM67r824vbSPMLV5UkjyBubX+GTBPmOet9oLO1R7Rp7kHirp/v1l9NXf61+DpqYNnDvW+ohiQtFmpAGGP4/bt7WXmwjcr0CH4cKmL7WNi9mCXWHI64RV5HVAMskzgTOUqDr4S17jSiMz6JWf0zOLLG62jqImixUAPicGMb33vzAEXpcHmBH79J8LGW/6HdzWKRvdDreGqQTKKOdOL8onEmh8IBnJxxuM9/FhNt9zqaukBaLFS/M8bwvdf2Yxvh2rHJr9gH2p7lMqeWp6z3EyfkcUI1WPy4TOcIh608/tQxhQNpM5HOOpxFX/I6mrpAWixUv3tlWz1v7W3myuIAuSFhfHQ3t8Ze5117pi5BPgrlRmopclr5TeNUqphAx5h5BHa/iNn6jNfR1AXQYqH6VWNnjP/vpR3MLM1iRpGfoBPlr7t/RbPJ5TnrPV7HUx4QYCaHCOLwrzXzORC8nEhaCeaVfyJ8ZKvOvxgmtFiofmPZDp///QZilkNFqBvHtrmz5TEK3RYeDd+i3U+jWMCKMCFxgDZyeLlzMg2l78V1bAK/v5tIR7PX8VQfaLFQ/cIYw7cXbWfjkQ7+6caxFOdkMLNrBdfFlvNabA5VTpnXEZXHygPtlEobT7ZO46BTSkPJewkl2vC9/lWM63odT/VCi4XqF79YdoDfr6tjWqGfPQcPkxet4f/ueJSD7lj+GF3gdTw1BAgwx3+EDLH51pE5NIUqaCmYS8beF0msedTreKoXWizUJfvt8v38rzf3Mj7bMLfIkBHyc2/zj3Hw8+vEnTj6NVMpISz+Lmc1tVY2P6ibRUvelcSyygm9/a86/2KI059iddGMMfxxfTXfeX0fpekuC8v8iAh3tv2Ocqua30TfT7PJ8jqmGkJsy2JjozDDV8uacBm/Ozae5svej5U5FvepezAtB72OqM5Bi4W6aG/vqOX//eMupo9J48ZxAXwiXN31Njd0v8liay7bzSSvI6ohKBgKUeHUUuoc4/et03mjqZgdgdm4iTDmyY9DtM3riOostFioi7Klpp1/eHYHuSGhQpoxrsOM8Ho+1vYo2+0J/NG+3uuIaggT4ArfIXIlwn8eu4q9Uknj2FuQ9iPYT32K7vYWHVI7xGixUBesqrGLTz+2jsLMIDePD5GZns7s7pX8VcuPOOSU8Kh1B65e0Ej1wo/LPLMbv2vxvfprWJ2YzOG8qwnUrIJH3kekS5cEGUq0WKg+M8awtfoY9z6yBr/AlXkxQmIzL7ycT3f+goN2CT9J3K3zKVSfpWFxbWA/GcT5et01rA4u5FjRQrIjNaS98iA4ttcRVYoWC9VnGw8e45OPrqczGueqvAg5aQGubn+Nvw7/mj3OOH5q3U2MNK9jqmEmZIeZEt5KsXTxb0fmsYj30l56HYF9r2I/9xmMnfA6okKLheqjTYfb+JsntuD3CR+o8FOcHeKDrb/lLzofZ7PM4qfRD5Eg6HVMNUylYVHeuZ1s082/113Jj2MfpmnelwjseQn3iY9BpNXriKOeFgt1XsYYnl17kHsfXYPr2NxS4aPAH+dTjT/k/eHXWWZfyf9uvJa4o+co1KXJCPhYwG7KTBO/PTaRz22eSFXJ7fhq1+L+/DrMbr0sq5e0WKhz6o5ZfOUP6/nnF3eT7be5dbyfSo7y+WP/xqzYJp7NuIdn7Jvw+7VQqP7hx2U2B5jpVrEzWsg9dZ/kmcxPkUjEkWf+CvvJe6DzqNcxR6WA1wHU0GOM4dXt9Xz/lZ3Ud8aZmgdzx/i4vusV7ux6DosgP7fuYnNjEYGA43VcNcIIUCFNZCU62BmcyUPNt/Nu7lS+lP0Skw8uwfx0Htz4JeT6f4CQTvocLFosFJAsEJ3d3SzZ38Ev36li77EweSHDe0ocrmUHdzc8Q4VzmO32eH4bu5lufw4B/faoAVQYTHA929hjj+PVzkms9/8tf5+7gY/Ln8ld9h+4G35D4j0PkXb1fYhPj24Hmv64K8Jxi8eW7uGXK48QtiA3aFhQaPGhjB28p/1PTHGqaHGzedS+nQ3OVPCL15HVKOHHZZa/hjLTwQ5rIt9pu4mXM2bx97kruTG6hKzXv4S14VECd/4AmajXSxlIWixGKWMMe2pbeGFrI8+uP0Jn3GFMuuG2jAP8X8GVXBldQ26kg2Ynm6fiN7BS5mDr10V5pJBOFtqbyctK45XoXP4++lFuzFnIPbnruKXlVYKPfxh70gewFjxA+vRbEZ+eju1v+tM/yrSGEyze1cCLGw6z5nAnPoEZoSY+lf0OH3NeJzMRJZEIsdMuZ4V1PZujl2F8aYTS9DBfecu1EzS3xXlP1maquYyN3cW823U3ub47uD6wlzur3uXaAw8QKi7DnnsfoSvuQvLKvY49YmixGOHCcYvV+xrYeKSddYfa2VzbiWug2NfF3wZX8TnfS5RKO11WOpvsSra5k9meGIcbSE6uc4jpkDk1ZAQDIULYTOMI42MHafUV0uQbw1vOTN6QOQCUHW1n/tHdzH/jy8wttJk1YyamaCrBcVcSzSonM68IEe1KvVDDpliIyB3ATwA/8CtjzMMeRxoyogmb6oY2jnXFaY9DXXuUnTUt7Gvo4FC7hWMEwXC51PKgbwO3+9dR6Wuiyi5lcfhy9rnlNAbLMMfLwrD5VqjRLCMUYBydjKMTl2oauoXuYCFd/lyW+OfyinsdNEJaY4I5cpDpvt8wURqYkBmnsiCNspJi/MWXE6qYi5TMgsxC0CJyTsPi14KI+IH/A9wK1ALrRWSRMWaXt8kunesaYrZDPGERi0aJxWPEYnHiiTjReJyW9i66YhZdMYu2zm4ilkt7OEZLDBpjAY4lQnQ4Zy6xMY4mZvgOc6evhnm+fUyllno3n33OZbzgXE+NVYA/4CeWiOHzBQnp8YMaxnwYCn1RxhiLkD95Te+uOHT6cmknh2p/MdvcicQlDTqBTvAfdiiikxLZRrEsp9jXRVHIojANsjLSyUhPpzAvh1Aojby8fILBEMFQGplZ2fgDIWzHJTM7B9sxZOfm4Q+EiFoumTl5iD8N/EEYQaO0hkWxAK4BqowxBwFE5GngbqBfi8Wu7Zv43NO7cA0Ykn9hGAQDGJP6N/UYTj52EczZtjnl37M9J7gX9Es6jSyi5OBSLB2U084VdFAgXeQQJtuESXcihEwMVwI0R4LUuwU8zRVEfVcDPf9qssCyiMUT+H0Ojnv2L3Vvz/flNSNpH8Mp60jax8W8TwAodGIU0ggWGKA7YYiSiRXKJubLQHx+On1+Wn1lbJLJdFtZOBE/nPeSGpHUDZKVByA5UdCHiz/5GwHB4MNFSBYzweATc+rj48+L6bGN4VKOb24pi/Gtf3jgEvZwdsOlWIwDano8rgWu7fkCEXkAOP4/1C0iewcpW1+NAZq9DuGh0f75Qf8P9PMPwudfAXz7//m7i918wrmeGC7FolfGmEeAR7zOcS4issEYs8DrHF4Z7Z8f9P9AP//w/vzDpaO6Dqjo8bg81aaUUmoQDJdisR6YKiITRSQE3AMs8jiTUkqNGsOiG8oYY4vIF4E3SQ6dfcwYs9PjWBdqyHaRDZLR/vlB/w/08w9johdFV0op1Zvh0g2llFLKQ1oslFJK9UqLhQdE5CsiYkRkjNdZBpOI/KeI7BGRbSLyoojke51pMIjIHSKyV0SqROQhr/MMJhGpEJGlIrJLRHaKyD96nckLIuIXkc0i8orXWS6WFotBJiIVwG3AEa+zeGAxMNsYMwfYB3zN4zwDrsdSNR8EZgL3ishMb1MNKhv4ijFmJrAQeHCUff7j/hHY7XWIS6HFYvD9CPhnYNSNLDDG/NkYY6ceriE5X2akO7FUjTEmARxfqmZUMMbUG2M2pe53kfyFOc7bVINLRMqBDwG/8jrLpdBiMYhE5G6gzhiz1essQ8BngNe9DjEIzrZUzaj6ZXmciFQCVwFrvU0y6H5M8g9E1+sgl2JYzLMYTkTkLWDsWZ76BvB1kl1QI9b5Pr8x5qXUa75BsnviycHMprwjItnA/9/e3bzaEMdxHH9/ipKQrbKhlI2NhLLwcO/G7cZeiCyI8lD+C2UhxZbYKBYWHpKVlBJhY2NhoYiSh7L9Wpy5Je5tcp0703Herzqdmt8svtNZfOb3+535zk3gdFV9azv/f5FkGvhYVc+S7Oi7nn9hWAxZVU3OdjzJBmAN8LJ58cpq4HmSzVX1ocMSF9Rc1z8jySFgGpio8XjIZ+xb1SRZzCAorlfVrb7r6dg2YE+SKWAJsCLJtara33Ndf82H8nqS5C2wqarGpgtn8wKr88D2qvrUdz1dSLKIwWb+BIOQeArsG8EOBPOSwZ3RFeBzVZ3uu54+NTOLs1U13Xct8+Gehbp0EVgOPEjyIsnlvgtaaM2G/kyrmtfAjXEJisY24ACwq/nNXzR32RoxziwkSa2cWUiSWhkWkqRWhoUkqZVhIUlqZVhIkloZFpKkVoaFNCRJTiZ5neSPNiYZuNC0KX+VZOMvY/eSfBnl9tX6/9nuQxqe48BkVb2bZWw3sK75bAEuNd8A54ClwNEuipTmw5mFNATN0+hrgbtJzsxyyl7gag08AVYmWQVQVQ+B791VK/09ZxbSEFTVsab31c45+n3N1ar8fRf1Sf/KmYUkqZVhIXVj7FuVa7QZFlI3bgMHm39FbQW+VpVLUBoZ7llI3bgDTAFvgB/A4ZmBJI+A9cCyJO+AI1V1v5cqpTnYolyS1MplKElSK5ehpCFKchg49dvhx1V1oo96pGFxGUqS1MplKElSK8NCktTKsJAktTIsJEmtfgKPZDQElwJKSwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model**"
      ],
      "metadata": {
        "id": "CgEyqxfmtIGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Model , Sequential\n",
        "from keras.layers import Dense, Flatten , Input\n",
        "from keras.callbacks import ModelCheckpoint , EarlyStopping , ReduceLROnPlateau\n",
        "from keras import regularizers , metrics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score , classification_report , accuracy_score , plot_confusion_matrix ,plot_roc_curve"
      ],
      "metadata": {
        "id": "dm8T7QhStHXh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = train[\"target\"]\n",
        "X = train.drop([\"target\"], axis=1)"
      ],
      "metadata": {
        "id": "awjz7mOftQiq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X = pd.DataFrame(scaler.fit_transform(X))"
      ],
      "metadata": {
        "id": "gKh39_QZtYyJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "M7WRnD4jt-D9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Input(shape=(len(X.columns))))\n",
        "\n",
        "#model.add(Dense(512, activation='swish',kernel_regularizer= regularizers.l2(1e-5)))\n",
        "\n",
        "model.add(Dense(64, activation='swish',kernel_regularizer= regularizers.l2(1e-5)))\n",
        "\n",
        "model.add(Dense(64 , activation='swish',kernel_regularizer= regularizers.l2(1e-5)))\n",
        "\n",
        "model.add(Dense(64, activation='swish',kernel_regularizer= regularizers.l2(1e-5)))\n",
        "\n",
        "model.add(Dense(16,activation='swish',kernel_regularizer= regularizers.l2(1e-5)))\n",
        "\n",
        "model.add(Dense(1 , activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quAheWzjuHz-",
        "outputId": "a3986fed-6952-4f58-8f25-7fe48ba11e1f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 64)                2048      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 16)                1040      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,425\n",
            "Trainable params: 11,425\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='Adam',loss='binary_crossentropy',metrics = [metrics.AUC(name='auc')])"
      ],
      "metadata": {
        "id": "vr2VteTKufhL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = ReduceLROnPlateau(monitor='val_loss',patience=5,factor=0.7,verbose=1)\n",
        "es = EarlyStopping(monitor='val_loss',patience=10,verbose=1)\n",
        "mc = ModelCheckpoint(filepath='/content',monitor='val_loss',verbose=1,save_best_only=True)"
      ],
      "metadata": {
        "id": "Tdq-kvNmuhss"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train,validation_data=(x_test, y_test), callbacks=[lr, es, mc], epochs=200, batch_size=2048)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyBrn79sujdN",
        "outputId": "d76367ef-7fc2-4ac6-a7b8-9374d97c812e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "350/352 [============================>.] - ETA: 0s - loss: 0.5100 - auc: 0.8288\n",
            "Epoch 1: val_loss improved from inf to 0.42513, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 10s 17ms/step - loss: 0.5096 - auc: 0.8291 - val_loss: 0.4251 - val_auc: 0.8880 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "347/352 [============================>.] - ETA: 0s - loss: 0.3773 - auc: 0.9127\n",
            "Epoch 2: val_loss improved from 0.42513 to 0.33297, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.3767 - auc: 0.9130 - val_loss: 0.3330 - val_auc: 0.9328 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "346/352 [============================>.] - ETA: 0s - loss: 0.3112 - auc: 0.9416\n",
            "Epoch 3: val_loss improved from 0.33297 to 0.29716, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 10ms/step - loss: 0.3110 - auc: 0.9417 - val_loss: 0.2972 - val_auc: 0.9469 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "347/352 [============================>.] - ETA: 0s - loss: 0.2860 - auc: 0.9510\n",
            "Epoch 4: val_loss improved from 0.29716 to 0.27950, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.2859 - auc: 0.9510 - val_loss: 0.2795 - val_auc: 0.9533 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "350/352 [============================>.] - ETA: 0s - loss: 0.2672 - auc: 0.9574\n",
            "Epoch 5: val_loss improved from 0.27950 to 0.26136, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.2672 - auc: 0.9574 - val_loss: 0.2614 - val_auc: 0.9594 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.2525 - auc: 0.9621\n",
            "Epoch 6: val_loss improved from 0.26136 to 0.24694, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.2524 - auc: 0.9621 - val_loss: 0.2469 - val_auc: 0.9639 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.2411 - auc: 0.9656\n",
            "Epoch 7: val_loss improved from 0.24694 to 0.23844, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 10ms/step - loss: 0.2411 - auc: 0.9656 - val_loss: 0.2384 - val_auc: 0.9664 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.2317 - auc: 0.9683\n",
            "Epoch 8: val_loss improved from 0.23844 to 0.23001, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.2317 - auc: 0.9683 - val_loss: 0.2300 - val_auc: 0.9689 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.2242 - auc: 0.9704\n",
            "Epoch 9: val_loss improved from 0.23001 to 0.22403, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.2242 - auc: 0.9704 - val_loss: 0.2240 - val_auc: 0.9704 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "345/352 [============================>.] - ETA: 0s - loss: 0.2186 - auc: 0.9719\n",
            "Epoch 10: val_loss improved from 0.22403 to 0.22012, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.2184 - auc: 0.9719 - val_loss: 0.2201 - val_auc: 0.9715 - lr: 0.0010\n",
            "Epoch 11/200\n",
            "343/352 [============================>.] - ETA: 0s - loss: 0.2131 - auc: 0.9733\n",
            "Epoch 11: val_loss improved from 0.22012 to 0.21396, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.2131 - auc: 0.9733 - val_loss: 0.2140 - val_auc: 0.9733 - lr: 0.0010\n",
            "Epoch 12/200\n",
            "350/352 [============================>.] - ETA: 0s - loss: 0.2087 - auc: 0.9744\n",
            "Epoch 12: val_loss improved from 0.21396 to 0.21042, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 10ms/step - loss: 0.2087 - auc: 0.9744 - val_loss: 0.2104 - val_auc: 0.9740 - lr: 0.0010\n",
            "Epoch 13/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.2048 - auc: 0.9754\n",
            "Epoch 13: val_loss improved from 0.21042 to 0.20705, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.2048 - auc: 0.9754 - val_loss: 0.2071 - val_auc: 0.9750 - lr: 0.0010\n",
            "Epoch 14/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.2009 - auc: 0.9764\n",
            "Epoch 14: val_loss improved from 0.20705 to 0.20399, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.2009 - auc: 0.9764 - val_loss: 0.2040 - val_auc: 0.9756 - lr: 0.0010\n",
            "Epoch 15/200\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1975 - auc: 0.9772\n",
            "Epoch 15: val_loss improved from 0.20399 to 0.19939, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1975 - auc: 0.9772 - val_loss: 0.1994 - val_auc: 0.9767 - lr: 0.0010\n",
            "Epoch 16/200\n",
            "345/352 [============================>.] - ETA: 0s - loss: 0.1947 - auc: 0.9779\n",
            "Epoch 16: val_loss improved from 0.19939 to 0.19668, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1947 - auc: 0.9779 - val_loss: 0.1967 - val_auc: 0.9773 - lr: 0.0010\n",
            "Epoch 17/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1918 - auc: 0.9785\n",
            "Epoch 17: val_loss improved from 0.19668 to 0.19533, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 4s 10ms/step - loss: 0.1918 - auc: 0.9785 - val_loss: 0.1953 - val_auc: 0.9778 - lr: 0.0010\n",
            "Epoch 18/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1893 - auc: 0.9791\n",
            "Epoch 18: val_loss improved from 0.19533 to 0.19470, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1893 - auc: 0.9791 - val_loss: 0.1947 - val_auc: 0.9780 - lr: 0.0010\n",
            "Epoch 19/200\n",
            "346/352 [============================>.] - ETA: 0s - loss: 0.1874 - auc: 0.9796\n",
            "Epoch 19: val_loss improved from 0.19470 to 0.19370, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 10ms/step - loss: 0.1874 - auc: 0.9796 - val_loss: 0.1937 - val_auc: 0.9788 - lr: 0.0010\n",
            "Epoch 20/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1852 - auc: 0.9800\n",
            "Epoch 20: val_loss improved from 0.19370 to 0.18725, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1852 - auc: 0.9800 - val_loss: 0.1873 - val_auc: 0.9796 - lr: 0.0010\n",
            "Epoch 21/200\n",
            "345/352 [============================>.] - ETA: 0s - loss: 0.1831 - auc: 0.9805\n",
            "Epoch 21: val_loss did not improve from 0.18725\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1832 - auc: 0.9805 - val_loss: 0.1883 - val_auc: 0.9797 - lr: 0.0010\n",
            "Epoch 22/200\n",
            "345/352 [============================>.] - ETA: 0s - loss: 0.1810 - auc: 0.9810\n",
            "Epoch 22: val_loss improved from 0.18725 to 0.18511, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1810 - auc: 0.9810 - val_loss: 0.1851 - val_auc: 0.9801 - lr: 0.0010\n",
            "Epoch 23/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1797 - auc: 0.9813\n",
            "Epoch 23: val_loss did not improve from 0.18511\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1797 - auc: 0.9813 - val_loss: 0.1860 - val_auc: 0.9799 - lr: 0.0010\n",
            "Epoch 24/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1779 - auc: 0.9817\n",
            "Epoch 24: val_loss improved from 0.18511 to 0.18503, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 10ms/step - loss: 0.1779 - auc: 0.9817 - val_loss: 0.1850 - val_auc: 0.9802 - lr: 0.0010\n",
            "Epoch 25/200\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1766 - auc: 0.9820\n",
            "Epoch 25: val_loss improved from 0.18503 to 0.18176, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1766 - auc: 0.9820 - val_loss: 0.1818 - val_auc: 0.9808 - lr: 0.0010\n",
            "Epoch 26/200\n",
            "343/352 [============================>.] - ETA: 0s - loss: 0.1749 - auc: 0.9823\n",
            "Epoch 26: val_loss improved from 0.18176 to 0.18030, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1748 - auc: 0.9823 - val_loss: 0.1803 - val_auc: 0.9812 - lr: 0.0010\n",
            "Epoch 27/200\n",
            "348/352 [============================>.] - ETA: 0s - loss: 0.1737 - auc: 0.9826\n",
            "Epoch 27: val_loss improved from 0.18030 to 0.17886, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 6s 17ms/step - loss: 0.1737 - auc: 0.9826 - val_loss: 0.1789 - val_auc: 0.9815 - lr: 0.0010\n",
            "Epoch 28/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1729 - auc: 0.9827\n",
            "Epoch 28: val_loss improved from 0.17886 to 0.17817, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 6s 18ms/step - loss: 0.1729 - auc: 0.9827 - val_loss: 0.1782 - val_auc: 0.9818 - lr: 0.0010\n",
            "Epoch 29/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1715 - auc: 0.9830\n",
            "Epoch 29: val_loss did not improve from 0.17817\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1714 - auc: 0.9830 - val_loss: 0.1804 - val_auc: 0.9818 - lr: 0.0010\n",
            "Epoch 30/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1703 - auc: 0.9833\n",
            "Epoch 30: val_loss did not improve from 0.17817\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1703 - auc: 0.9833 - val_loss: 0.1814 - val_auc: 0.9818 - lr: 0.0010\n",
            "Epoch 31/200\n",
            "348/352 [============================>.] - ETA: 0s - loss: 0.1694 - auc: 0.9835\n",
            "Epoch 31: val_loss improved from 0.17817 to 0.17505, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 10ms/step - loss: 0.1695 - auc: 0.9835 - val_loss: 0.1750 - val_auc: 0.9823 - lr: 0.0010\n",
            "Epoch 32/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1680 - auc: 0.9838\n",
            "Epoch 32: val_loss did not improve from 0.17505\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1680 - auc: 0.9838 - val_loss: 0.1757 - val_auc: 0.9826 - lr: 0.0010\n",
            "Epoch 33/200\n",
            "348/352 [============================>.] - ETA: 0s - loss: 0.1672 - auc: 0.9839\n",
            "Epoch 33: val_loss improved from 0.17505 to 0.17465, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1672 - auc: 0.9839 - val_loss: 0.1746 - val_auc: 0.9826 - lr: 0.0010\n",
            "Epoch 34/200\n",
            "347/352 [============================>.] - ETA: 0s - loss: 0.1665 - auc: 0.9841\n",
            "Epoch 34: val_loss improved from 0.17465 to 0.17327, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1664 - auc: 0.9841 - val_loss: 0.1733 - val_auc: 0.9827 - lr: 0.0010\n",
            "Epoch 35/200\n",
            "347/352 [============================>.] - ETA: 0s - loss: 0.1656 - auc: 0.9843\n",
            "Epoch 35: val_loss improved from 0.17327 to 0.17157, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1656 - auc: 0.9842 - val_loss: 0.1716 - val_auc: 0.9830 - lr: 0.0010\n",
            "Epoch 36/200\n",
            "346/352 [============================>.] - ETA: 0s - loss: 0.1645 - auc: 0.9845\n",
            "Epoch 36: val_loss improved from 0.17157 to 0.17106, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1645 - auc: 0.9845 - val_loss: 0.1711 - val_auc: 0.9833 - lr: 0.0010\n",
            "Epoch 37/200\n",
            "345/352 [============================>.] - ETA: 0s - loss: 0.1640 - auc: 0.9846\n",
            "Epoch 37: val_loss improved from 0.17106 to 0.17053, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 4s 10ms/step - loss: 0.1640 - auc: 0.9846 - val_loss: 0.1705 - val_auc: 0.9833 - lr: 0.0010\n",
            "Epoch 38/200\n",
            "350/352 [============================>.] - ETA: 0s - loss: 0.1635 - auc: 0.9847\n",
            "Epoch 38: val_loss improved from 0.17053 to 0.17015, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1635 - auc: 0.9847 - val_loss: 0.1702 - val_auc: 0.9835 - lr: 0.0010\n",
            "Epoch 39/200\n",
            "346/352 [============================>.] - ETA: 0s - loss: 0.1625 - auc: 0.9849\n",
            "Epoch 39: val_loss improved from 0.17015 to 0.16883, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1626 - auc: 0.9849 - val_loss: 0.1688 - val_auc: 0.9837 - lr: 0.0010\n",
            "Epoch 40/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1617 - auc: 0.9850\n",
            "Epoch 40: val_loss improved from 0.16883 to 0.16854, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1617 - auc: 0.9850 - val_loss: 0.1685 - val_auc: 0.9837 - lr: 0.0010\n",
            "Epoch 41/200\n",
            "345/352 [============================>.] - ETA: 0s - loss: 0.1611 - auc: 0.9852\n",
            "Epoch 41: val_loss improved from 0.16854 to 0.16802, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1610 - auc: 0.9852 - val_loss: 0.1680 - val_auc: 0.9838 - lr: 0.0010\n",
            "Epoch 42/200\n",
            "347/352 [============================>.] - ETA: 0s - loss: 0.1605 - auc: 0.9853\n",
            "Epoch 42: val_loss did not improve from 0.16802\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1605 - auc: 0.9853 - val_loss: 0.1684 - val_auc: 0.9838 - lr: 0.0010\n",
            "Epoch 43/200\n",
            "346/352 [============================>.] - ETA: 0s - loss: 0.1600 - auc: 0.9854\n",
            "Epoch 43: val_loss improved from 0.16802 to 0.16735, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1600 - auc: 0.9854 - val_loss: 0.1673 - val_auc: 0.9840 - lr: 0.0010\n",
            "Epoch 44/200\n",
            "345/352 [============================>.] - ETA: 0s - loss: 0.1597 - auc: 0.9854\n",
            "Epoch 44: val_loss did not improve from 0.16735\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1596 - auc: 0.9854 - val_loss: 0.1692 - val_auc: 0.9839 - lr: 0.0010\n",
            "Epoch 45/200\n",
            "350/352 [============================>.] - ETA: 0s - loss: 0.1589 - auc: 0.9856\n",
            "Epoch 45: val_loss improved from 0.16735 to 0.16548, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 4s 10ms/step - loss: 0.1590 - auc: 0.9856 - val_loss: 0.1655 - val_auc: 0.9843 - lr: 0.0010\n",
            "Epoch 46/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1582 - auc: 0.9857\n",
            "Epoch 46: val_loss did not improve from 0.16548\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 0.1582 - auc: 0.9857 - val_loss: 0.1659 - val_auc: 0.9842 - lr: 0.0010\n",
            "Epoch 47/200\n",
            "349/352 [============================>.] - ETA: 0s - loss: 0.1578 - auc: 0.9858\n",
            "Epoch 47: val_loss did not improve from 0.16548\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1578 - auc: 0.9858 - val_loss: 0.1657 - val_auc: 0.9844 - lr: 0.0010\n",
            "Epoch 48/200\n",
            "346/352 [============================>.] - ETA: 0s - loss: 0.1570 - auc: 0.9859\n",
            "Epoch 48: val_loss improved from 0.16548 to 0.16488, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1570 - auc: 0.9859 - val_loss: 0.1649 - val_auc: 0.9845 - lr: 0.0010\n",
            "Epoch 49/200\n",
            "347/352 [============================>.] - ETA: 0s - loss: 0.1565 - auc: 0.9860\n",
            "Epoch 49: val_loss did not improve from 0.16488\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1564 - auc: 0.9861 - val_loss: 0.1655 - val_auc: 0.9845 - lr: 0.0010\n",
            "Epoch 50/200\n",
            "348/352 [============================>.] - ETA: 0s - loss: 0.1562 - auc: 0.9861\n",
            "Epoch 50: val_loss improved from 0.16488 to 0.16264, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1562 - auc: 0.9861 - val_loss: 0.1626 - val_auc: 0.9849 - lr: 0.0010\n",
            "Epoch 51/200\n",
            "349/352 [============================>.] - ETA: 0s - loss: 0.1558 - auc: 0.9862\n",
            "Epoch 51: val_loss did not improve from 0.16264\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 0.1558 - auc: 0.9862 - val_loss: 0.1634 - val_auc: 0.9848 - lr: 0.0010\n",
            "Epoch 52/200\n",
            "345/352 [============================>.] - ETA: 0s - loss: 0.1551 - auc: 0.9863\n",
            "Epoch 52: val_loss improved from 0.16264 to 0.16214, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1551 - auc: 0.9863 - val_loss: 0.1621 - val_auc: 0.9850 - lr: 0.0010\n",
            "Epoch 53/200\n",
            "347/352 [============================>.] - ETA: 0s - loss: 0.1547 - auc: 0.9864\n",
            "Epoch 53: val_loss did not improve from 0.16214\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1547 - auc: 0.9864 - val_loss: 0.1631 - val_auc: 0.9848 - lr: 0.0010\n",
            "Epoch 54/200\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1541 - auc: 0.9865\n",
            "Epoch 54: val_loss did not improve from 0.16214\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 0.1541 - auc: 0.9865 - val_loss: 0.1631 - val_auc: 0.9848 - lr: 0.0010\n",
            "Epoch 55/200\n",
            "344/352 [============================>.] - ETA: 0s - loss: 0.1540 - auc: 0.9865\n",
            "Epoch 55: val_loss improved from 0.16214 to 0.16128, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1539 - auc: 0.9865 - val_loss: 0.1613 - val_auc: 0.9851 - lr: 0.0010\n",
            "Epoch 56/200\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1535 - auc: 0.9866\n",
            "Epoch 56: val_loss improved from 0.16128 to 0.15991, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1535 - auc: 0.9866 - val_loss: 0.1599 - val_auc: 0.9854 - lr: 0.0010\n",
            "Epoch 57/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1531 - auc: 0.9867\n",
            "Epoch 57: val_loss did not improve from 0.15991\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1531 - auc: 0.9867 - val_loss: 0.1612 - val_auc: 0.9853 - lr: 0.0010\n",
            "Epoch 58/200\n",
            "348/352 [============================>.] - ETA: 0s - loss: 0.1525 - auc: 0.9868\n",
            "Epoch 58: val_loss did not improve from 0.15991\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1526 - auc: 0.9868 - val_loss: 0.1614 - val_auc: 0.9852 - lr: 0.0010\n",
            "Epoch 59/200\n",
            "350/352 [============================>.] - ETA: 0s - loss: 0.1527 - auc: 0.9868\n",
            "Epoch 59: val_loss did not improve from 0.15991\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1526 - auc: 0.9868 - val_loss: 0.1617 - val_auc: 0.9851 - lr: 0.0010\n",
            "Epoch 60/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1520 - auc: 0.9869\n",
            "Epoch 60: val_loss did not improve from 0.15991\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1520 - auc: 0.9869 - val_loss: 0.1606 - val_auc: 0.9854 - lr: 0.0010\n",
            "Epoch 61/200\n",
            "346/352 [============================>.] - ETA: 0s - loss: 0.1515 - auc: 0.9870\n",
            "Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
            "\n",
            "Epoch 61: val_loss did not improve from 0.15991\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1516 - auc: 0.9870 - val_loss: 0.1602 - val_auc: 0.9854 - lr: 0.0010\n",
            "Epoch 62/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1492 - auc: 0.9874\n",
            "Epoch 62: val_loss improved from 0.15991 to 0.15811, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 4s 10ms/step - loss: 0.1492 - auc: 0.9874 - val_loss: 0.1581 - val_auc: 0.9857 - lr: 7.0000e-04\n",
            "Epoch 63/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1490 - auc: 0.9875\n",
            "Epoch 63: val_loss did not improve from 0.15811\n",
            "352/352 [==============================] - 2s 7ms/step - loss: 0.1489 - auc: 0.9874 - val_loss: 0.1596 - val_auc: 0.9857 - lr: 7.0000e-04\n",
            "Epoch 64/200\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1490 - auc: 0.9874\n",
            "Epoch 64: val_loss did not improve from 0.15811\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 0.1490 - auc: 0.9874 - val_loss: 0.1584 - val_auc: 0.9858 - lr: 7.0000e-04\n",
            "Epoch 65/200\n",
            "350/352 [============================>.] - ETA: 0s - loss: 0.1488 - auc: 0.9875\n",
            "Epoch 65: val_loss improved from 0.15811 to 0.15734, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1488 - auc: 0.9875 - val_loss: 0.1573 - val_auc: 0.9859 - lr: 7.0000e-04\n",
            "Epoch 66/200\n",
            "348/352 [============================>.] - ETA: 0s - loss: 0.1482 - auc: 0.9876\n",
            "Epoch 66: val_loss improved from 0.15734 to 0.15683, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 4s 11ms/step - loss: 0.1482 - auc: 0.9876 - val_loss: 0.1568 - val_auc: 0.9860 - lr: 7.0000e-04\n",
            "Epoch 67/200\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1484 - auc: 0.9875\n",
            "Epoch 67: val_loss improved from 0.15683 to 0.15621, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1484 - auc: 0.9875 - val_loss: 0.1562 - val_auc: 0.9861 - lr: 7.0000e-04\n",
            "Epoch 68/200\n",
            "349/352 [============================>.] - ETA: 0s - loss: 0.1477 - auc: 0.9877\n",
            "Epoch 68: val_loss did not improve from 0.15621\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1477 - auc: 0.9877 - val_loss: 0.1574 - val_auc: 0.9859 - lr: 7.0000e-04\n",
            "Epoch 69/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1477 - auc: 0.9877\n",
            "Epoch 69: val_loss did not improve from 0.15621\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1477 - auc: 0.9877 - val_loss: 0.1569 - val_auc: 0.9860 - lr: 7.0000e-04\n",
            "Epoch 70/200\n",
            "350/352 [============================>.] - ETA: 0s - loss: 0.1474 - auc: 0.9877\n",
            "Epoch 70: val_loss did not improve from 0.15621\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1475 - auc: 0.9877 - val_loss: 0.1563 - val_auc: 0.9861 - lr: 7.0000e-04\n",
            "Epoch 71/200\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1472 - auc: 0.9878\n",
            "Epoch 71: val_loss did not improve from 0.15621\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1472 - auc: 0.9878 - val_loss: 0.1569 - val_auc: 0.9860 - lr: 7.0000e-04\n",
            "Epoch 72/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1470 - auc: 0.9878\n",
            "Epoch 72: val_loss improved from 0.15621 to 0.15575, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1470 - auc: 0.9878 - val_loss: 0.1557 - val_auc: 0.9862 - lr: 7.0000e-04\n",
            "Epoch 73/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1467 - auc: 0.9878\n",
            "Epoch 73: val_loss did not improve from 0.15575\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1467 - auc: 0.9878 - val_loss: 0.1571 - val_auc: 0.9860 - lr: 7.0000e-04\n",
            "Epoch 74/200\n",
            "349/352 [============================>.] - ETA: 0s - loss: 0.1464 - auc: 0.9879\n",
            "Epoch 74: val_loss did not improve from 0.15575\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1464 - auc: 0.9879 - val_loss: 0.1561 - val_auc: 0.9862 - lr: 7.0000e-04\n",
            "Epoch 75/200\n",
            "350/352 [============================>.] - ETA: 0s - loss: 0.1463 - auc: 0.9879\n",
            "Epoch 75: val_loss did not improve from 0.15575\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1463 - auc: 0.9879 - val_loss: 0.1564 - val_auc: 0.9861 - lr: 7.0000e-04\n",
            "Epoch 76/200\n",
            "346/352 [============================>.] - ETA: 0s - loss: 0.1463 - auc: 0.9879\n",
            "Epoch 76: val_loss improved from 0.15575 to 0.15530, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1463 - auc: 0.9879 - val_loss: 0.1553 - val_auc: 0.9863 - lr: 7.0000e-04\n",
            "Epoch 77/200\n",
            "350/352 [============================>.] - ETA: 0s - loss: 0.1461 - auc: 0.9879\n",
            "Epoch 77: val_loss did not improve from 0.15530\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1461 - auc: 0.9879 - val_loss: 0.1563 - val_auc: 0.9862 - lr: 7.0000e-04\n",
            "Epoch 78/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1461 - auc: 0.9880\n",
            "Epoch 78: val_loss did not improve from 0.15530\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1461 - auc: 0.9880 - val_loss: 0.1555 - val_auc: 0.9863 - lr: 7.0000e-04\n",
            "Epoch 79/200\n",
            "345/352 [============================>.] - ETA: 0s - loss: 0.1457 - auc: 0.9880\n",
            "Epoch 79: val_loss did not improve from 0.15530\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1457 - auc: 0.9880 - val_loss: 0.1555 - val_auc: 0.9863 - lr: 7.0000e-04\n",
            "Epoch 80/200\n",
            "349/352 [============================>.] - ETA: 0s - loss: 0.1454 - auc: 0.9881\n",
            "Epoch 80: val_loss did not improve from 0.15530\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1453 - auc: 0.9881 - val_loss: 0.1559 - val_auc: 0.9862 - lr: 7.0000e-04\n",
            "Epoch 81/200\n",
            "345/352 [============================>.] - ETA: 0s - loss: 0.1456 - auc: 0.9880\n",
            "Epoch 81: val_loss improved from 0.15530 to 0.15505, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1456 - auc: 0.9880 - val_loss: 0.1550 - val_auc: 0.9865 - lr: 7.0000e-04\n",
            "Epoch 82/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1454 - auc: 0.9881\n",
            "Epoch 82: val_loss improved from 0.15505 to 0.15402, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 4s 11ms/step - loss: 0.1454 - auc: 0.9881 - val_loss: 0.1540 - val_auc: 0.9865 - lr: 7.0000e-04\n",
            "Epoch 83/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1452 - auc: 0.9881\n",
            "Epoch 83: val_loss did not improve from 0.15402\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1452 - auc: 0.9881 - val_loss: 0.1548 - val_auc: 0.9864 - lr: 7.0000e-04\n",
            "Epoch 84/200\n",
            "348/352 [============================>.] - ETA: 0s - loss: 0.1447 - auc: 0.9882\n",
            "Epoch 84: val_loss did not improve from 0.15402\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1448 - auc: 0.9882 - val_loss: 0.1543 - val_auc: 0.9865 - lr: 7.0000e-04\n",
            "Epoch 85/200\n",
            "345/352 [============================>.] - ETA: 0s - loss: 0.1447 - auc: 0.9882\n",
            "Epoch 85: val_loss did not improve from 0.15402\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1447 - auc: 0.9882 - val_loss: 0.1546 - val_auc: 0.9864 - lr: 7.0000e-04\n",
            "Epoch 86/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1444 - auc: 0.9882\n",
            "Epoch 86: val_loss did not improve from 0.15402\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1444 - auc: 0.9882 - val_loss: 0.1560 - val_auc: 0.9863 - lr: 7.0000e-04\n",
            "Epoch 87/200\n",
            "350/352 [============================>.] - ETA: 0s - loss: 0.1444 - auc: 0.9882\n",
            "Epoch 87: val_loss improved from 0.15402 to 0.15368, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1444 - auc: 0.9883 - val_loss: 0.1537 - val_auc: 0.9866 - lr: 7.0000e-04\n",
            "Epoch 88/200\n",
            "344/352 [============================>.] - ETA: 0s - loss: 0.1442 - auc: 0.9883\n",
            "Epoch 88: val_loss did not improve from 0.15368\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1441 - auc: 0.9883 - val_loss: 0.1542 - val_auc: 0.9865 - lr: 7.0000e-04\n",
            "Epoch 89/200\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1441 - auc: 0.9883\n",
            "Epoch 89: val_loss did not improve from 0.15368\n",
            "352/352 [==============================] - 2s 5ms/step - loss: 0.1441 - auc: 0.9883 - val_loss: 0.1555 - val_auc: 0.9866 - lr: 7.0000e-04\n",
            "Epoch 90/200\n",
            "349/352 [============================>.] - ETA: 0s - loss: 0.1440 - auc: 0.9883\n",
            "Epoch 90: val_loss improved from 0.15368 to 0.15332, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1440 - auc: 0.9883 - val_loss: 0.1533 - val_auc: 0.9867 - lr: 7.0000e-04\n",
            "Epoch 91/200\n",
            "349/352 [============================>.] - ETA: 0s - loss: 0.1435 - auc: 0.9884\n",
            "Epoch 91: val_loss did not improve from 0.15332\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1435 - auc: 0.9884 - val_loss: 0.1548 - val_auc: 0.9864 - lr: 7.0000e-04\n",
            "Epoch 92/200\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1437 - auc: 0.9884\n",
            "Epoch 92: val_loss did not improve from 0.15332\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1437 - auc: 0.9884 - val_loss: 0.1541 - val_auc: 0.9866 - lr: 7.0000e-04\n",
            "Epoch 93/200\n",
            "347/352 [============================>.] - ETA: 0s - loss: 0.1435 - auc: 0.9884\n",
            "Epoch 93: val_loss did not improve from 0.15332\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1434 - auc: 0.9884 - val_loss: 0.1534 - val_auc: 0.9866 - lr: 7.0000e-04\n",
            "Epoch 94/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1433 - auc: 0.9884\n",
            "Epoch 94: val_loss improved from 0.15332 to 0.15304, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1433 - auc: 0.9884 - val_loss: 0.1530 - val_auc: 0.9867 - lr: 7.0000e-04\n",
            "Epoch 95/200\n",
            "344/352 [============================>.] - ETA: 0s - loss: 0.1434 - auc: 0.9884\n",
            "Epoch 95: val_loss did not improve from 0.15304\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1432 - auc: 0.9884 - val_loss: 0.1537 - val_auc: 0.9866 - lr: 7.0000e-04\n",
            "Epoch 96/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1432 - auc: 0.9885\n",
            "Epoch 96: val_loss did not improve from 0.15304\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1432 - auc: 0.9885 - val_loss: 0.1532 - val_auc: 0.9868 - lr: 7.0000e-04\n",
            "Epoch 97/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1430 - auc: 0.9885\n",
            "Epoch 97: val_loss improved from 0.15304 to 0.15301, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1430 - auc: 0.9885 - val_loss: 0.1530 - val_auc: 0.9867 - lr: 7.0000e-04\n",
            "Epoch 98/200\n",
            "345/352 [============================>.] - ETA: 0s - loss: 0.1427 - auc: 0.9885\n",
            "Epoch 98: val_loss did not improve from 0.15301\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1428 - auc: 0.9885 - val_loss: 0.1531 - val_auc: 0.9867 - lr: 7.0000e-04\n",
            "Epoch 99/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1427 - auc: 0.9886\n",
            "Epoch 99: val_loss improved from 0.15301 to 0.15255, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1427 - auc: 0.9886 - val_loss: 0.1525 - val_auc: 0.9868 - lr: 7.0000e-04\n",
            "Epoch 100/200\n",
            "350/352 [============================>.] - ETA: 0s - loss: 0.1427 - auc: 0.9885\n",
            "Epoch 100: val_loss did not improve from 0.15255\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1427 - auc: 0.9885 - val_loss: 0.1528 - val_auc: 0.9867 - lr: 7.0000e-04\n",
            "Epoch 101/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1425 - auc: 0.9886\n",
            "Epoch 101: val_loss did not improve from 0.15255\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1425 - auc: 0.9886 - val_loss: 0.1536 - val_auc: 0.9868 - lr: 7.0000e-04\n",
            "Epoch 102/200\n",
            "346/352 [============================>.] - ETA: 0s - loss: 0.1424 - auc: 0.9886\n",
            "Epoch 102: val_loss improved from 0.15255 to 0.15219, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1424 - auc: 0.9886 - val_loss: 0.1522 - val_auc: 0.9868 - lr: 7.0000e-04\n",
            "Epoch 103/200\n",
            "349/352 [============================>.] - ETA: 0s - loss: 0.1421 - auc: 0.9886\n",
            "Epoch 103: val_loss did not improve from 0.15219\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1422 - auc: 0.9886 - val_loss: 0.1527 - val_auc: 0.9868 - lr: 7.0000e-04\n",
            "Epoch 104/200\n",
            "350/352 [============================>.] - ETA: 0s - loss: 0.1422 - auc: 0.9886\n",
            "Epoch 104: val_loss improved from 0.15219 to 0.15203, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 4s 10ms/step - loss: 0.1423 - auc: 0.9886 - val_loss: 0.1520 - val_auc: 0.9869 - lr: 7.0000e-04\n",
            "Epoch 105/200\n",
            "344/352 [============================>.] - ETA: 0s - loss: 0.1420 - auc: 0.9887\n",
            "Epoch 105: val_loss did not improve from 0.15203\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1419 - auc: 0.9887 - val_loss: 0.1531 - val_auc: 0.9867 - lr: 7.0000e-04\n",
            "Epoch 106/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1418 - auc: 0.9887\n",
            "Epoch 106: val_loss did not improve from 0.15203\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1418 - auc: 0.9887 - val_loss: 0.1525 - val_auc: 0.9868 - lr: 7.0000e-04\n",
            "Epoch 107/200\n",
            "347/352 [============================>.] - ETA: 0s - loss: 0.1418 - auc: 0.9887\n",
            "Epoch 107: val_loss did not improve from 0.15203\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1418 - auc: 0.9887 - val_loss: 0.1522 - val_auc: 0.9869 - lr: 7.0000e-04\n",
            "Epoch 108/200\n",
            "348/352 [============================>.] - ETA: 0s - loss: 0.1416 - auc: 0.9887\n",
            "Epoch 108: val_loss improved from 0.15203 to 0.15183, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1415 - auc: 0.9887 - val_loss: 0.1518 - val_auc: 0.9869 - lr: 7.0000e-04\n",
            "Epoch 109/200\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1415 - auc: 0.9888\n",
            "Epoch 109: val_loss did not improve from 0.15183\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1415 - auc: 0.9888 - val_loss: 0.1523 - val_auc: 0.9869 - lr: 7.0000e-04\n",
            "Epoch 110/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1411 - auc: 0.9888\n",
            "Epoch 110: val_loss did not improve from 0.15183\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1411 - auc: 0.9888 - val_loss: 0.1519 - val_auc: 0.9870 - lr: 7.0000e-04\n",
            "Epoch 111/200\n",
            "347/352 [============================>.] - ETA: 0s - loss: 0.1413 - auc: 0.9888\n",
            "Epoch 111: val_loss improved from 0.15183 to 0.15174, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1413 - auc: 0.9888 - val_loss: 0.1517 - val_auc: 0.9870 - lr: 7.0000e-04\n",
            "Epoch 112/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1410 - auc: 0.9888\n",
            "Epoch 112: val_loss did not improve from 0.15174\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1411 - auc: 0.9888 - val_loss: 0.1520 - val_auc: 0.9870 - lr: 7.0000e-04\n",
            "Epoch 113/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1411 - auc: 0.9888\n",
            "Epoch 113: ReduceLROnPlateau reducing learning rate to 0.0004900000232737511.\n",
            "\n",
            "Epoch 113: val_loss did not improve from 0.15174\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1411 - auc: 0.9888 - val_loss: 0.1529 - val_auc: 0.9869 - lr: 7.0000e-04\n",
            "Epoch 114/200\n",
            "346/352 [============================>.] - ETA: 0s - loss: 0.1394 - auc: 0.9891\n",
            "Epoch 114: val_loss improved from 0.15174 to 0.15016, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1393 - auc: 0.9891 - val_loss: 0.1502 - val_auc: 0.9872 - lr: 4.9000e-04\n",
            "Epoch 115/200\n",
            "345/352 [============================>.] - ETA: 0s - loss: 0.1393 - auc: 0.9891\n",
            "Epoch 115: val_loss improved from 0.15016 to 0.15014, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1393 - auc: 0.9891 - val_loss: 0.1501 - val_auc: 0.9872 - lr: 4.9000e-04\n",
            "Epoch 116/200\n",
            "345/352 [============================>.] - ETA: 0s - loss: 0.1391 - auc: 0.9891\n",
            "Epoch 116: val_loss did not improve from 0.15014\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1391 - auc: 0.9891 - val_loss: 0.1502 - val_auc: 0.9872 - lr: 4.9000e-04\n",
            "Epoch 117/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1393 - auc: 0.9891\n",
            "Epoch 117: val_loss improved from 0.15014 to 0.15009, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1393 - auc: 0.9891 - val_loss: 0.1501 - val_auc: 0.9872 - lr: 4.9000e-04\n",
            "Epoch 118/200\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1390 - auc: 0.9892\n",
            "Epoch 118: val_loss improved from 0.15009 to 0.14966, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 10ms/step - loss: 0.1390 - auc: 0.9892 - val_loss: 0.1497 - val_auc: 0.9873 - lr: 4.9000e-04\n",
            "Epoch 119/200\n",
            "348/352 [============================>.] - ETA: 0s - loss: 0.1389 - auc: 0.9892\n",
            "Epoch 119: val_loss did not improve from 0.14966\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1390 - auc: 0.9892 - val_loss: 0.1504 - val_auc: 0.9872 - lr: 4.9000e-04\n",
            "Epoch 120/200\n",
            "348/352 [============================>.] - ETA: 0s - loss: 0.1390 - auc: 0.9892\n",
            "Epoch 120: val_loss did not improve from 0.14966\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1389 - auc: 0.9892 - val_loss: 0.1506 - val_auc: 0.9872 - lr: 4.9000e-04\n",
            "Epoch 121/200\n",
            "350/352 [============================>.] - ETA: 0s - loss: 0.1388 - auc: 0.9892\n",
            "Epoch 121: val_loss did not improve from 0.14966\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1389 - auc: 0.9892 - val_loss: 0.1498 - val_auc: 0.9872 - lr: 4.9000e-04\n",
            "Epoch 122/200\n",
            "348/352 [============================>.] - ETA: 0s - loss: 0.1387 - auc: 0.9892\n",
            "Epoch 122: val_loss did not improve from 0.14966\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1387 - auc: 0.9892 - val_loss: 0.1500 - val_auc: 0.9872 - lr: 4.9000e-04\n",
            "Epoch 123/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1385 - auc: 0.9893\n",
            "Epoch 123: ReduceLROnPlateau reducing learning rate to 0.00034300000406801696.\n",
            "\n",
            "Epoch 123: val_loss did not improve from 0.14966\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1385 - auc: 0.9893 - val_loss: 0.1499 - val_auc: 0.9873 - lr: 4.9000e-04\n",
            "Epoch 124/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1376 - auc: 0.9894\n",
            "Epoch 124: val_loss improved from 0.14966 to 0.14927, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 4s 11ms/step - loss: 0.1376 - auc: 0.9894 - val_loss: 0.1493 - val_auc: 0.9874 - lr: 3.4300e-04\n",
            "Epoch 125/200\n",
            "346/352 [============================>.] - ETA: 0s - loss: 0.1375 - auc: 0.9894\n",
            "Epoch 125: val_loss improved from 0.14927 to 0.14893, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1374 - auc: 0.9894 - val_loss: 0.1489 - val_auc: 0.9874 - lr: 3.4300e-04\n",
            "Epoch 126/200\n",
            "347/352 [============================>.] - ETA: 0s - loss: 0.1374 - auc: 0.9894\n",
            "Epoch 126: val_loss did not improve from 0.14893\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1374 - auc: 0.9894 - val_loss: 0.1491 - val_auc: 0.9874 - lr: 3.4300e-04\n",
            "Epoch 127/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1373 - auc: 0.9894\n",
            "Epoch 127: val_loss did not improve from 0.14893\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1373 - auc: 0.9894 - val_loss: 0.1495 - val_auc: 0.9874 - lr: 3.4300e-04\n",
            "Epoch 128/200\n",
            "346/352 [============================>.] - ETA: 0s - loss: 0.1371 - auc: 0.9895\n",
            "Epoch 128: val_loss did not improve from 0.14893\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1372 - auc: 0.9895 - val_loss: 0.1494 - val_auc: 0.9873 - lr: 3.4300e-04\n",
            "Epoch 129/200\n",
            "348/352 [============================>.] - ETA: 0s - loss: 0.1372 - auc: 0.9895\n",
            "Epoch 129: val_loss improved from 0.14893 to 0.14862, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1372 - auc: 0.9895 - val_loss: 0.1486 - val_auc: 0.9875 - lr: 3.4300e-04\n",
            "Epoch 130/200\n",
            "348/352 [============================>.] - ETA: 0s - loss: 0.1372 - auc: 0.9894\n",
            "Epoch 130: val_loss did not improve from 0.14862\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1372 - auc: 0.9895 - val_loss: 0.1492 - val_auc: 0.9874 - lr: 3.4300e-04\n",
            "Epoch 131/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1371 - auc: 0.9895\n",
            "Epoch 131: val_loss did not improve from 0.14862\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1371 - auc: 0.9895 - val_loss: 0.1495 - val_auc: 0.9873 - lr: 3.4300e-04\n",
            "Epoch 132/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1371 - auc: 0.9895\n",
            "Epoch 132: val_loss did not improve from 0.14862\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1371 - auc: 0.9895 - val_loss: 0.1493 - val_auc: 0.9875 - lr: 3.4300e-04\n",
            "Epoch 133/200\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1370 - auc: 0.9895\n",
            "Epoch 133: val_loss did not improve from 0.14862\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1370 - auc: 0.9895 - val_loss: 0.1488 - val_auc: 0.9874 - lr: 3.4300e-04\n",
            "Epoch 134/200\n",
            "347/352 [============================>.] - ETA: 0s - loss: 0.1369 - auc: 0.9895\n",
            "Epoch 134: ReduceLROnPlateau reducing learning rate to 0.00024009999469853935.\n",
            "\n",
            "Epoch 134: val_loss did not improve from 0.14862\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1369 - auc: 0.9895 - val_loss: 0.1490 - val_auc: 0.9874 - lr: 3.4300e-04\n",
            "Epoch 135/200\n",
            "350/352 [============================>.] - ETA: 0s - loss: 0.1361 - auc: 0.9896\n",
            "Epoch 135: val_loss improved from 0.14862 to 0.14836, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1361 - auc: 0.9896 - val_loss: 0.1484 - val_auc: 0.9875 - lr: 2.4010e-04\n",
            "Epoch 136/200\n",
            "349/352 [============================>.] - ETA: 0s - loss: 0.1361 - auc: 0.9896\n",
            "Epoch 136: val_loss did not improve from 0.14836\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1360 - auc: 0.9896 - val_loss: 0.1484 - val_auc: 0.9875 - lr: 2.4010e-04\n",
            "Epoch 137/200\n",
            "347/352 [============================>.] - ETA: 0s - loss: 0.1360 - auc: 0.9897\n",
            "Epoch 137: val_loss improved from 0.14836 to 0.14826, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1360 - auc: 0.9896 - val_loss: 0.1483 - val_auc: 0.9875 - lr: 2.4010e-04\n",
            "Epoch 138/200\n",
            "347/352 [============================>.] - ETA: 0s - loss: 0.1359 - auc: 0.9896\n",
            "Epoch 138: val_loss improved from 0.14826 to 0.14791, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1359 - auc: 0.9897 - val_loss: 0.1479 - val_auc: 0.9876 - lr: 2.4010e-04\n",
            "Epoch 139/200\n",
            "350/352 [============================>.] - ETA: 0s - loss: 0.1360 - auc: 0.9896\n",
            "Epoch 139: val_loss did not improve from 0.14791\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1360 - auc: 0.9896 - val_loss: 0.1481 - val_auc: 0.9875 - lr: 2.4010e-04\n",
            "Epoch 140/200\n",
            "350/352 [============================>.] - ETA: 0s - loss: 0.1359 - auc: 0.9897\n",
            "Epoch 140: val_loss did not improve from 0.14791\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1359 - auc: 0.9897 - val_loss: 0.1481 - val_auc: 0.9876 - lr: 2.4010e-04\n",
            "Epoch 141/200\n",
            "346/352 [============================>.] - ETA: 0s - loss: 0.1359 - auc: 0.9897\n",
            "Epoch 141: val_loss did not improve from 0.14791\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1358 - auc: 0.9897 - val_loss: 0.1489 - val_auc: 0.9875 - lr: 2.4010e-04\n",
            "Epoch 142/200\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1359 - auc: 0.9897\n",
            "Epoch 142: val_loss did not improve from 0.14791\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1359 - auc: 0.9897 - val_loss: 0.1480 - val_auc: 0.9876 - lr: 2.4010e-04\n",
            "Epoch 143/200\n",
            "346/352 [============================>.] - ETA: 0s - loss: 0.1357 - auc: 0.9897\n",
            "Epoch 143: ReduceLROnPlateau reducing learning rate to 0.00016806999628897755.\n",
            "\n",
            "Epoch 143: val_loss did not improve from 0.14791\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1357 - auc: 0.9897 - val_loss: 0.1480 - val_auc: 0.9876 - lr: 2.4010e-04\n",
            "Epoch 144/200\n",
            "350/352 [============================>.] - ETA: 0s - loss: 0.1352 - auc: 0.9898\n",
            "Epoch 144: val_loss improved from 0.14791 to 0.14768, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1351 - auc: 0.9898 - val_loss: 0.1477 - val_auc: 0.9876 - lr: 1.6807e-04\n",
            "Epoch 145/200\n",
            "345/352 [============================>.] - ETA: 0s - loss: 0.1351 - auc: 0.9898\n",
            "Epoch 145: val_loss did not improve from 0.14768\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1351 - auc: 0.9898 - val_loss: 0.1479 - val_auc: 0.9876 - lr: 1.6807e-04\n",
            "Epoch 146/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1352 - auc: 0.9898\n",
            "Epoch 146: val_loss improved from 0.14768 to 0.14766, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 4s 11ms/step - loss: 0.1352 - auc: 0.9898 - val_loss: 0.1477 - val_auc: 0.9876 - lr: 1.6807e-04\n",
            "Epoch 147/200\n",
            "350/352 [============================>.] - ETA: 0s - loss: 0.1351 - auc: 0.9898\n",
            "Epoch 147: val_loss did not improve from 0.14766\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1351 - auc: 0.9898 - val_loss: 0.1477 - val_auc: 0.9876 - lr: 1.6807e-04\n",
            "Epoch 148/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1350 - auc: 0.9898\n",
            "Epoch 148: val_loss improved from 0.14766 to 0.14758, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1350 - auc: 0.9898 - val_loss: 0.1476 - val_auc: 0.9876 - lr: 1.6807e-04\n",
            "Epoch 149/200\n",
            "347/352 [============================>.] - ETA: 0s - loss: 0.1351 - auc: 0.9898\n",
            "Epoch 149: val_loss did not improve from 0.14758\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1350 - auc: 0.9898 - val_loss: 0.1476 - val_auc: 0.9876 - lr: 1.6807e-04\n",
            "Epoch 150/200\n",
            "348/352 [============================>.] - ETA: 0s - loss: 0.1350 - auc: 0.9898\n",
            "Epoch 150: val_loss did not improve from 0.14758\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1350 - auc: 0.9898 - val_loss: 0.1480 - val_auc: 0.9876 - lr: 1.6807e-04\n",
            "Epoch 151/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1350 - auc: 0.9898\n",
            "Epoch 151: val_loss did not improve from 0.14758\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1350 - auc: 0.9898 - val_loss: 0.1477 - val_auc: 0.9876 - lr: 1.6807e-04\n",
            "Epoch 152/200\n",
            "344/352 [============================>.] - ETA: 0s - loss: 0.1349 - auc: 0.9898\n",
            "Epoch 152: val_loss did not improve from 0.14758\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1349 - auc: 0.9898 - val_loss: 0.1476 - val_auc: 0.9876 - lr: 1.6807e-04\n",
            "Epoch 153/200\n",
            "349/352 [============================>.] - ETA: 0s - loss: 0.1350 - auc: 0.9898\n",
            "Epoch 153: ReduceLROnPlateau reducing learning rate to 0.00011764899536501615.\n",
            "\n",
            "Epoch 153: val_loss did not improve from 0.14758\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1349 - auc: 0.9898 - val_loss: 0.1479 - val_auc: 0.9876 - lr: 1.6807e-04\n",
            "Epoch 154/200\n",
            "349/352 [============================>.] - ETA: 0s - loss: 0.1345 - auc: 0.9899\n",
            "Epoch 154: val_loss improved from 0.14758 to 0.14751, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1345 - auc: 0.9899 - val_loss: 0.1475 - val_auc: 0.9876 - lr: 1.1765e-04\n",
            "Epoch 155/200\n",
            "350/352 [============================>.] - ETA: 0s - loss: 0.1344 - auc: 0.9899\n",
            "Epoch 155: val_loss improved from 0.14751 to 0.14747, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1344 - auc: 0.9899 - val_loss: 0.1475 - val_auc: 0.9876 - lr: 1.1765e-04\n",
            "Epoch 156/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1344 - auc: 0.9899\n",
            "Epoch 156: val_loss did not improve from 0.14747\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1345 - auc: 0.9899 - val_loss: 0.1476 - val_auc: 0.9876 - lr: 1.1765e-04\n",
            "Epoch 157/200\n",
            "344/352 [============================>.] - ETA: 0s - loss: 0.1344 - auc: 0.9899\n",
            "Epoch 157: val_loss improved from 0.14747 to 0.14737, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 10ms/step - loss: 0.1344 - auc: 0.9899 - val_loss: 0.1474 - val_auc: 0.9877 - lr: 1.1765e-04\n",
            "Epoch 158/200\n",
            "349/352 [============================>.] - ETA: 0s - loss: 0.1344 - auc: 0.9899\n",
            "Epoch 158: val_loss improved from 0.14737 to 0.14733, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1344 - auc: 0.9899 - val_loss: 0.1473 - val_auc: 0.9877 - lr: 1.1765e-04\n",
            "Epoch 159/200\n",
            "345/352 [============================>.] - ETA: 0s - loss: 0.1343 - auc: 0.9899\n",
            "Epoch 159: val_loss did not improve from 0.14733\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1344 - auc: 0.9899 - val_loss: 0.1475 - val_auc: 0.9876 - lr: 1.1765e-04\n",
            "Epoch 160/200\n",
            "350/352 [============================>.] - ETA: 0s - loss: 0.1344 - auc: 0.9899\n",
            "Epoch 160: val_loss improved from 0.14733 to 0.14731, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1344 - auc: 0.9899 - val_loss: 0.1473 - val_auc: 0.9877 - lr: 1.1765e-04\n",
            "Epoch 161/200\n",
            "346/352 [============================>.] - ETA: 0s - loss: 0.1343 - auc: 0.9899\n",
            "Epoch 161: val_loss improved from 0.14731 to 0.14724, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 10ms/step - loss: 0.1343 - auc: 0.9899 - val_loss: 0.1472 - val_auc: 0.9877 - lr: 1.1765e-04\n",
            "Epoch 162/200\n",
            "348/352 [============================>.] - ETA: 0s - loss: 0.1342 - auc: 0.9899\n",
            "Epoch 162: val_loss improved from 0.14724 to 0.14720, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 4s 11ms/step - loss: 0.1343 - auc: 0.9899 - val_loss: 0.1472 - val_auc: 0.9877 - lr: 1.1765e-04\n",
            "Epoch 163/200\n",
            "346/352 [============================>.] - ETA: 0s - loss: 0.1343 - auc: 0.9899\n",
            "Epoch 163: val_loss did not improve from 0.14720\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1343 - auc: 0.9899 - val_loss: 0.1474 - val_auc: 0.9877 - lr: 1.1765e-04\n",
            "Epoch 164/200\n",
            "346/352 [============================>.] - ETA: 0s - loss: 0.1343 - auc: 0.9899\n",
            "Epoch 164: val_loss did not improve from 0.14720\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1342 - auc: 0.9899 - val_loss: 0.1475 - val_auc: 0.9877 - lr: 1.1765e-04\n",
            "Epoch 165/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1343 - auc: 0.9899\n",
            "Epoch 165: val_loss improved from 0.14720 to 0.14720, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1342 - auc: 0.9899 - val_loss: 0.1472 - val_auc: 0.9877 - lr: 1.1765e-04\n",
            "Epoch 166/200\n",
            "347/352 [============================>.] - ETA: 0s - loss: 0.1342 - auc: 0.9899\n",
            "Epoch 166: val_loss did not improve from 0.14720\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1342 - auc: 0.9899 - val_loss: 0.1475 - val_auc: 0.9876 - lr: 1.1765e-04\n",
            "Epoch 167/200\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1342 - auc: 0.9899\n",
            "Epoch 167: ReduceLROnPlateau reducing learning rate to 8.235429777414538e-05.\n",
            "\n",
            "Epoch 167: val_loss did not improve from 0.14720\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1342 - auc: 0.9899 - val_loss: 0.1473 - val_auc: 0.9877 - lr: 1.1765e-04\n",
            "Epoch 168/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1338 - auc: 0.9900\n",
            "Epoch 168: val_loss improved from 0.14720 to 0.14717, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 10ms/step - loss: 0.1339 - auc: 0.9900 - val_loss: 0.1472 - val_auc: 0.9877 - lr: 8.2354e-05\n",
            "Epoch 169/200\n",
            "345/352 [============================>.] - ETA: 0s - loss: 0.1339 - auc: 0.9900\n",
            "Epoch 169: val_loss improved from 0.14717 to 0.14710, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 10ms/step - loss: 0.1339 - auc: 0.9900 - val_loss: 0.1471 - val_auc: 0.9877 - lr: 8.2354e-05\n",
            "Epoch 170/200\n",
            "350/352 [============================>.] - ETA: 0s - loss: 0.1338 - auc: 0.9900\n",
            "Epoch 170: val_loss did not improve from 0.14710\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1339 - auc: 0.9900 - val_loss: 0.1475 - val_auc: 0.9877 - lr: 8.2354e-05\n",
            "Epoch 171/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1339 - auc: 0.9900\n",
            "Epoch 171: val_loss improved from 0.14710 to 0.14705, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1339 - auc: 0.9900 - val_loss: 0.1470 - val_auc: 0.9877 - lr: 8.2354e-05\n",
            "Epoch 172/200\n",
            "350/352 [============================>.] - ETA: 0s - loss: 0.1338 - auc: 0.9900\n",
            "Epoch 172: val_loss did not improve from 0.14705\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1338 - auc: 0.9900 - val_loss: 0.1472 - val_auc: 0.9877 - lr: 8.2354e-05\n",
            "Epoch 173/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1338 - auc: 0.9900\n",
            "Epoch 173: val_loss did not improve from 0.14705\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1338 - auc: 0.9900 - val_loss: 0.1472 - val_auc: 0.9877 - lr: 8.2354e-05\n",
            "Epoch 174/200\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1338 - auc: 0.9900\n",
            "Epoch 174: ReduceLROnPlateau reducing learning rate to 5.76480058953166e-05.\n",
            "\n",
            "Epoch 174: val_loss did not improve from 0.14705\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1338 - auc: 0.9900 - val_loss: 0.1471 - val_auc: 0.9877 - lr: 8.2354e-05\n",
            "Epoch 175/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1336 - auc: 0.9900\n",
            "Epoch 175: val_loss did not improve from 0.14705\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1336 - auc: 0.9900 - val_loss: 0.1471 - val_auc: 0.9877 - lr: 5.7648e-05\n",
            "Epoch 176/200\n",
            "348/352 [============================>.] - ETA: 0s - loss: 0.1336 - auc: 0.9900\n",
            "Epoch 176: val_loss improved from 0.14705 to 0.14702, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1336 - auc: 0.9900 - val_loss: 0.1470 - val_auc: 0.9877 - lr: 5.7648e-05\n",
            "Epoch 177/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1335 - auc: 0.9900\n",
            "Epoch 177: val_loss improved from 0.14702 to 0.14691, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 9ms/step - loss: 0.1335 - auc: 0.9900 - val_loss: 0.1469 - val_auc: 0.9877 - lr: 5.7648e-05\n",
            "Epoch 178/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1335 - auc: 0.9900\n",
            "Epoch 178: val_loss did not improve from 0.14691\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1335 - auc: 0.9900 - val_loss: 0.1470 - val_auc: 0.9877 - lr: 5.7648e-05\n",
            "Epoch 179/200\n",
            "349/352 [============================>.] - ETA: 0s - loss: 0.1336 - auc: 0.9900\n",
            "Epoch 179: val_loss did not improve from 0.14691\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1335 - auc: 0.9900 - val_loss: 0.1470 - val_auc: 0.9877 - lr: 5.7648e-05\n",
            "Epoch 180/200\n",
            "349/352 [============================>.] - ETA: 0s - loss: 0.1335 - auc: 0.9900\n",
            "Epoch 180: val_loss did not improve from 0.14691\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1335 - auc: 0.9900 - val_loss: 0.1471 - val_auc: 0.9877 - lr: 5.7648e-05\n",
            "Epoch 181/200\n",
            "348/352 [============================>.] - ETA: 0s - loss: 0.1335 - auc: 0.9900\n",
            "Epoch 181: val_loss did not improve from 0.14691\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1335 - auc: 0.9900 - val_loss: 0.1470 - val_auc: 0.9877 - lr: 5.7648e-05\n",
            "Epoch 182/200\n",
            "346/352 [============================>.] - ETA: 0s - loss: 0.1336 - auc: 0.9900\n",
            "Epoch 182: ReduceLROnPlateau reducing learning rate to 4.0353603617404586e-05.\n",
            "\n",
            "Epoch 182: val_loss did not improve from 0.14691\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1335 - auc: 0.9900 - val_loss: 0.1472 - val_auc: 0.9877 - lr: 5.7648e-05\n",
            "Epoch 183/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1334 - auc: 0.9901\n",
            "Epoch 183: val_loss did not improve from 0.14691\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1333 - auc: 0.9901 - val_loss: 0.1469 - val_auc: 0.9877 - lr: 4.0354e-05\n",
            "Epoch 184/200\n",
            "343/352 [============================>.] - ETA: 0s - loss: 0.1333 - auc: 0.9901\n",
            "Epoch 184: val_loss did not improve from 0.14691\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1333 - auc: 0.9901 - val_loss: 0.1469 - val_auc: 0.9877 - lr: 4.0354e-05\n",
            "Epoch 185/200\n",
            "344/352 [============================>.] - ETA: 0s - loss: 0.1333 - auc: 0.9901\n",
            "Epoch 185: val_loss did not improve from 0.14691\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1333 - auc: 0.9901 - val_loss: 0.1470 - val_auc: 0.9877 - lr: 4.0354e-05\n",
            "Epoch 186/200\n",
            "347/352 [============================>.] - ETA: 0s - loss: 0.1333 - auc: 0.9901\n",
            "Epoch 186: val_loss did not improve from 0.14691\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1333 - auc: 0.9901 - val_loss: 0.1470 - val_auc: 0.9877 - lr: 4.0354e-05\n",
            "Epoch 187/200\n",
            "348/352 [============================>.] - ETA: 0s - loss: 0.1334 - auc: 0.9900\n",
            "Epoch 187: ReduceLROnPlateau reducing learning rate to 2.8247522277524694e-05.\n",
            "\n",
            "Epoch 187: val_loss improved from 0.14691 to 0.14686, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 3s 10ms/step - loss: 0.1333 - auc: 0.9901 - val_loss: 0.1469 - val_auc: 0.9877 - lr: 4.0354e-05\n",
            "Epoch 188/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1332 - auc: 0.9901\n",
            "Epoch 188: val_loss improved from 0.14686 to 0.14684, saving model to /content\n",
            "INFO:tensorflow:Assets written to: /content/assets\n",
            "352/352 [==============================] - 4s 11ms/step - loss: 0.1332 - auc: 0.9901 - val_loss: 0.1468 - val_auc: 0.9877 - lr: 2.8248e-05\n",
            "Epoch 189/200\n",
            "348/352 [============================>.] - ETA: 0s - loss: 0.1332 - auc: 0.9901\n",
            "Epoch 189: val_loss did not improve from 0.14684\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1332 - auc: 0.9901 - val_loss: 0.1469 - val_auc: 0.9877 - lr: 2.8248e-05\n",
            "Epoch 190/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1332 - auc: 0.9901\n",
            "Epoch 190: val_loss did not improve from 0.14684\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1332 - auc: 0.9901 - val_loss: 0.1469 - val_auc: 0.9877 - lr: 2.8248e-05\n",
            "Epoch 191/200\n",
            "349/352 [============================>.] - ETA: 0s - loss: 0.1331 - auc: 0.9901\n",
            "Epoch 191: val_loss did not improve from 0.14684\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1332 - auc: 0.9901 - val_loss: 0.1469 - val_auc: 0.9877 - lr: 2.8248e-05\n",
            "Epoch 192/200\n",
            "350/352 [============================>.] - ETA: 0s - loss: 0.1331 - auc: 0.9901\n",
            "Epoch 192: ReduceLROnPlateau reducing learning rate to 1.977326610358432e-05.\n",
            "\n",
            "Epoch 192: val_loss did not improve from 0.14684\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1332 - auc: 0.9901 - val_loss: 0.1469 - val_auc: 0.9877 - lr: 2.8248e-05\n",
            "Epoch 193/200\n",
            "347/352 [============================>.] - ETA: 0s - loss: 0.1331 - auc: 0.9901\n",
            "Epoch 193: val_loss did not improve from 0.14684\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1331 - auc: 0.9901 - val_loss: 0.1469 - val_auc: 0.9877 - lr: 1.9773e-05\n",
            "Epoch 194/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1331 - auc: 0.9901\n",
            "Epoch 194: val_loss did not improve from 0.14684\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1331 - auc: 0.9901 - val_loss: 0.1469 - val_auc: 0.9877 - lr: 1.9773e-05\n",
            "Epoch 195/200\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1331 - auc: 0.9901\n",
            "Epoch 195: val_loss did not improve from 0.14684\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1331 - auc: 0.9901 - val_loss: 0.1469 - val_auc: 0.9877 - lr: 1.9773e-05\n",
            "Epoch 196/200\n",
            "350/352 [============================>.] - ETA: 0s - loss: 0.1330 - auc: 0.9901\n",
            "Epoch 196: val_loss did not improve from 0.14684\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1331 - auc: 0.9901 - val_loss: 0.1469 - val_auc: 0.9877 - lr: 1.9773e-05\n",
            "Epoch 197/200\n",
            "348/352 [============================>.] - ETA: 0s - loss: 0.1330 - auc: 0.9901\n",
            "Epoch 197: ReduceLROnPlateau reducing learning rate to 1.3841286272509023e-05.\n",
            "\n",
            "Epoch 197: val_loss did not improve from 0.14684\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1331 - auc: 0.9901 - val_loss: 0.1469 - val_auc: 0.9877 - lr: 1.9773e-05\n",
            "Epoch 198/200\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1330 - auc: 0.9901\n",
            "Epoch 198: val_loss did not improve from 0.14684\n",
            "352/352 [==============================] - 2s 6ms/step - loss: 0.1330 - auc: 0.9901 - val_loss: 0.1469 - val_auc: 0.9878 - lr: 1.3841e-05\n",
            "Epoch 198: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = scaler.fit_transform(test)"
      ],
      "metadata": {
        "id": "Lh4KcoMX8JN0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(test_df).squeeze()"
      ],
      "metadata": {
        "id": "ab6tiwMr8ONe"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "nmiejgY-KmvT",
        "outputId": "da4a3a57-bd15-40a6-e86c-fe32aa895025"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            f_00      f_01      f_02      f_03      f_04      f_05      f_06  \\\n",
              "0       0.442517  0.174380 -0.999816  0.762741  0.186778 -1.074775  0.501888   \n",
              "1      -0.605598 -0.305715  0.627667 -0.578898 -1.750931  1.355550 -0.190911   \n",
              "2       0.303990  2.445110  0.246515  0.818248  0.359731 -1.331845  1.358622   \n",
              "3       0.154053  0.260126 -1.367092 -0.093175 -1.111034 -0.948481  1.119220   \n",
              "4      -1.651904 -0.424266 -0.667356 -0.322124 -0.089462  0.181705  1.784983   \n",
              "...          ...       ...       ...       ...       ...       ...       ...   \n",
              "699995  0.640110  0.897808 -0.523956  1.563760 -0.092281 -0.610867  0.535426   \n",
              "699996 -0.191771 -0.035246 -0.118533  0.584750  2.126977  0.568659 -0.052663   \n",
              "699997 -0.331704 -0.328845 -1.185503  1.022128 -0.483099 -0.107146 -0.968281   \n",
              "699998 -2.031073 -1.238398  0.964699 -1.045950  0.906064  0.634301 -0.707474   \n",
              "699999 -0.085906 -0.002124  2.227375  0.217145  3.179153 -1.660188  0.891989   \n",
              "\n",
              "        f_07  f_08  f_09  ...      f_21      f_22      f_23      f_24  \\\n",
              "0          6     6     0  ... -1.006400 -1.193879 -2.435736 -2.427430   \n",
              "1          1     3     4  ...  2.382405  0.149442  1.883322 -2.848714   \n",
              "2          3     3     4  ... -7.026098  1.312277 -5.157192  1.714005   \n",
              "3          0     0     4  ... -0.594532 -3.939475  1.754570 -2.364007   \n",
              "4          2     2     2  ...  0.084906 -0.985736 -0.130467 -3.557893   \n",
              "...      ...   ...   ...  ...       ...       ...       ...       ...   \n",
              "699995     0     1     6  ...  2.604048  1.122867  0.518110  1.243837   \n",
              "699996     4     3     4  ...  3.029857  1.384682 -1.135740  2.982713   \n",
              "699997     1     1     2  ...  4.021273 -1.845266  1.096011 -2.734508   \n",
              "699998     5     1     1  ...  1.453864 -1.696606  1.018995  1.973697   \n",
              "699999     0     3     4  ... -3.549082 -4.325318 -5.017221  0.251268   \n",
              "\n",
              "            f_25      f_26        f_28  f_29  f_30  unique_length  \n",
              "0      -1.966887  5.734205   99.478419     0     0              5  \n",
              "1      -0.725155  3.194219  -65.993825     1     0              6  \n",
              "2       0.585032  0.066898  -87.405622     0     1              5  \n",
              "3      -1.003320  3.893099 -281.293460     0     0              5  \n",
              "4       1.210687  1.861884   25.629415     0     2              5  \n",
              "...          ...       ...         ...   ...   ...            ...  \n",
              "699995  0.575111  0.076372  204.186539     0     0              6  \n",
              "699996 -1.511760  2.225218  -97.694591     0     2              6  \n",
              "699997 -4.885955 -2.248739  130.622745     1     0              6  \n",
              "699998 -0.353068 -3.333449 -364.625148     0     0              6  \n",
              "699999 -3.236026 -0.362070 -155.417342     0     1              6  \n",
              "\n",
              "[700000 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-13a87309-5a46-4896-98f3-5214f8c26aee\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f_00</th>\n",
              "      <th>f_01</th>\n",
              "      <th>f_02</th>\n",
              "      <th>f_03</th>\n",
              "      <th>f_04</th>\n",
              "      <th>f_05</th>\n",
              "      <th>f_06</th>\n",
              "      <th>f_07</th>\n",
              "      <th>f_08</th>\n",
              "      <th>f_09</th>\n",
              "      <th>...</th>\n",
              "      <th>f_21</th>\n",
              "      <th>f_22</th>\n",
              "      <th>f_23</th>\n",
              "      <th>f_24</th>\n",
              "      <th>f_25</th>\n",
              "      <th>f_26</th>\n",
              "      <th>f_28</th>\n",
              "      <th>f_29</th>\n",
              "      <th>f_30</th>\n",
              "      <th>unique_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.442517</td>\n",
              "      <td>0.174380</td>\n",
              "      <td>-0.999816</td>\n",
              "      <td>0.762741</td>\n",
              "      <td>0.186778</td>\n",
              "      <td>-1.074775</td>\n",
              "      <td>0.501888</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.006400</td>\n",
              "      <td>-1.193879</td>\n",
              "      <td>-2.435736</td>\n",
              "      <td>-2.427430</td>\n",
              "      <td>-1.966887</td>\n",
              "      <td>5.734205</td>\n",
              "      <td>99.478419</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.605598</td>\n",
              "      <td>-0.305715</td>\n",
              "      <td>0.627667</td>\n",
              "      <td>-0.578898</td>\n",
              "      <td>-1.750931</td>\n",
              "      <td>1.355550</td>\n",
              "      <td>-0.190911</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>2.382405</td>\n",
              "      <td>0.149442</td>\n",
              "      <td>1.883322</td>\n",
              "      <td>-2.848714</td>\n",
              "      <td>-0.725155</td>\n",
              "      <td>3.194219</td>\n",
              "      <td>-65.993825</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.303990</td>\n",
              "      <td>2.445110</td>\n",
              "      <td>0.246515</td>\n",
              "      <td>0.818248</td>\n",
              "      <td>0.359731</td>\n",
              "      <td>-1.331845</td>\n",
              "      <td>1.358622</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>-7.026098</td>\n",
              "      <td>1.312277</td>\n",
              "      <td>-5.157192</td>\n",
              "      <td>1.714005</td>\n",
              "      <td>0.585032</td>\n",
              "      <td>0.066898</td>\n",
              "      <td>-87.405622</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.154053</td>\n",
              "      <td>0.260126</td>\n",
              "      <td>-1.367092</td>\n",
              "      <td>-0.093175</td>\n",
              "      <td>-1.111034</td>\n",
              "      <td>-0.948481</td>\n",
              "      <td>1.119220</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.594532</td>\n",
              "      <td>-3.939475</td>\n",
              "      <td>1.754570</td>\n",
              "      <td>-2.364007</td>\n",
              "      <td>-1.003320</td>\n",
              "      <td>3.893099</td>\n",
              "      <td>-281.293460</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.651904</td>\n",
              "      <td>-0.424266</td>\n",
              "      <td>-0.667356</td>\n",
              "      <td>-0.322124</td>\n",
              "      <td>-0.089462</td>\n",
              "      <td>0.181705</td>\n",
              "      <td>1.784983</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.084906</td>\n",
              "      <td>-0.985736</td>\n",
              "      <td>-0.130467</td>\n",
              "      <td>-3.557893</td>\n",
              "      <td>1.210687</td>\n",
              "      <td>1.861884</td>\n",
              "      <td>25.629415</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>699995</th>\n",
              "      <td>0.640110</td>\n",
              "      <td>0.897808</td>\n",
              "      <td>-0.523956</td>\n",
              "      <td>1.563760</td>\n",
              "      <td>-0.092281</td>\n",
              "      <td>-0.610867</td>\n",
              "      <td>0.535426</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>2.604048</td>\n",
              "      <td>1.122867</td>\n",
              "      <td>0.518110</td>\n",
              "      <td>1.243837</td>\n",
              "      <td>0.575111</td>\n",
              "      <td>0.076372</td>\n",
              "      <td>204.186539</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>699996</th>\n",
              "      <td>-0.191771</td>\n",
              "      <td>-0.035246</td>\n",
              "      <td>-0.118533</td>\n",
              "      <td>0.584750</td>\n",
              "      <td>2.126977</td>\n",
              "      <td>0.568659</td>\n",
              "      <td>-0.052663</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>3.029857</td>\n",
              "      <td>1.384682</td>\n",
              "      <td>-1.135740</td>\n",
              "      <td>2.982713</td>\n",
              "      <td>-1.511760</td>\n",
              "      <td>2.225218</td>\n",
              "      <td>-97.694591</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>699997</th>\n",
              "      <td>-0.331704</td>\n",
              "      <td>-0.328845</td>\n",
              "      <td>-1.185503</td>\n",
              "      <td>1.022128</td>\n",
              "      <td>-0.483099</td>\n",
              "      <td>-0.107146</td>\n",
              "      <td>-0.968281</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>4.021273</td>\n",
              "      <td>-1.845266</td>\n",
              "      <td>1.096011</td>\n",
              "      <td>-2.734508</td>\n",
              "      <td>-4.885955</td>\n",
              "      <td>-2.248739</td>\n",
              "      <td>130.622745</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>699998</th>\n",
              "      <td>-2.031073</td>\n",
              "      <td>-1.238398</td>\n",
              "      <td>0.964699</td>\n",
              "      <td>-1.045950</td>\n",
              "      <td>0.906064</td>\n",
              "      <td>0.634301</td>\n",
              "      <td>-0.707474</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1.453864</td>\n",
              "      <td>-1.696606</td>\n",
              "      <td>1.018995</td>\n",
              "      <td>1.973697</td>\n",
              "      <td>-0.353068</td>\n",
              "      <td>-3.333449</td>\n",
              "      <td>-364.625148</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>699999</th>\n",
              "      <td>-0.085906</td>\n",
              "      <td>-0.002124</td>\n",
              "      <td>2.227375</td>\n",
              "      <td>0.217145</td>\n",
              "      <td>3.179153</td>\n",
              "      <td>-1.660188</td>\n",
              "      <td>0.891989</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.549082</td>\n",
              "      <td>-4.325318</td>\n",
              "      <td>-5.017221</td>\n",
              "      <td>0.251268</td>\n",
              "      <td>-3.236026</td>\n",
              "      <td>-0.362070</td>\n",
              "      <td>-155.417342</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>700000 rows × 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13a87309-5a46-4896-98f3-5214f8c26aee')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-13a87309-5a46-4896-98f3-5214f8c26aee button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-13a87309-5a46-4896-98f3-5214f8c26aee');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = pd.DataFrame({\"id\" : test[\"id\"],\"target\": pred})"
      ],
      "metadata": {
        "id": "S2HYUFFL8a_F"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.to_csv(\"submission.csv\",index=False)"
      ],
      "metadata": {
        "id": "E_41FPUx8fF0"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}